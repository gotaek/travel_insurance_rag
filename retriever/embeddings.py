import os
from functools import lru_cache
from typing import List, Optional, Dict, Any
import numpy as np

# FlagEmbedding (BGE ê³„ì—´)
from FlagEmbedding import BGEM3FlagModel # m3ìš©
from FlagEmbedding import FlagModel       # generic wrapper

# ìºì‹± ê´€ë¦¬ì
from graph.cache_manager import cache_manager

EMB_NAME = os.getenv("EMB_MODEL_NAME", "dragonkue/multilingual-e5-small-ko")
EMB_BATCH = int(os.getenv("EMB_BATCH", "32"))

@lru_cache()
def _load_model():
    name = EMB_NAME.strip().lower()
    
    # Hugging Face í† í° ì„¤ì •
    hf_token = os.getenv("HUGGINGFACE_TOKEN")
    if hf_token:
        os.environ["HF_TOKEN"] = hf_token
        print("ğŸ”‘ Hugging Face í† í°ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print(f"ğŸ”„ {EMB_NAME} ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    # multilingual-e5-small-ko ëª¨ë¸ ì‚¬ìš©
    if "e5" in name or "multilingual" in name:
        model = FlagModel(EMB_NAME, use_fp16=True)
        return ("generic", model)
    # BGE-m3-ko ëª¨ë¸ ì‚¬ìš© (í•œêµ­ì–´ íŠ¹í™”)
    elif "m3-ko" in name or "dragonkue" in name:
        model = BGEM3FlagModel(EMB_NAME, use_fp16=True)
        return ("m3", model)
    # bge-m3 (ë©€í‹°ë§êµ¬ì–¼) ì§€ì›
    elif "m3" in name:
        model = BGEM3FlagModel(EMB_NAME, use_fp16=True)
        return ("m3", model)
    # ê¸°íƒ€ ëª¨ë¸: generic FlagModel ì‚¬ìš©
    else:
        model = FlagModel(EMB_NAME, use_fp16=True)
        return ("generic", model)

def embed_texts(texts: List[str]) -> np.ndarray:
    """
    í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (ìºì‹± ì§€ì›)
    """
    if not texts:
        return np.array([])
    
    # ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
    cached_embeddings = cache_manager.get_cached_embeddings(texts)
    if cached_embeddings is not None:
        print(f"âœ… ì„ë² ë”© ìºì‹œ íˆíŠ¸: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
        return cached_embeddings
    
    # ìºì‹œ ë¯¸ìŠ¤ - ëª¨ë¸ ë¡œë”© ë° ì„ë² ë”© ìƒì„±
    print(f"ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
    kind, model = _load_model()
    
    if kind == "m3":
        # returns {"dense_vecs": np.ndarray, ...}
        out = model.encode(texts, batch_size=EMB_BATCH)
        embeddings = out["dense_vecs"].astype("float32")
    else:
        # generic
        vecs = model.encode(texts, batch_size=EMB_BATCH)
        embeddings = np.array(vecs, dtype="float32").copy()
    
    # ê²°ê³¼ ìºì‹±
    cache_manager.cache_embeddings(texts, embeddings)
    print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ ë° ìºì‹±: {embeddings.shape}")
    
    return embeddings


def preload_embedding_model() -> bool:
    """
    ì„œë²„ ì‹œì‘ ì‹œ ì„ë² ë”© ëª¨ë¸ ì‚¬ì „ ë¡œë”©
    """
    try:
        print("ğŸ”„ ì„ë² ë”© ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì‹œì‘...")
        _load_model()
        print("âœ… ì„ë² ë”© ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì™„ë£Œ")
        return True
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì‹¤íŒ¨: {e}")
        return False


def get_embedding_model_info() -> Dict[str, Any]:
    """
    í˜„ì¬ ë¡œë”©ëœ ì„ë² ë”© ëª¨ë¸ ì •ë³´ ë°˜í™˜
    """
    try:
        kind, model = _load_model()
        return {
            "model_name": EMB_NAME,
            "model_type": kind,
            "batch_size": EMB_BATCH,
            "status": "loaded"
        }
    except Exception as e:
        return {
            "model_name": EMB_NAME,
            "model_type": "unknown",
            "batch_size": EMB_BATCH,
            "status": f"error: {e}"
        }