import os
from functools import lru_cache
from typing import List
import numpy as np

# FlagEmbedding (BGE ê³„ì—´)
from FlagEmbedding import BGEM3FlagModel # m3ìš©
from FlagEmbedding import FlagModel       # generic wrapper

EMB_NAME = os.getenv("EMB_MODEL_NAME", "dragonkue/multilingual-e5-small-ko")
EMB_BATCH = int(os.getenv("EMB_BATCH", "32"))

@lru_cache()
def _load_model():
    name = EMB_NAME.strip().lower()
    
    # Hugging Face í† í° ì„¤ì •
    hf_token = os.getenv("HUGGINGFACE_TOKEN")
    if hf_token:
        os.environ["HF_TOKEN"] = hf_token
        print("ðŸ”‘ Hugging Face í† í°ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print(f"ðŸ”„ {EMB_NAME} ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    # multilingual-e5-small-ko ëª¨ë¸ ì‚¬ìš©
    if "e5" in name or "multilingual" in name:
        model = FlagModel(EMB_NAME, use_fp16=True)
        return ("generic", model)
    # BGE-m3-ko ëª¨ë¸ ì‚¬ìš© (í•œêµ­ì–´ íŠ¹í™”)
    elif "m3-ko" in name or "dragonkue" in name:
        model = BGEM3FlagModel(EMB_NAME, use_fp16=True)
        return ("m3", model)
    # bge-m3 (ë©€í‹°ë§êµ¬ì–¼) ì§€ì›
    elif "m3" in name:
        model = BGEM3FlagModel(EMB_NAME, use_fp16=True)
        return ("m3", model)
    # ê¸°íƒ€ ëª¨ë¸: generic FlagModel ì‚¬ìš©
    else:
        model = FlagModel(EMB_NAME, use_fp16=True)
        return ("generic", model)

def embed_texts(texts: List[str]) -> np.ndarray:
    kind, model = _load_model()
    if kind == "m3":
        # returns {"dense_vecs": np.ndarray, ...}
        out = model.encode(texts, batch_size=EMB_BATCH)
        return out["dense_vecs"].astype("float32")
    # generic
    vecs = model.encode(texts, batch_size=EMB_BATCH)
    return np.array(vecs, dtype="float32").copy()