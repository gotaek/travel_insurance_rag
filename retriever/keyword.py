from typing import List, Dict, Any, Optional
from rank_bm25 import BM25Okapi
import os
import json
from pathlib import Path

try:
    import chromadb
except Exception:
    chromadb = None

def _tokenize(s: str) -> List[str]:
    # ê°„ë‹¨ í† í¬ë‚˜ì´ì €(ê³µë°± ë¶„ë¦¬). í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ëŒ€ì²´ìš© ìŠ¤í….
    return s.lower().split()

def _load_full_corpus() -> List[Dict[str, Any]]:
    """
    ì „ì²´ ì½”í¼ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    ë²¡í„° DBì—ì„œ ëª¨ë“  ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜, ë³„ë„ ì¸ë±ìŠ¤ì—ì„œ ë¡œë“œ.
    """
    try:
        # ë²¡í„° DBì—ì„œ ì „ì²´ ë¬¸ì„œ ë¡œë“œ ì‹œë„
        from app.deps import get_settings
        settings = get_settings()
        
        # Chroma DBì—ì„œ ì „ì²´ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        if chromadb:
            try:
                client = chromadb.PersistentClient(
                    path=settings.VECTOR_DIR,
                    settings=chromadb.config.Settings(anonymized_telemetry=False)
                )
                collection = client.get_collection("insurance_docs")
                
                # ì „ì²´ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 10000ê°œ)
                # include ë©”íƒ€ë°ì´í„°ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨
                results = collection.get(
                    limit=10000,
                    include=['documents', 'metadatas']
                )
                
                if results and results.get('documents'):
                    corpus = []
                    for i, doc in enumerate(results['documents']):
                        metadata = results['metadatas'][i] if results.get('metadatas') else {}
                        corpus.append({
                            "text": doc,
                            "doc_id": metadata.get("doc_id", f"doc_{i}"),
                            "page": metadata.get("page", 0),
                            **metadata
                        })
                    
                    # ë³´í—˜ì‚¬ë³„ ë¶„í¬ í™•ì¸
                    insurer_counts = {}
                    for item in corpus:
                        insurer = item.get("insurer", "Unknown")
                        insurer_counts[insurer] = insurer_counts.get(insurer, 0) + 1
                    
                    print(f"ğŸ“Š í‚¤ì›Œë“œ ê²€ìƒ‰ìš© ì „ì²´ ì½”í¼ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(corpus)}ê°œ ë¬¸ì„œ")
                    print(f"ğŸ“‹ ë³´í—˜ì‚¬ë³„ ë¶„í¬: {insurer_counts}")
                    
                    return corpus
            except Exception as e:
                print(f"âš ï¸ ë²¡í„° DBì—ì„œ ì „ì²´ ì½”í¼ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # í´ë°±: ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return []
        
    except Exception as e:
        print(f"âš ï¸ ì „ì²´ ì½”í¼ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

class KeywordStore:
    """
    Simple BM25 keyword search over pre-tokenized corpus.
    - docs: List[Dict] with at least a "text" field.
    """
    def __init__(self, docs: List[Dict[str, Any]]):
        self.docs = docs or []
        corpus = [ _tokenize(d.get("text", "")) for d in self.docs ]
        self.bm25 = BM25Okapi(corpus) if corpus else None

    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        if not self.docs or self.bm25 is None:
            return []
        scores = self.bm25.get_scores(_tokenize(query))
        ranked = sorted(
            (
                {**self.docs[i], "score_kw": float(scores[i])}
                for i in range(len(self.docs))
            ),
            key=lambda x: x["score_kw"],
            reverse=True,
        )
        return ranked[:k]

# Backward-compatible functional wrapper
def keyword_search(query: str, corpus_meta: List[Dict[str, Any]], k: int = 5) -> List[Dict[str, Any]]:
    """
    corpus_meta: [{"text": "...", "doc_id": "...", "page": 1, ...}, ...]
    - corpus_metaê°€ ë¹„ì—ˆìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜(ì•ˆì „)
    """
    store = KeywordStore(corpus_meta)
    return store.search(query, k=k)

# ì „ì—­ KeywordStore ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_keyword_store_cache: Optional[KeywordStore] = None

def keyword_search_full_corpus(query: str, k: int = 5, insurer_filter: Optional[List[str]] = None) -> List[Dict[str, Any]]:
    """
    ì „ì²´ ì½”í¼ìŠ¤ì—ì„œ BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    KeywordStore ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìºì‹±í•˜ì—¬ ì„±ëŠ¥ ìµœì í™”
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        insurer_filter: ë³´í—˜ì‚¬ í•„í„° (ì„ íƒì‚¬í•­)
        
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    global _keyword_store_cache
    
    # ìºì‹œëœ ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    if _keyword_store_cache is None:
        print("ğŸ”„ í‚¤ì›Œë“œ ê²€ìƒ‰ìš© ì „ì²´ ì½”í¼ìŠ¤ ë¡œë“œ ì¤‘...")
        full_corpus = _load_full_corpus()
        if not full_corpus:
            print("âš ï¸ ì „ì²´ ì½”í¼ìŠ¤ ë¡œë“œ ì‹¤íŒ¨")
            return []
        _keyword_store_cache = KeywordStore(full_corpus)
        print(f"âœ… í‚¤ì›Œë“œ ê²€ìƒ‰ìš© ì½”í¼ìŠ¤ ì¤€ë¹„ ì™„ë£Œ: {len(_keyword_store_cache.docs)}ê°œ ë¬¸ì„œ")
    
    # ë³´í—˜ì‚¬ í•„í„°ë§ì´ ìˆëŠ” ê²½ìš° ì‚¬ì „ í•„í„°ë§ ì ìš©
    if insurer_filter:
        filtered_docs = _apply_insurer_filter_to_corpus(_keyword_store_cache.docs, insurer_filter)
        if not filtered_docs:
            return []
        
        # í•„í„°ë§ëœ ë¬¸ì„œë¡œ ì„ì‹œ KeywordStore ìƒì„±
        temp_store = KeywordStore(filtered_docs)
        
        # BM25 ê²€ìƒ‰ ìˆ˜í–‰
        results = temp_store.search(query, k=k)
        
        return results
    else:
        # BM25 ê²€ìƒ‰ ìˆ˜í–‰
        results = _keyword_store_cache.search(query, k=k)
        
        return results

def _apply_insurer_filter_to_corpus(docs: List[Dict[str, Any]], insurer_filter: List[str]) -> List[Dict[str, Any]]:
    """
    ì½”í¼ìŠ¤ì—ì„œ ë³´í—˜ì‚¬ í•„í„°ë¥¼ ì ìš©í•˜ì—¬ í•„í„°ë§ëœ ë¬¸ì„œë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    insurer í•„ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ë§¤ì¹­ ìˆ˜í–‰
    
    Args:
        docs: ì „ì²´ ë¬¸ì„œ ì½”í¼ìŠ¤
        insurer_filter: ë³´í—˜ì‚¬ í•„í„° ë¦¬ìŠ¤íŠ¸
        
    Returns:
        í•„í„°ë§ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    if not insurer_filter:
        return docs
    
    import unicodedata
    
    def normalize_korean(text: str) -> str:
        """í•œê¸€ ì •ê·œí™” (ì™„ì„±í˜• -> ì¡°í•©í˜•) - DBê°€ NFD í˜•íƒœë¡œ ì €ì¥ë¨"""
        return unicodedata.normalize('NFD', text)
    
    filtered_docs = []
    for doc in docs:
        doc_insurer = doc.get("insurer", "")
        doc_insurer_normalized = normalize_korean(doc_insurer).lower()
        
        # ë³´í—˜ì‚¬ í•„í„°ì™€ ë§¤ì¹­ë˜ëŠ”ì§€ í™•ì¸
        matched = False
        for filter_insurer in insurer_filter:
            normalized_filter = normalize_korean(filter_insurer).lower()
            
            # ì •í™•í•œ ë§¤ì¹­ ìš°ì„  ì‹œë„
            if doc_insurer_normalized == normalized_filter:
                filtered_docs.append(doc)
                matched = True
                break
            
            # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„ (ì¹´ì¹´ì˜¤ -> ì¹´ì¹´ì˜¤í˜ì´)
            if normalized_filter in doc_insurer_normalized or doc_insurer_normalized in normalized_filter:
                filtered_docs.append(doc)
                matched = True
                break
    
    return filtered_docs

def _apply_insurer_filter_to_keyword_results(results: List[Dict[str, Any]], insurer_filter: List[str]) -> List[Dict[str, Any]]:
    """
    í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ì— ë³´í—˜ì‚¬ í•„í„°ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
    í•œê¸€ ì •ê·œí™”ë¥¼ í†µí•´ ì¡°í•©í˜•/ì™„ì„±í˜• í•œê¸€ì„ í†µì¼í•˜ì—¬ ë§¤ì¹­í•©ë‹ˆë‹¤.
    
    Args:
        results: í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼
        insurer_filter: ë³´í—˜ì‚¬ í•„í„° ë¦¬ìŠ¤íŠ¸
        
    Returns:
        í•„í„°ë§ëœ ê²€ìƒ‰ ê²°ê³¼
    """
    if not insurer_filter:
        return results
    
    import unicodedata
    
    def normalize_korean(text: str) -> str:
        """í•œê¸€ ì •ê·œí™” (ì™„ì„±í˜• -> ì¡°í•©í˜•) - DBê°€ NFD í˜•íƒœë¡œ ì €ì¥ë¨"""
        return unicodedata.normalize('NFD', text)
    
    filtered_results = []
    for result in results:
        doc_insurer = result.get("insurer", "")
        doc_insurer_normalized = normalize_korean(doc_insurer).lower()
        
        # ë³´í—˜ì‚¬ í•„í„°ì™€ ë§¤ì¹­ë˜ëŠ”ì§€ í™•ì¸
        matched = False
        for filter_insurer in insurer_filter:
            normalized_filter = normalize_korean(filter_insurer).lower()
            
            # ì •í™•í•œ ë§¤ì¹­ ìš°ì„  ì‹œë„
            if doc_insurer_normalized == normalized_filter:
                filtered_results.append(result)
                matched = True
                break
            
            # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
            if normalized_filter in doc_insurer_normalized or doc_insurer_normalized in normalized_filter:
                filtered_results.append(result)
                matched = True
                break
    
    return filtered_results

def get_keyword_store_info() -> Dict[str, Any]:
    """KeywordStore ìºì‹œ ì •ë³´ ë°˜í™˜"""
    global _keyword_store_cache
    return {
        "is_cached": _keyword_store_cache is not None,
        "corpus_size": len(_keyword_store_cache.docs) if _keyword_store_cache else 0
    }

def clear_keyword_store_cache():
    """KeywordStore ìºì‹œ ì´ˆê¸°í™”"""
    global _keyword_store_cache
    _keyword_store_cache = None

__all__ = ["KeywordStore", "keyword_search", "keyword_search_full_corpus"]