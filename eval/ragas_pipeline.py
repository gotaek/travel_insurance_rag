import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

import pandas as pd

from graph.builder import build_graph
from eval.recall_at_k import recall_at_k
from eval.faithfulness import simple_faithfulness

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# RAGASëŠ” ì„ íƒì ìœ¼ë¡œ ì‹¤í–‰ (ì„¤ì¹˜ ì‹¤íŒ¨/í™˜ê²½ ë¬¸ì œ ëŒ€ë¹„)
_USE_RAGAS = True
try:
    from datasets import Dataset
    from ragas import evaluate
    from ragas.metrics import answer_relevancy, faithfulness as ragas_faithfulness, context_precision
    logger.info("RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
except ImportError as e:
    _USE_RAGAS = False
    logger.warning(f"RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}. ê¸°ë³¸ ì§€í‘œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
except Exception as e:
    _USE_RAGAS = False
    logger.error(f"RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}. ê¸°ë³¸ ì§€í‘œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

DATA_PATH = Path("eval/questions.jsonl")
OUT_DIR = Path("eval/out")
OUT_DIR.mkdir(parents=True, exist_ok=True)

def run_one(graph, row: Dict[str, Any]) -> Dict[str, Any]:
    """
    ë‹¨ì¼ ì§ˆë¬¸ì— ëŒ€í•´ RAG ì‹œìŠ¤í…œì„ ì‹¤í–‰í•˜ê³  í‰ê°€ ì§€í‘œë¥¼ ê³„ì‚°.
    
    Args:
        graph: RAG ì‹œìŠ¤í…œ ê·¸ë˜í”„
        row: í‰ê°€ ì§ˆë¬¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        
    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (ì§€í‘œ, ë©”íƒ€ë°ì´í„°, RAGAS ì…ë ¥ í¬í•¨)
    """
    try:
        # ì§ˆë¬¸ ì •ë³´ ì¶”ì¶œ
        question = row.get("question", "")
        if not question:
            logger.warning(f"ë¹ˆ ì§ˆë¬¸ ë°œê²¬: {row.get('id', 'unknown')}")
            return _create_empty_result(row)
        
        gold_doc_ids = row.get("gold_doc_ids", [])
        state = {"question": question}
        
        # RAG ì‹œìŠ¤í…œ ì‹¤í–‰
        start_time = time.time()
        out = graph.invoke(state)
        execution_time = time.time() - start_time
        
        # ê¸°ë³¸ ì§€í‘œ ê³„ì‚°
        r_at5 = recall_at_k(out, gold_doc_ids, k=5)
        faith = simple_faithfulness(out)

        # RAGAS ì…ë ¥ ì¤€ë¹„
        ragas_item = _prepare_ragas_item(question, out)

        return {
            "id": row.get("id"),
            "question": question,
            "intent": out.get("intent"),
            "needs_web": out.get("needs_web"),
            "recall_at_5": r_at5,
            "faithfulness_simple": faith,
            "execution_time": execution_time,
            "ragas_item": ragas_item,
            "trace": out.get("trace"),
            "gold_doc_ids": gold_doc_ids,
            # ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            "expected_answer_type": row.get("expected_answer_type"),
            "difficulty": row.get("difficulty"),
            "category": row.get("category"),
            "tags": row.get("tags", []),
        }
        
    except Exception as e:
        logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ID: {row.get('id', 'unknown')}): {e}")
        return _create_empty_result(row, error=str(e))

def _create_empty_result(row: Dict[str, Any], error: Optional[str] = None) -> Dict[str, Any]:
    """ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±."""
    return {
        "id": row.get("id"),
        "question": row.get("question", ""),
        "intent": None,
        "needs_web": None,
        "recall_at_5": 0.0,
        "faithfulness_simple": 0.0,
        "execution_time": 0.0,
        "ragas_item": {
            "question": row.get("question", ""),
            "answer": "",
            "contexts": [],
            "ground_truths": []
        },
        "trace": None,
        "gold_doc_ids": row.get("gold_doc_ids", []),
        "error": error,
        # ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        "expected_answer_type": row.get("expected_answer_type"),
        "difficulty": row.get("difficulty"),
        "category": row.get("category"),
        "tags": row.get("tags", []),
    }

def _prepare_ragas_item(question: str, out: Dict[str, Any]) -> Dict[str, Any]:
    """RAGAS í‰ê°€ë¥¼ ìœ„í•œ ì…ë ¥ ë°ì´í„° ì¤€ë¹„."""
    draft_answer = out.get("draft_answer", {})
    refined_docs = out.get("refined", [])
    
    # ë‹µë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    answer_text = ""
    if isinstance(draft_answer, dict):
        answer_text = draft_answer.get("conclusion", "")
    
    # ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    contexts = []
    for doc in refined_docs:
        if isinstance(doc, dict) and doc.get("text"):
            contexts.append(doc["text"])
    
    return {
        "question": question,
        "answer": answer_text,
        "contexts": contexts,
        "ground_truths": []  # í•„ìš”ì‹œ ì •ë‹µ ë¬¸ì¥ ì¶”ê°€
    }

def load_questions() -> List[Dict[str, Any]]:
    """
    í‰ê°€ ì§ˆë¬¸ë“¤ì„ JSONL íŒŒì¼ì—ì„œ ë¡œë“œ.
    
    Returns:
        ì§ˆë¬¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    if not DATA_PATH.exists():
        raise FileNotFoundError(f"í‰ê°€ ì§ˆë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DATA_PATH}")
    
    rows = []
    for line_num, line in enumerate(DATA_PATH.read_text(encoding="utf-8").splitlines(), 1):
        line = line.strip()
        if not line or line.startswith('//'):  # ë¹ˆ ì¤„ì´ë‚˜ ì£¼ì„ ì œì™¸
            continue
            
        try:
            question_data = json.loads(line)
            if not isinstance(question_data, dict):
                logger.warning(f"ë¼ì¸ {line_num}: ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ë°ì´í„° ë¬´ì‹œ")
                continue
            rows.append(question_data)
        except json.JSONDecodeError as e:
            logger.error(f"ë¼ì¸ {line_num} JSON íŒŒì‹± ì˜¤ë¥˜: {line} - {e}")
            continue
    
    logger.info(f"ë¡œë“œëœ í‰ê°€ ì§ˆë¬¸: {len(rows)}ê°œ")
    return rows

def run_evaluation_parallel(graph, questions: List[Dict[str, Any]], max_workers: int = 2) -> List[Dict[str, Any]]:
    """
    ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ í‰ê°€ ì‹¤í–‰.
    
    Args:
        graph: RAG ì‹œìŠ¤í…œ ê·¸ë˜í”„
        questions: í‰ê°€ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        max_workers: ìµœëŒ€ ì›Œì»¤ ìˆ˜
        
    Returns:
        í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    results = []
    total_questions = len(questions)
    
    logger.info(f"ë³‘ë ¬ í‰ê°€ ì‹œì‘ (ì›Œì»¤ ìˆ˜: {max_workers})")
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ê° ì§ˆë¬¸ì— ëŒ€í•´ Future ê°ì²´ ìƒì„±
        future_to_question = {
            executor.submit(run_one, graph, question): question 
            for question in questions
        }
        
        # ì™„ë£Œëœ ì‘ì—…ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
        completed = 0
        for future in as_completed(future_to_question):
            try:
                result = future.result()
                results.append(result)
                completed += 1
                
                # ì§„í–‰ ìƒí™© ë¡œê¹…
                if completed % max(1, total_questions // 10) == 0 or completed == total_questions:
                    elapsed = time.time() - start_time
                    avg_time = elapsed / completed
                    remaining = (total_questions - completed) * avg_time
                    logger.info(f"ì§„í–‰ë¥ : {completed}/{total_questions} ({completed/total_questions*100:.1f}%) - "
                              f"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining:.1f}ì´ˆ")
                              
            except Exception as e:
                question = future_to_question[future]
                logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨ (ID: {question.get('id', 'unknown')}): {e}")
                results.append(_create_empty_result(question, error=str(e)))
    
    total_time = time.time() - start_time
    logger.info(f"í‰ê°€ ì™„ë£Œ - ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ, í‰ê· : {total_time/total_questions:.2f}ì´ˆ/ì§ˆë¬¸")
    
    return results

def save_results(results: List[Dict[str, Any]]) -> None:
    """í‰ê°€ ê²°ê³¼ë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ì €ì¥."""
    if not results:
        logger.warning("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­ CSV ì €ì¥ (ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° í¬í•¨)
    df_data = {
        "id": [x.get("id") for x in results],
        "question": [x.get("question", "") for x in results],
        "intent": [x.get("intent") for x in results],
        "needs_web": [x.get("needs_web") for x in results],
        "recall_at_5": [x.get("recall_at_5", 0.0) for x in results],
        "faithfulness_simple": [x.get("faithfulness_simple", 0.0) for x in results],
        "execution_time": [x.get("execution_time", 0.0) for x in results],
        "expected_answer_type": [x.get("expected_answer_type") for x in results],
        "difficulty": [x.get("difficulty") for x in results],
        "category": [x.get("category") for x in results],
    }
    
    # ì˜¤ë¥˜ê°€ ìˆëŠ” ê²½ìš° ì˜¤ë¥˜ ì •ë³´ ì¶”ê°€
    if any("error" in x for x in results):
        df_data["error"] = [x.get("error", "") for x in results]
    
    df = pd.DataFrame(df_data)
    metrics_path = OUT_DIR / "metrics.csv"
    df.to_csv(metrics_path, index=False, encoding="utf-8")
    logger.info(f"ê¸°ë³¸ ë©”íŠ¸ë¦­ ì €ì¥: {metrics_path}")

    # RAGAS í‰ê°€ ì‹¤í–‰ (ì„ íƒì )
    if _USE_RAGAS:
        try:
            ragas_items = [x.get("ragas_item", {}) for x in results]
            # ë¹ˆ í•­ëª© í•„í„°ë§
            valid_ragas_items = [item for item in ragas_items if item.get("question") and item.get("answer")]
            
            if valid_ragas_items:
                ds = Dataset.from_list(valid_ragas_items)
                ragas_res = evaluate(
                    ds,
                    metrics=[answer_relevancy, ragas_faithfulness, context_precision],
                )
                ragas_df = ragas_res.to_pandas()
                ragas_path = OUT_DIR / "ragas.csv"
                ragas_df.to_csv(ragas_path, index=False, encoding="utf-8")
                logger.info(f"RAGAS ë©”íŠ¸ë¦­ ì €ì¥: {ragas_path}")
            else:
                logger.warning("ìœ íš¨í•œ RAGAS í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            logger.error(f"RAGAS í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

    # ì „ì²´ ê²°ê³¼ JSON ì €ì¥
    raw_path = OUT_DIR / "raw.json"
    with open(raw_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    logger.info(f"ì „ì²´ ê²°ê³¼ ì €ì¥: {raw_path}")

def print_summary(results: List[Dict[str, Any]]) -> None:
    """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥."""
    if not results:
        logger.warning("ì¶œë ¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê¸°ë³¸ í†µê³„ ê³„ì‚°
    total_questions = len(results)
    successful_results = [r for r in results if "error" not in r]
    failed_count = total_questions - len(successful_results)
    
    if successful_results:
        recall_scores = [r.get("recall_at_5", 0.0) for r in successful_results]
        faith_scores = [r.get("faithfulness_simple", 0.0) for r in successful_results]
        exec_times = [r.get("execution_time", 0.0) for r in successful_results]
        
        avg_recall = sum(recall_scores) / len(recall_scores)
        avg_faith = sum(faith_scores) / len(faith_scores)
        avg_exec_time = sum(exec_times) / len(exec_times)
        
        # Intent ë¶„í¬
        intents = [r.get("intent") for r in successful_results if r.get("intent")]
        intent_counts = {}
        for intent in intents:
            intent_counts[intent] = intent_counts.get(intent, 0) + 1
    else:
        avg_recall = avg_faith = avg_exec_time = 0.0
        intent_counts = {}
    
    print("\n" + "="*60)
    print("ğŸ“ˆ í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("="*60)
    print(f"  ğŸ“Š ì´ ì§ˆë¬¸ ìˆ˜: {total_questions}")
    print(f"  âœ… ì„±ê³µ: {len(successful_results)}")
    print(f"  âŒ ì‹¤íŒ¨: {failed_count}")
    
    if successful_results:
        print(f"  ğŸ¯ í‰ê·  Recall@5: {avg_recall:.3f}")
        print(f"  ğŸ” í‰ê·  Faithfulness: {avg_faith:.3f}")
        print(f"  â±ï¸  í‰ê·  ì‹¤í–‰ ì‹œê°„: {avg_exec_time:.2f}ì´ˆ")
        
        if intent_counts:
            print(f"  ğŸ“‹ Intent ë¶„í¬:")
            for intent, count in sorted(intent_counts.items()):
                print(f"    - {intent}: {count}ê°œ")
    
    print(f"\nâœ… ì €ì¥ëœ íŒŒì¼:")
    print(f"  - {OUT_DIR / 'metrics.csv'}")
    if _USE_RAGAS:
        print(f"  - {OUT_DIR / 'ragas.csv'}")
    print(f"  - {OUT_DIR / 'raw.json'}")

def main():
    """ë©”ì¸ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰."""
    try:
        # ì§ˆë¬¸ ë¡œë“œ
        questions = load_questions()
        if not questions:
            logger.error("í‰ê°€í•  ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # RAG ê·¸ë˜í”„ ë¹Œë“œ
        logger.info("RAG ê·¸ë˜í”„ ë¹Œë“œ ì¤‘...")
        graph = build_graph()
        
        # í‰ê°€ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬)
        results = run_evaluation_parallel(graph, questions, max_workers=2)
        
        # ê²°ê³¼ ì €ì¥
        save_results(results)
        
        # ìš”ì•½ ì¶œë ¥
        print_summary(results)
        
    except Exception as e:
        logger.error(f"í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

if __name__ == "__main__":
    main()