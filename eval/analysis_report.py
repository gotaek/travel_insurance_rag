#!/usr/bin/env python3
"""
í‰ê°€ ê²°ê³¼ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ë° ë‚´ë³´ë‚´ê¸°
"""

import json
import logging
import statistics
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
import warnings

import pandas as pd
import numpy as np

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings('ignore', category=FutureWarning)

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

class EvaluationAnalyzer:
    """RAG ì‹œìŠ¤í…œ í‰ê°€ ê²°ê³¼ ë¶„ì„ê¸°"""
    
    def __init__(self, metrics_path: str = "eval/out/metrics.csv", raw_path: str = "eval/out/raw.json"):
        """
        ë¶„ì„ê¸° ì´ˆê¸°í™”.
        
        Args:
            metrics_path: ë©”íŠ¸ë¦­ CSV íŒŒì¼ ê²½ë¡œ
            raw_path: ì›ì‹œ ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œ
        """
        self.metrics_path = Path(metrics_path)
        self.raw_path = Path(raw_path)
        self.metrics_df: Optional[pd.DataFrame] = None
        self.raw_data: Optional[List[Dict[str, Any]]] = None
        self.analysis_results: Dict[str, Any] = {}
        
    def load_data(self) -> bool:
        """
        í‰ê°€ ê²°ê³¼ ë°ì´í„° ë¡œë“œ.
        
        Returns:
            bool: ë°ì´í„° ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ë©”íŠ¸ë¦­ ë°ì´í„° ë¡œë“œ
            if not self.metrics_path.exists():
                logger.error(f"ë©”íŠ¸ë¦­ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.metrics_path}")
                return False
                
            self.metrics_df = pd.read_csv(self.metrics_path)
            logger.info(f"ë©”íŠ¸ë¦­ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.metrics_df)}ê°œ ì§ˆë¬¸")
            
            # ì›ì‹œ ë°ì´í„° ë¡œë“œ (ì„ íƒì )
            if self.raw_path.exists():
                with open(self.raw_path, 'r', encoding='utf-8') as f:
                    self.raw_data = json.load(f)
                logger.info(f"ì›ì‹œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.raw_data)}ê°œ ê²°ê³¼")
            else:
                logger.warning(f"ì›ì‹œ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.raw_path}")
                self.raw_data = []
            
            return True
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
    
    def validate_data(self) -> bool:
        """ë°ì´í„° ìœ íš¨ì„± ê²€ì¦."""
        if self.metrics_df is None:
            return False
            
        required_columns = ['id', 'recall_at_5', 'faithfulness_simple']
        missing_columns = [col for col in required_columns if col not in self.metrics_df.columns]
        
        if missing_columns:
            logger.error(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_columns}")
            return False
            
        # ë°ì´í„° íƒ€ì… ê²€ì¦
        try:
            self.metrics_df['recall_at_5'] = pd.to_numeric(self.metrics_df['recall_at_5'], errors='coerce')
            self.metrics_df['faithfulness_simple'] = pd.to_numeric(self.metrics_df['faithfulness_simple'], errors='coerce')
        except Exception as e:
            logger.error(f"ë°ì´í„° íƒ€ì… ë³€í™˜ ì˜¤ë¥˜: {e}")
            return False
            
        return True
    
    def calculate_overall_statistics(self) -> Dict[str, Any]:
        """ì „ì²´ í†µê³„ ê³„ì‚°."""
        if self.metrics_df is None:
            return {}
            
        # ê¸°ë³¸ í†µê³„
        total_questions = len(self.metrics_df)
        successful_questions = len(self.metrics_df.dropna(subset=['recall_at_5', 'faithfulness_simple']))
        
        # ì„±ëŠ¥ ì§€í‘œ
        recall_scores = self.metrics_df['recall_at_5'].dropna()
        faith_scores = self.metrics_df['faithfulness_simple'].dropna()
        
        stats = {
            'total_questions': total_questions,
            'successful_questions': successful_questions,
            'success_rate': successful_questions / total_questions if total_questions > 0 else 0,
            'recall_at_5': {
                'mean': recall_scores.mean() if len(recall_scores) > 0 else 0,
                'median': recall_scores.median() if len(recall_scores) > 0 else 0,
                'std': recall_scores.std() if len(recall_scores) > 0 else 0,
                'min': recall_scores.min() if len(recall_scores) > 0 else 0,
                'max': recall_scores.max() if len(recall_scores) > 0 else 0,
            },
            'faithfulness': {
                'mean': faith_scores.mean() if len(faith_scores) > 0 else 0,
                'median': faith_scores.median() if len(faith_scores) > 0 else 0,
                'std': faith_scores.std() if len(faith_scores) > 0 else 0,
                'min': faith_scores.min() if len(faith_scores) > 0 else 0,
                'max': faith_scores.max() if len(faith_scores) > 0 else 0,
            }
        }
        
        # ì‹¤í–‰ ì‹œê°„ í†µê³„ (ìˆëŠ” ê²½ìš°)
        if 'execution_time' in self.metrics_df.columns:
            exec_times = self.metrics_df['execution_time'].dropna()
            stats['execution_time'] = {
                'mean': exec_times.mean() if len(exec_times) > 0 else 0,
                'median': exec_times.median() if len(exec_times) > 0 else 0,
                'std': exec_times.std() if len(exec_times) > 0 else 0,
                'total': exec_times.sum() if len(exec_times) > 0 else 0,
            }
        
        return stats
    
    def analyze_by_intent(self) -> Dict[str, Any]:
        """Intentë³„ ì„±ëŠ¥ ë¶„ì„."""
        if self.metrics_df is None or 'intent' not in self.metrics_df.columns:
            return {}
            
        intent_stats = self.metrics_df.groupby('intent').agg({
            'recall_at_5': ['mean', 'std', 'count'],
            'faithfulness_simple': ['mean', 'std'],
        }).round(3)
        
        # ì»¬ëŸ¼ëª… ì •ë¦¬
        intent_stats.columns = ['Recall@5_Mean', 'Recall@5_Std', 'Count', 'Faithfulness_Mean', 'Faithfulness_Std']
        
        # Intentë³„ ìƒì„¸ ë¶„ì„
        intent_analysis = {}
        for intent in self.metrics_df['intent'].dropna().unique():
            intent_data = self.metrics_df[self.metrics_df['intent'] == intent]
            
            intent_analysis[intent] = {
                'count': len(intent_data),
                'recall_at_5_mean': intent_data['recall_at_5'].mean(),
                'faithfulness_mean': intent_data['faithfulness_simple'].mean(),
                'performance_grade': self._calculate_performance_grade(
                    intent_data['recall_at_5'].mean(),
                    intent_data['faithfulness_simple'].mean()
                )
            }
        
        return {
            'summary_table': intent_stats,
            'detailed_analysis': intent_analysis
        }
    
    def analyze_by_difficulty(self) -> Dict[str, Any]:
        """ë‚œì´ë„ë³„ ì„±ëŠ¥ ë¶„ì„."""
        if self.metrics_df is None or 'difficulty' not in self.metrics_df.columns:
            return {}
            
        difficulty_stats = self.metrics_df.groupby('difficulty').agg({
            'recall_at_5': ['mean', 'std', 'count'],
            'faithfulness_simple': ['mean', 'std'],
        }).round(3)
        
        difficulty_stats.columns = ['Recall@5_Mean', 'Recall@5_Std', 'Count', 'Faithfulness_Mean', 'Faithfulness_Std']
        
        difficulty_analysis = {}
        for difficulty in self.metrics_df['difficulty'].dropna().unique():
            difficulty_data = self.metrics_df[self.metrics_df['difficulty'] == difficulty]
            
            difficulty_analysis[difficulty] = {
                'count': len(difficulty_data),
                'recall_at_5_mean': difficulty_data['recall_at_5'].mean(),
                'faithfulness_mean': difficulty_data['faithfulness_simple'].mean(),
                'performance_grade': self._calculate_performance_grade(
                    difficulty_data['recall_at_5'].mean(),
                    difficulty_data['faithfulness_simple'].mean()
                )
            }
        
        return {
            'summary_table': difficulty_stats,
            'detailed_analysis': difficulty_analysis
        }
    
    def analyze_by_category(self) -> Dict[str, Any]:
        """ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„."""
        if self.metrics_df is None or 'category' not in self.metrics_df.columns:
            return {}
            
        category_stats = self.metrics_df.groupby('category').agg({
            'recall_at_5': ['mean', 'std', 'count'],
            'faithfulness_simple': ['mean', 'std'],
        }).round(3)
        
        category_stats.columns = ['Recall@5_Mean', 'Recall@5_Std', 'Count', 'Faithfulness_Mean', 'Faithfulness_Std']
        
        category_analysis = {}
        for category in self.metrics_df['category'].dropna().unique():
            category_data = self.metrics_df[self.metrics_df['category'] == category]
            
            category_analysis[category] = {
                'count': len(category_data),
                'recall_at_5_mean': category_data['recall_at_5'].mean(),
                'faithfulness_mean': category_data['faithfulness_simple'].mean(),
                'performance_grade': self._calculate_performance_grade(
                    category_data['recall_at_5'].mean(),
                    category_data['faithfulness_simple'].mean()
                )
            }
        
        return {
            'summary_table': category_stats,
            'detailed_analysis': category_analysis
        }
    
    def _calculate_performance_grade(self, recall: float, faithfulness: float) -> str:
        """ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°."""
        if pd.isna(recall) or pd.isna(faithfulness):
            return "N/A"
            
        avg_score = (recall + faithfulness) / 2
        
        if avg_score >= 0.8:
            return "ìš°ìˆ˜"
        elif avg_score >= 0.6:
            return "ì–‘í˜¸"
        elif avg_score >= 0.4:
            return "ë³´í†µ"
        else:
            return "ê°œì„ í•„ìš”"
    
    def identify_issues(self) -> Dict[str, Any]:
        """ë¬¸ì œì  ì‹ë³„ ë° ë¶„ì„."""
        if self.metrics_df is None:
            return {}
            
        issues = {
            'low_faithfulness': [],
            'low_recall': [],
            'failed_questions': [],
            'slow_execution': [],
            'poor_answers': [],
            'context_issues': []
        }
        
        # ë‚®ì€ Faithfulness ì‹ë³„ (ë” ì—„ê²©í•œ ê¸°ì¤€)
        low_faith_threshold = 0.2  # 0.1ì—ì„œ 0.2ë¡œ ìƒí–¥ ì¡°ì •
        low_faith = self.metrics_df[self.metrics_df['faithfulness_simple'] < low_faith_threshold]
        for _, row in low_faith.iterrows():
            issues['low_faithfulness'].append({
                'id': row.get('id', 'unknown'),
                'question': row.get('question', ''),
                'faithfulness': row['faithfulness_simple'],
                'intent': row.get('intent', 'unknown')
            })
        
        # ë‚®ì€ Recall ì‹ë³„
        low_recall_threshold = 0.8  # 0.5ì—ì„œ 0.8ë¡œ ìƒí–¥ ì¡°ì • (í˜„ì¬ëŠ” ëª¨ë‘ 1.0ì´ë¯€ë¡œ)
        low_recall = self.metrics_df[self.metrics_df['recall_at_5'] < low_recall_threshold]
        for _, row in low_recall.iterrows():
            issues['low_recall'].append({
                'id': row.get('id', 'unknown'),
                'question': row.get('question', ''),
                'recall': row['recall_at_5'],
                'intent': row.get('intent', 'unknown')
            })
        
        # ì‹¤íŒ¨í•œ ì§ˆë¬¸ ì‹ë³„
        if 'error' in self.metrics_df.columns:
            failed = self.metrics_df[self.metrics_df['error'].notna()]
            for _, row in failed.iterrows():
                issues['failed_questions'].append({
                    'id': row.get('id', 'unknown'),
                    'question': row.get('question', ''),
                    'error': row['error']
                })
        
        # ëŠë¦° ì‹¤í–‰ ì‹œê°„ ì‹ë³„
        if 'execution_time' in self.metrics_df.columns:
            exec_times = self.metrics_df['execution_time'].dropna()
            if len(exec_times) > 0:
                slow_threshold = exec_times.quantile(0.9)  # ìƒìœ„ 10%
                slow_exec = self.metrics_df[self.metrics_df['execution_time'] > slow_threshold]
                for _, row in slow_exec.iterrows():
                    issues['slow_execution'].append({
                        'id': row.get('id', 'unknown'),
                        'question': row.get('question', ''),
                        'execution_time': row['execution_time'],
                        'intent': row.get('intent', 'unknown')
                    })
        
        # ë‹µë³€ í’ˆì§ˆ ë¬¸ì œ ì‹ë³„ (raw ë°ì´í„°ì—ì„œ)
        if self.raw_data:
            for item in self.raw_data:
                ragas_item = item.get('ragas_item', {})
                answer = ragas_item.get('answer', '')
                
                # ì˜ë¯¸ì—†ëŠ” ë‹µë³€ íŒ¨í„´ ê°ì§€
                meaningless_patterns = [
                    'ì§ˆë¬¸ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤',
                    'ì§ˆë¬¸ì„ ì´í•´í–ˆìŠµë‹ˆë‹¤',
                    'ì§ˆë¬¸ì„ ë°›ì•˜ìŠµë‹ˆë‹¤',
                    'ì§ˆë¬¸ì„ íŒŒì•…í–ˆìŠµë‹ˆë‹¤',
                    'ì§ˆë¬¸ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤',
                    'ì§ˆë¬¸ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤',
                    'ì§ˆë¬¸ì„ ê²€í† í–ˆìŠµë‹ˆë‹¤',
                    'ì§ˆë¬¸ì„ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤',
                    'ì§ˆë¬¸ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤',
                    'ì§ˆë¬¸ì„ ê²€í† í•˜ê² ìŠµë‹ˆë‹¤'
                ]
                
                if any(pattern in answer for pattern in meaningless_patterns):
                    issues['poor_answers'].append({
                        'id': item.get('id', 'unknown'),
                        'question': ragas_item.get('question', ''),
                        'answer': answer[:100] + '...' if len(answer) > 100 else answer,
                        'intent': item.get('intent', 'unknown'),
                        'issue_type': 'meaningless_response'
                    })
                
                # ë„ˆë¬´ ì§§ì€ ë‹µë³€
                if len(answer.strip()) < 20:
                    issues['poor_answers'].append({
                        'id': item.get('id', 'unknown'),
                        'question': ragas_item.get('question', ''),
                        'answer': answer,
                        'intent': item.get('intent', 'unknown'),
                        'issue_type': 'too_short'
                    })
                
                # ì»¨í…ìŠ¤íŠ¸ ë¬¸ì œ ë¶„ì„
                contexts = ragas_item.get('contexts', [])
                if contexts:
                    # ë„ˆë¬´ ì§§ì€ ì»¨í…ìŠ¤íŠ¸
                    short_contexts = [ctx for ctx in contexts if len(ctx.strip()) < 10]
                    if len(short_contexts) > len(contexts) * 0.5:  # 50% ì´ìƒì´ ì§§ì€ ê²½ìš°
                        issues['context_issues'].append({
                            'id': item.get('id', 'unknown'),
                            'question': ragas_item.get('question', ''),
                            'context_count': len(contexts),
                            'short_context_count': len(short_contexts),
                            'intent': item.get('intent', 'unknown'),
                            'issue_type': 'short_contexts'
                        })
                    
                    # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš°
                    if not contexts or all(not ctx.strip() for ctx in contexts):
                        issues['context_issues'].append({
                            'id': item.get('id', 'unknown'),
                            'question': ragas_item.get('question', ''),
                            'context_count': len(contexts),
                            'intent': item.get('intent', 'unknown'),
                            'issue_type': 'no_context'
                        })
        
        return issues
    
    def generate_recommendations(self) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±."""
        if self.metrics_df is None:
            return []
            
        recommendations = []
        issues = self.identify_issues()
        overall_stats = self.calculate_overall_statistics()
        
        # Faithfulness ë¬¸ì œ ë¶„ì„ ë° ì œì•ˆ
        if overall_stats['faithfulness']['mean'] < 0.2:
            recommendations.extend([
                "ğŸš¨ ì‹¬ê°í•œ Faithfulness ë¬¸ì œ ë°œê²¬:",
                "  - í˜„ì¬ í‰ê·  Faithfulnessê°€ ë§¤ìš° ë‚®ìŒ (0.2 ë¯¸ë§Œ)",
                "  - ë‹µë³€ì´ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì™€ ê±°ì˜ ì—°ê´€ì„±ì´ ì—†ìŒ",
                "  - ì¦‰ì‹œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°œì„  í•„ìš”"
            ])
        
        # ë‹µë³€ í’ˆì§ˆ ë¬¸ì œ ë¶„ì„
        if issues['poor_answers']:
            meaningless_count = len([p for p in issues['poor_answers'] if p.get('issue_type') == 'meaningless_response'])
            short_count = len([p for p in issues['poor_answers'] if p.get('issue_type') == 'too_short'])
            
            if meaningless_count > 0:
                recommendations.extend([
                    f"ğŸ’¬ ë‹µë³€ í’ˆì§ˆ ë¬¸ì œ ({meaningless_count}ê°œ ì§ˆë¬¸):",
                    "  - 'ì§ˆë¬¸ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤' ê°™ì€ ì˜ë¯¸ì—†ëŠ” ë‹µë³€ ìƒì„±",
                    "  - ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ ì „ë©´ ê°œì„  í•„ìš”",
                    "  - ì»¨í…ìŠ¤íŠ¸ í™œìš©í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •"
                ])
            
            if short_count > 0:
                recommendations.extend([
                    f"ğŸ“ ë„ˆë¬´ ì§§ì€ ë‹µë³€ ({short_count}ê°œ ì§ˆë¬¸):",
                    "  - 20ì ë¯¸ë§Œì˜ ë¶ˆì¶©ë¶„í•œ ë‹µë³€",
                    "  - ìµœì†Œ ë‹µë³€ ê¸¸ì´ ìš”êµ¬ì‚¬í•­ ì¶”ê°€",
                    "  - ë‹µë³€ ìƒì„± ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¡°ì •"
                ])
        
        # ì»¨í…ìŠ¤íŠ¸ ë¬¸ì œ ë¶„ì„
        if issues['context_issues']:
            no_context_count = len([c for c in issues['context_issues'] if c.get('issue_type') == 'no_context'])
            short_context_count = len([c for c in issues['context_issues'] if c.get('issue_type') == 'short_contexts'])
            
            if no_context_count > 0:
                recommendations.extend([
                    f"ğŸ“„ ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡± ({no_context_count}ê°œ ì§ˆë¬¸):",
                    "  - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìŒ",
                    "  - ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì ê²€ ë° ë¬¸ì„œ ì¸ë±ì‹± í™•ì¸",
                    "  - ê²€ìƒ‰ ì¿¼ë¦¬ ì „ì²˜ë¦¬ ê°œì„ "
                ])
            
            if short_context_count > 0:
                recommendations.extend([
                    f"ğŸ“ ì§§ì€ ì»¨í…ìŠ¤íŠ¸ ({short_context_count}ê°œ ì§ˆë¬¸):",
                    "  - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ (10ì ë¯¸ë§Œ)",
                    "  - ë¬¸ì„œ ì²­í‚¹ ì „ëµ ê°œì„  í•„ìš”",
                    "  - ìµœì†Œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ë³´ì¥"
                ])
        
        # Recall ì„±ëŠ¥ ë¶„ì„
        if overall_stats['recall_at_5']['mean'] < 1.0:
            recommendations.extend([
                "ğŸ” Recall@5 ê°œì„  í•„ìš”:",
                "  - ì¼ë¶€ ì§ˆë¬¸ì—ì„œ ì •ë‹µ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•¨",
                "  - ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ íŠœë‹ ë° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ì¡°ì •",
                "  - ë¬¸ì„œ ì²­í‚¹ ì „ëµ ê°œì„  ë° ì„ë² ë”© ëª¨ë¸ ìµœì í™”"
            ])
        else:
            recommendations.append("âœ… Recall@5ëŠ” ì–‘í˜¸í•¨ (ëª¨ë“  ì§ˆë¬¸ì—ì„œ ì •ë‹µ ë¬¸ì„œ ê²€ìƒ‰ ì„±ê³µ)")
        
        # Intentë³„ ì œì•ˆ
        intent_analysis = self.analyze_by_intent()
        for intent, analysis in intent_analysis.get('detailed_analysis', {}).items():
            if analysis['performance_grade'] in ['ë³´í†µ', 'ê°œì„ í•„ìš”']:
                recommendations.append(f"ğŸ¯ {intent} Intent ê°œì„ : í˜„ì¬ ì„±ëŠ¥ ë“±ê¸‰ '{analysis['performance_grade']}'")
        
        # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì œì•ˆ
        recommendations.extend([
            "\nğŸ¯ ìš°ì„ ìˆœìœ„ë³„ ê°œì„  ë°©ì•ˆ:",
            "  1ìˆœìœ„: ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ ì „ë©´ ê°œì„  (Faithfulness 0.0 ë¬¸ì œ í•´ê²°)",
            "  2ìˆœìœ„: ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ í–¥ìƒ (ë¬¸ì„œ ì²­í‚¹ ë° ê²€ìƒ‰ ê°œì„ )",
            "  3ìˆœìœ„: ë‹µë³€ ê¸¸ì´ ë° í’ˆì§ˆ ë³´ì¥ (ìµœì†Œ ìš”êµ¬ì‚¬í•­ ì„¤ì •)",
            "  4ìˆœìœ„: Intentë³„ ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸ ê°œë°œ"
        ])
        
        # êµ¬ì²´ì ì¸ ê¸°ìˆ ì  ì œì•ˆ
        recommendations.extend([
            "\nğŸ”§ êµ¬ì²´ì ì¸ ê¸°ìˆ ì  ê°œì„ ì‚¬í•­:",
            "  - QA í”„ë¡¬í”„íŠ¸ì— 'ë°˜ë“œì‹œ ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ë¼' ì¶”ê°€",
            "  - ë‹µë³€ ìƒì„± ì‹œ ì»¨í…ìŠ¤íŠ¸ í™œìš©ë„ ê²€ì¦ ë¡œì§ ì¶”ê°€",
            "  - ì˜ë¯¸ì—†ëŠ” ë‹µë³€ íŒ¨í„´ ê°ì§€ ë° ì¬ìƒì„± ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„",
            "  - ìµœì†Œ ë‹µë³€ ê¸¸ì´ ì œí•œ (ì˜ˆ: 50ì ì´ìƒ) ì„¤ì •"
        ])
        
        return recommendations
    
    def print_analysis_report(self) -> None:
        """ë¶„ì„ ë¦¬í¬íŠ¸ ì½˜ì†” ì¶œë ¥."""
        if not self.load_data() or not self.validate_data():
            print("âŒ ë°ì´í„° ë¡œë“œ ë˜ëŠ” ê²€ì¦ ì‹¤íŒ¨")
            return
        
        # ë¶„ì„ ì‹¤í–‰
        overall_stats = self.calculate_overall_statistics()
        intent_analysis = self.analyze_by_intent()
        difficulty_analysis = self.analyze_by_difficulty()
        category_analysis = self.analyze_by_category()
        issues = self.identify_issues()
        recommendations = self.generate_recommendations()
        
        # ë¦¬í¬íŠ¸ ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ“Š RAG ì‹œìŠ¤í…œ í‰ê°€ ê²°ê³¼ ë¶„ì„ ë¦¬í¬íŠ¸")
        print("="*80)
        print(f"ğŸ“… ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ì „ì²´ ì„±ëŠ¥
        print(f"\nğŸ¯ ì „ì²´ ì„±ëŠ¥ ìš”ì•½:")
        print(f"  ğŸ“Š ì´ ì§ˆë¬¸ ìˆ˜: {overall_stats['total_questions']}")
        print(f"  âœ… ì„±ê³µí•œ ì§ˆë¬¸: {overall_stats['successful_questions']} ({overall_stats['success_rate']*100:.1f}%)")
        print(f"  ğŸ¯ í‰ê·  Recall@5: {overall_stats['recall_at_5']['mean']:.3f} Â± {overall_stats['recall_at_5']['std']:.3f}")
        print(f"  ğŸ” í‰ê·  Faithfulness: {overall_stats['faithfulness']['mean']:.3f} Â± {overall_stats['faithfulness']['std']:.3f}")
        
        if 'execution_time' in overall_stats:
            print(f"  â±ï¸  í‰ê·  ì‹¤í–‰ ì‹œê°„: {overall_stats['execution_time']['mean']:.2f}ì´ˆ")
            print(f"  â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {overall_stats['execution_time']['total']:.2f}ì´ˆ")
        
        # Intentë³„ ì„±ëŠ¥
        if intent_analysis:
            print(f"\nğŸ“‹ Intentë³„ ì„±ëŠ¥ ë¶„ì„:")
            for intent, analysis in intent_analysis['detailed_analysis'].items():
                grade_emoji = {"ìš°ìˆ˜": "ğŸŸ¢", "ì–‘í˜¸": "ğŸŸ¡", "ë³´í†µ": "ğŸŸ ", "ê°œì„ í•„ìš”": "ğŸ”´"}.get(analysis['performance_grade'], "âšª")
                print(f"  {grade_emoji} {intent}: {analysis['count']}ê°œ ì§ˆë¬¸, "
                      f"Recall@5 {analysis['recall_at_5_mean']:.3f}, "
                      f"Faithfulness {analysis['faithfulness_mean']:.3f} "
                      f"({analysis['performance_grade']})")
        
        # ë‚œì´ë„ë³„ ì„±ëŠ¥
        if difficulty_analysis:
            print(f"\nğŸ¯ ë‚œì´ë„ë³„ ì„±ëŠ¥ ë¶„ì„:")
            for difficulty, analysis in difficulty_analysis['detailed_analysis'].items():
                grade_emoji = {"ìš°ìˆ˜": "ğŸŸ¢", "ì–‘í˜¸": "ğŸŸ¡", "ë³´í†µ": "ğŸŸ ", "ê°œì„ í•„ìš”": "ğŸ”´"}.get(analysis['performance_grade'], "âšª")
                difficulty_emoji = {"easy": "ğŸŸ¢", "medium": "ğŸŸ¡", "hard": "ğŸ”´"}.get(difficulty, "âšª")
                print(f"  {difficulty_emoji} {difficulty}: {analysis['count']}ê°œ ì§ˆë¬¸, "
                      f"Recall@5 {analysis['recall_at_5_mean']:.3f}, "
                      f"Faithfulness {analysis['faithfulness_mean']:.3f} "
                      f"({analysis['performance_grade']})")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥
        if category_analysis:
            print(f"\nğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„:")
            for category, analysis in category_analysis['detailed_analysis'].items():
                grade_emoji = {"ìš°ìˆ˜": "ğŸŸ¢", "ì–‘í˜¸": "ğŸŸ¡", "ë³´í†µ": "ğŸŸ ", "ê°œì„ í•„ìš”": "ğŸ”´"}.get(analysis['performance_grade'], "âšª")
                print(f"  {grade_emoji} {category}: {analysis['count']}ê°œ ì§ˆë¬¸, "
                      f"Recall@5 {analysis['recall_at_5_mean']:.3f}, "
                      f"Faithfulness {analysis['faithfulness_mean']:.3f} "
                      f"({analysis['performance_grade']})")
    
        # ë¬¸ì œì  ë¶„ì„
        print(f"\nâš ï¸ ë¬¸ì œì  ë¶„ì„:")
        if issues['low_faithfulness']:
            print(f"  ğŸ” ë‚®ì€ Faithfulness ({len(issues['low_faithfulness'])}ê°œ):")
            for issue in issues['low_faithfulness'][:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                print(f"    * {issue['id']}: {issue['faithfulness']:.3f} ({issue['intent']})")
        
        if issues['low_recall']:
            print(f"  ğŸ¯ ë‚®ì€ Recall@5 ({len(issues['low_recall'])}ê°œ):")
            for issue in issues['low_recall'][:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                print(f"    * {issue['id']}: {issue['recall']:.3f} ({issue['intent']})")
        
        if issues['failed_questions']:
            print(f"  âŒ ì‹¤íŒ¨í•œ ì§ˆë¬¸ ({len(issues['failed_questions'])}ê°œ):")
            for issue in issues['failed_questions'][:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                print(f"    * {issue['id']}: {issue['error'][:50]}...")
        
        # ë‹µë³€ í’ˆì§ˆ ë¬¸ì œ
        if issues['poor_answers']:
            print(f"  ğŸ’¬ ë‹µë³€ í’ˆì§ˆ ë¬¸ì œ ({len(issues['poor_answers'])}ê°œ):")
            meaningless = [p for p in issues['poor_answers'] if p.get('issue_type') == 'meaningless_response']
            short_answers = [p for p in issues['poor_answers'] if p.get('issue_type') == 'too_short']
            
            if meaningless:
                print(f"    - ì˜ë¯¸ì—†ëŠ” ë‹µë³€: {len(meaningless)}ê°œ")
                for issue in meaningless[:3]:
                    print(f"      * {issue['id']}: '{issue['answer'][:30]}...' ({issue['intent']})")
            
            if short_answers:
                print(f"    - ë„ˆë¬´ ì§§ì€ ë‹µë³€: {len(short_answers)}ê°œ")
                for issue in short_answers[:3]:
                    print(f"      * {issue['id']}: '{issue['answer']}' ({issue['intent']})")
        
        # ì»¨í…ìŠ¤íŠ¸ ë¬¸ì œ
        if issues['context_issues']:
            print(f"  ğŸ“„ ì»¨í…ìŠ¤íŠ¸ ë¬¸ì œ ({len(issues['context_issues'])}ê°œ):")
            no_context = [c for c in issues['context_issues'] if c.get('issue_type') == 'no_context']
            short_context = [c for c in issues['context_issues'] if c.get('issue_type') == 'short_contexts']
            
            if no_context:
                print(f"    - ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ: {len(no_context)}ê°œ")
                for issue in no_context[:3]:
                    print(f"      * {issue['id']}: {issue['question'][:30]}... ({issue['intent']})")
            
            if short_context:
                print(f"    - ì§§ì€ ì»¨í…ìŠ¤íŠ¸: {len(short_context)}ê°œ")
                for issue in short_context[:3]:
                    print(f"      * {issue['id']}: {issue['short_context_count']}/{issue['context_count']}ê°œ ì§§ìŒ ({issue['intent']})")
    
        # ê°œì„  ì œì•ˆ
        print(f"\nğŸ’¡ ê°œì„  ì œì•ˆ:")
        for recommendation in recommendations:
            print(f"  {recommendation}")
        
        print("\n" + "="*80)
    
    def export_to_html(self, output_path: str = "eval/out/analysis_report.html") -> None:
        """HTML í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ë¦¬í¬íŠ¸ ë‚´ë³´ë‚´ê¸°."""
        # TODO: HTML ë¦¬í¬íŠ¸ ìƒì„± ê¸°ëŠ¥ êµ¬í˜„
        logger.info(f"HTML ë¦¬í¬íŠ¸ ë‚´ë³´ë‚´ê¸°: {output_path}")
        pass
    
    def export_to_json(self, output_path: str = "eval/out/analysis_results.json") -> None:
        """JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°."""
        if not self.load_data() or not self.validate_data():
            return
        
        # DataFrameì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        intent_analysis = self.analyze_by_intent()
        if 'summary_table' in intent_analysis:
            # DataFrameì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            intent_analysis['summary_table'] = intent_analysis['summary_table'].to_dict('index')
        
        difficulty_analysis = self.analyze_by_difficulty()
        if 'summary_table' in difficulty_analysis:
            # DataFrameì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            difficulty_analysis['summary_table'] = difficulty_analysis['summary_table'].to_dict('index')
        
        category_analysis = self.analyze_by_category()
        if 'summary_table' in category_analysis:
            # DataFrameì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            category_analysis['summary_table'] = category_analysis['summary_table'].to_dict('index')
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'overall_statistics': self.calculate_overall_statistics(),
            'intent_analysis': intent_analysis,
            'difficulty_analysis': difficulty_analysis,
            'category_analysis': category_analysis,
            'issues': self.identify_issues(),
            'recommendations': self.generate_recommendations()
        }
        
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ë¶„ì„ ê²°ê³¼ JSON ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_file}")

def main():
    """ë©”ì¸ ë¶„ì„ í•¨ìˆ˜."""
    analyzer = EvaluationAnalyzer()
    analyzer.print_analysis_report()
    analyzer.export_to_json()

if __name__ == "__main__":
    main()
