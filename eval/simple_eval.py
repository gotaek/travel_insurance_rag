#!/usr/bin/env python3
"""
ê¸°ë³¸ì ì¸ RAG ì‹œìŠ¤í…œ í‰ê°€ ë„êµ¬
RAGAS ì—†ì´ ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ í‰ê°€ ì‹œìŠ¤í…œ
"""

import json
import logging
import time
import re
from pathlib import Path
from typing import Dict, Any, List, Optional
import sys
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from graph.builder import build_graph

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ê²½ë¡œ ì„¤ì •
QUESTIONS_PATH = Path("eval/questions.jsonl")
OUTPUT_DIR = Path("eval/out")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


def load_questions() -> List[Dict[str, Any]]:
    """í‰ê°€ ì§ˆë¬¸ë“¤ì„ JSONL íŒŒì¼ì—ì„œ ë¡œë“œ."""
    if not QUESTIONS_PATH.exists():
        raise FileNotFoundError(f"í‰ê°€ ì§ˆë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {QUESTIONS_PATH}")
    
    questions = []
    for line_num, line in enumerate(QUESTIONS_PATH.read_text(encoding="utf-8").splitlines(), 1):
        line = line.strip()
        if not line or line.startswith('//'):
            continue
        
        try:
            question_data = json.loads(line)
            if isinstance(question_data, dict):
                questions.append(question_data)
        except json.JSONDecodeError as e:
            logger.error(f"ë¼ì¸ {line_num} JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            continue
    
    logger.info(f"ë¡œë“œëœ í‰ê°€ ì§ˆë¬¸: {len(questions)}ê°œ")
    return questions


def run_rag_system(graph, question: str) -> Dict[str, Any]:
    """
    RAG ì‹œìŠ¤í…œì„ ì‹¤í–‰í•˜ì—¬ ë‹µë³€ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±.
    
    Args:
        graph: RAG ì‹œìŠ¤í…œ ê·¸ë˜í”„
        question: ì§ˆë¬¸
        
    Returns:
        RAG ì‹œìŠ¤í…œ ì‹¤í–‰ ê²°ê³¼
    """
    try:
        start_time = time.time()
        state = {"question": question}
        result = graph.invoke(state)
        end_time = time.time()
        
        # ë‹µë³€ ì¶”ì¶œ
        draft_answer = result.get("draft_answer", {})
        answer_text = ""
        if isinstance(draft_answer, dict):
            answer_text = draft_answer.get("conclusion", "")
        
        # ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        refined_docs = result.get("refined", [])
        contexts = []
        for doc in refined_docs:
            if isinstance(doc, dict) and doc.get("text"):
                contexts.append(doc["text"])
        
        return {
            "answer": answer_text,
            "contexts": contexts,
            "response_time": end_time - start_time,
            "success": True,
            "raw_result": result
        }
        
    except Exception as e:
        logger.error(f"RAG ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "answer": "",
            "contexts": [],
            "response_time": 0,
            "success": False,
            "error": str(e)
        }


def calculate_recall_at_k_direct(ground_truths: List[str], contexts: List[str], k: int) -> float:
    """
    íŠ¹ì • k ê°’ì— ëŒ€í•œ Recall@K ì§ì ‘ ê³„ì‚° (ì¬ê·€ ì—†ìŒ).
    ê°œì„ ëœ ìœ ì—°í•œ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©.
    
    Args:
        ground_truths: ì •ë‹µ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        contexts: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        k: ìƒìœ„ Kê°œ ì»¨í…ìŠ¤íŠ¸ë§Œ ê³ ë ¤
        
    Returns:
        Recall@K ì ìˆ˜
    """
    if not ground_truths or not contexts:
        return 0.0
    
    # ìƒìœ„ kê°œ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
    top_k_contexts = contexts[:k]
    
    # ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°
    combined_context = " ".join(top_k_contexts).lower()
    
    # ì •ë‹µ í‚¤ì›Œë“œì™€ ë§¤ì¹­ í™•ì¸
    matched_ground_truths = []
    
    for ground_truth in ground_truths:
        # í‚¤ì›Œë“œë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ê³  íŠ¹ìˆ˜ë¬¸ì ì œê±°
        clean_ground_truth = re.sub(r'[^\w\s]', '', ground_truth.lower())
        clean_ground_truth = re.sub(r'\s+', ' ', clean_ground_truth).strip()
        
        # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
        ground_truth_words = clean_ground_truth.split()
        
        # ìœ ì—°í•œ ë§¤ì¹­ì„ ìœ„í•œ í‚¤ì›Œë“œ ë§¤í•‘
        keyword_mappings = {
            "í•­ê³µê¸°": ["í•­ê³µí¸", "í•­ê³µ", "ë¹„í–‰ê¸°"],
            "ì¶œë°œ": ["ì¶œêµ­", "ì´ë¥™"],
            "ì§€ì—°": ["ì—°ì°©", "ì§€ì—°"],
            "ë³´ìƒ": ["ë³´ì¥", "ì§€ê¸‰", "ë³´í—˜ê¸ˆ"],
            "ê¸ˆì•¡": ["ë¹„ìš©", "í•œë„", "ê¸ˆì•¡"],
            "ì‹œê°„": ["ì‹œê°„", "ë¶„", "ì‹œ"],
            "íƒ‘ìŠ¹ê¶Œ": ["í•­ê³µê¶Œ", "í‹°ì¼“", "ìŠ¹ì°¨ê¶Œ"],
            "í™•ì¸ì„œ": ["ì¦ëª…ì„œ", "ì„œë¥˜", "ë¬¸ì„œ"],
            "ì œì¶œ": ["ì œì¶œ", "ì œì‹œ", "ê°–ì¶¤"]
        }
        
        # ê° ë‹¨ì–´ê°€ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸ (ìœ ì—°í•œ ë§¤ì¹­ í¬í•¨)
        word_matches = 0
        for word in ground_truth_words:
            if len(word) > 1:  # 1ê¸€ì ì´ìƒ ë‹¨ì–´ ê³ ë ¤
                # ì§ì ‘ ë§¤ì¹­
                if word in combined_context:
                    word_matches += 1
                # ìœ ì—°í•œ ë§¤ì¹­ (ë™ì˜ì–´/ìœ ì‚¬ì–´)
                elif word in keyword_mappings:
                    synonyms = keyword_mappings[word]
                    for synonym in synonyms:
                        if synonym in combined_context:
                            word_matches += 1
                            break
        
        # 30% ì´ìƒì˜ ë‹¨ì–´ê°€ ë§¤ì¹­ë˜ë©´ í•´ë‹¹ ground_truthëŠ” ë§¤ì¹­ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼ (ì„ê³„ê°’ ë‚®ì¶¤)
        if word_matches >= len(ground_truth_words) * 0.3:
            matched_ground_truths.append(ground_truth)
    
    # Recall@K ê³„ì‚°
    total_ground_truths = len(ground_truths)
    matched_count = len(matched_ground_truths)
    
    return matched_count / total_ground_truths if total_ground_truths > 0 else 0.0


def calculate_recall_at_k(ground_truths: List[str], contexts: List[str], k: int = 5) -> Dict[str, Any]:
    """
    Recall@K ë©”íŠ¸ë¦­ ê³„ì‚°.
    
    Args:
        ground_truths: ì •ë‹µ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        contexts: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        k: ìƒìœ„ Kê°œ ì»¨í…ìŠ¤íŠ¸ë§Œ ê³ ë ¤
        
    Returns:
        Recall@K ë©”íŠ¸ë¦­
    """
    if not ground_truths or not contexts:
        return {
            "recall_at_k": 0.0,
            "recall_at_1": 0.0,
            "recall_at_3": 0.0,
            "recall_at_5": 0.0,
            "matched_ground_truths": [],
            "total_ground_truths": len(ground_truths)
        }
    
    # ìƒìœ„ kê°œ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
    top_k_contexts = contexts[:k]
    
    # ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°
    combined_context = " ".join(top_k_contexts).lower()
    
    # ì •ë‹µ í‚¤ì›Œë“œì™€ ë§¤ì¹­ í™•ì¸
    matched_ground_truths = []
    
    for ground_truth in ground_truths:
        # í‚¤ì›Œë“œë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ê³  íŠ¹ìˆ˜ë¬¸ì ì œê±°
        clean_ground_truth = re.sub(r'[^\w\s]', '', ground_truth.lower())
        clean_ground_truth = re.sub(r'\s+', ' ', clean_ground_truth).strip()
        
        # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
        ground_truth_words = clean_ground_truth.split()
        
        # ê° ë‹¨ì–´ê°€ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
        word_matches = 0
        for word in ground_truth_words:
            if len(word) > 2 and word in combined_context:  # 2ê¸€ì ì´ìƒ ë‹¨ì–´ë§Œ ê³ ë ¤
                word_matches += 1
        
        # 50% ì´ìƒì˜ ë‹¨ì–´ê°€ ë§¤ì¹­ë˜ë©´ í•´ë‹¹ ground_truthëŠ” ë§¤ì¹­ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
        if word_matches >= len(ground_truth_words) * 0.5:
            matched_ground_truths.append(ground_truth)
    
    # Recall@K ê³„ì‚°
    total_ground_truths = len(ground_truths)
    matched_count = len(matched_ground_truths)
    
    recall_at_k = matched_count / total_ground_truths if total_ground_truths > 0 else 0.0
    
    # ë‹¤ì–‘í•œ k ê°’ì— ëŒ€í•œ recall ê³„ì‚° (ì¬ê·€ í˜¸ì¶œ ëŒ€ì‹  ì§ì ‘ ê³„ì‚°)
    recall_at_1 = calculate_recall_at_k_direct(ground_truths, contexts, 1)
    recall_at_3 = calculate_recall_at_k_direct(ground_truths, contexts, 3)
    recall_at_5 = recall_at_k  # ì´ë¯¸ k=5ë¡œ ê³„ì‚°ë¨
    
    return {
        "recall_at_k": recall_at_k,
        "recall_at_1": recall_at_1,
        "recall_at_3": recall_at_3,
        "recall_at_5": recall_at_5,
        "matched_ground_truths": matched_ground_truths,
        "total_ground_truths": total_ground_truths,
        "matched_count": matched_count
    }


def calculate_basic_metrics(question_data: Dict[str, Any], rag_result: Dict[str, Any]) -> Dict[str, Any]:
    """
    ê¸°ë³¸ì ì¸ í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (Recall@K í¬í•¨).
    
    Args:
        question_data: ì§ˆë¬¸ ë°ì´í„°
        rag_result: RAG ì‹œìŠ¤í…œ ì‹¤í–‰ ê²°ê³¼
        
    Returns:
        í‰ê°€ ë©”íŠ¸ë¦­
    """
    metrics = {
        "question_id": question_data.get("id", "unknown"),
        "question": question_data.get("question", ""),
        "category": question_data.get("category", "unknown"),
        "intent": question_data.get("intent", "unknown"),
        "response_time": rag_result.get("response_time", 0),
        "success": rag_result.get("success", False),
        "answer_length": len(rag_result.get("answer", "")),
        "context_count": len(rag_result.get("contexts", [])),
        "context_length": sum(len(ctx) for ctx in rag_result.get("contexts", [])),
    }
    
    # Recall@K ë©”íŠ¸ë¦­ ê³„ì‚°
    ground_truths = question_data.get("ground_truths", [])
    contexts = rag_result.get("contexts", [])
    
    recall_metrics = calculate_recall_at_k(ground_truths, contexts, k=5)
    metrics.update(recall_metrics)
    
    # ê¸°ì¡´ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ (ë‹µë³€ ê¸°ë°˜)
    answer = rag_result.get("answer", "").lower()
    
    if ground_truths and answer:
        matched_keywords = 0
        total_keywords = len(ground_truths)
        
        for keyword in ground_truths:
            # í‚¤ì›Œë“œë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ê³  ê³µë°± ì œê±°
            clean_keyword = re.sub(r'\s+', '', keyword.lower())
            clean_answer = re.sub(r'\s+', '', answer)
            
            if clean_keyword in clean_answer:
                matched_keywords += 1
        
        metrics["answer_keyword_match_score"] = matched_keywords / total_keywords if total_keywords > 0 else 0
        metrics["answer_matched_keywords"] = matched_keywords
        metrics["answer_total_keywords"] = total_keywords
    else:
        metrics["answer_keyword_match_score"] = 0
        metrics["answer_matched_keywords"] = 0
        metrics["answer_total_keywords"] = 0
    
    # ë‹µë³€ í’ˆì§ˆ ì ìˆ˜ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
    quality_score = 0
    
    # ë‹µë³€ ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´ ê°ì )
    answer_length = metrics["answer_length"]
    if 50 <= answer_length <= 500:
        quality_score += 0.2
    elif 20 <= answer_length < 50 or 500 < answer_length <= 1000:
        quality_score += 0.15
    else:
        quality_score += 0.1
    
    # ì»¨í…ìŠ¤íŠ¸ ì ìˆ˜
    if metrics["context_count"] > 0:
        quality_score += 0.2
    
    # Recall@K ì ìˆ˜ (ê°€ì¥ ì¤‘ìš”)
    quality_score += metrics["recall_at_k"] * 0.5
    
    # ë‹µë³€ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
    quality_score += metrics["answer_keyword_match_score"] * 0.1
    
    metrics["quality_score"] = min(quality_score, 1.0)
    
    return metrics


def run_evaluation(questions: List[Dict[str, Any]], graph) -> List[Dict[str, Any]]:
    """
    ì „ì²´ í‰ê°€ ì‹¤í–‰.
    
    Args:
        questions: í‰ê°€ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        graph: RAG ì‹œìŠ¤í…œ ê·¸ë˜í”„
        
    Returns:
        í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    results = []
    
    for i, question_data in enumerate(questions, 1):
        question = question_data.get("question", "")
        if not question:
            continue
        
        logger.info(f"í‰ê°€ ì§„í–‰ ì¤‘: {i}/{len(questions)} - {question_data.get('id', 'unknown')}")
        
        # RAG ì‹œìŠ¤í…œ ì‹¤í–‰
        rag_result = run_rag_system(graph, question)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = calculate_basic_metrics(question_data, rag_result)
        
        # ê²°ê³¼ ì €ì¥
        result = {
            **metrics,
            "answer": rag_result.get("answer", ""),
            "contexts": rag_result.get("contexts", []),
            "ground_truths": question_data.get("ground_truths", []),
            "error": rag_result.get("error", None)
        }
        
        results.append(result)
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        logger.info(f"  - ì‘ë‹µì‹œê°„: {metrics['response_time']:.2f}ì´ˆ")
        logger.info(f"  - ë‹µë³€ê¸¸ì´: {metrics['answer_length']}ì")
        logger.info(f"  - ì»¨í…ìŠ¤íŠ¸: {metrics['context_count']}ê°œ")
        logger.info(f"  - Recall@1: {metrics['recall_at_1']:.3f}")
        logger.info(f"  - Recall@3: {metrics['recall_at_3']:.3f}")
        logger.info(f"  - Recall@5: {metrics['recall_at_5']:.3f}")
        logger.info(f"  - ë§¤ì¹­ëœì •ë‹µ: {metrics['matched_count']}/{metrics['total_ground_truths']}")
        logger.info(f"  - ë‹µë³€í‚¤ì›Œë“œë§¤ì¹­: {metrics['answer_matched_keywords']}/{metrics['answer_total_keywords']}")
        logger.info(f"  - í’ˆì§ˆì ìˆ˜: {metrics['quality_score']:.3f}")
    
    return results


def save_results(results: List[Dict[str, Any]]) -> None:
    """í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥."""
    if not results:
        logger.warning("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    try:
        # CSV í˜•íƒœë¡œ ì €ì¥
        csv_path = OUTPUT_DIR / "simple_eval_results.csv"
        with open(csv_path, 'w', encoding='utf-8') as f:
            # í—¤ë” ì‘ì„± (Recall@K ë©”íŠ¸ë¦­ í¬í•¨)
            headers = [
                "question_id", "question", "category", "intent",
                "response_time", "success", "answer_length", "context_count", "context_length",
                "recall_at_k", "recall_at_1", "recall_at_3", "recall_at_5",
                "matched_ground_truths", "total_ground_truths", "matched_count",
                "answer_keyword_match_score", "answer_matched_keywords", "answer_total_keywords",
                "quality_score"
            ]
            f.write(",".join(headers) + "\n")
            
            # ë°ì´í„° ì‘ì„±
            for result in results:
                row = [
                    str(result.get(header, "")) for header in headers
                ]
                f.write(",".join(row) + "\n")
        
        logger.info(f"CSV ê²°ê³¼ ì €ì¥: {csv_path}")
        
        # JSON í˜•íƒœë¡œ ì €ì¥
        json_path = OUTPUT_DIR / "simple_eval_results.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        logger.info(f"JSON ê²°ê³¼ ì €ì¥: {json_path}")
        
        # ìš”ì•½ í†µê³„ ì €ì¥
        summary_path = OUTPUT_DIR / "simple_eval_summary.json"
        summary = calculate_summary_stats(results)
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        logger.info(f"ìš”ì•½ í†µê³„ ì €ì¥: {summary_path}")
        
    except Exception as e:
        logger.error(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")


def calculate_summary_stats(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ìš”ì•½ í†µê³„ ê³„ì‚°."""
    if not results:
        return {}
    
    # ê¸°ë³¸ í†µê³„
    total_questions = len(results)
    successful_questions = sum(1 for r in results if r.get("success", False))
    
    # í‰ê· ê°’ ê³„ì‚°
    avg_response_time = sum(r.get("response_time", 0) for r in results) / total_questions
    avg_answer_length = sum(r.get("answer_length", 0) for r in results) / total_questions
    avg_context_count = sum(r.get("context_count", 0) for r in results) / total_questions
    avg_recall_at_1 = sum(r.get("recall_at_1", 0) for r in results) / total_questions
    avg_recall_at_3 = sum(r.get("recall_at_3", 0) for r in results) / total_questions
    avg_recall_at_5 = sum(r.get("recall_at_5", 0) for r in results) / total_questions
    avg_answer_keyword_match = sum(r.get("answer_keyword_match_score", 0) for r in results) / total_questions
    avg_quality_score = sum(r.get("quality_score", 0) for r in results) / total_questions
    
    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    category_stats = {}
    for result in results:
        category = result.get("category", "unknown")
        if category not in category_stats:
            category_stats[category] = {
                "count": 0,
                "avg_quality_score": 0,
                "avg_recall_at_5": 0,
                "avg_answer_keyword_match": 0,
                "avg_response_time": 0
            }
        
        category_stats[category]["count"] += 1
        category_stats[category]["avg_quality_score"] += result.get("quality_score", 0)
        category_stats[category]["avg_recall_at_5"] += result.get("recall_at_5", 0)
        category_stats[category]["avg_answer_keyword_match"] += result.get("answer_keyword_match_score", 0)
        category_stats[category]["avg_response_time"] += result.get("response_time", 0)
    
    # ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ê³„ì‚°
    for category in category_stats:
        count = category_stats[category]["count"]
        category_stats[category]["avg_quality_score"] /= count
        category_stats[category]["avg_recall_at_5"] /= count
        category_stats[category]["avg_answer_keyword_match"] /= count
        category_stats[category]["avg_response_time"] /= count
    
    return {
        "evaluation_date": datetime.now().isoformat(),
        "total_questions": total_questions,
        "successful_questions": successful_questions,
        "success_rate": successful_questions / total_questions,
        "average_metrics": {
            "response_time": avg_response_time,
            "answer_length": avg_answer_length,
            "context_count": avg_context_count,
            "recall_at_1": avg_recall_at_1,
            "recall_at_3": avg_recall_at_3,
            "recall_at_5": avg_recall_at_5,
            "answer_keyword_match_score": avg_answer_keyword_match,
            "quality_score": avg_quality_score
        },
        "category_stats": category_stats
    }


def print_summary(results: List[Dict[str, Any]]) -> None:
    """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥."""
    if not results:
        print("âŒ ì¶œë ¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    summary = calculate_summary_stats(results)
    
    print("\n" + "="*60)
    print("ğŸ“Š ê¸°ë³¸ í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("="*60)
    
    print(f"ğŸ“‹ ì´ í‰ê°€ ì§ˆë¬¸: {summary['total_questions']}ê°œ")
    print(f"âœ… ì„±ê³µí•œ ì§ˆë¬¸: {summary['successful_questions']}ê°œ")
    print(f"ğŸ“ˆ ì„±ê³µë¥ : {summary['success_rate']:.1%}")
    
    print(f"\nğŸ“Š í‰ê·  ë©”íŠ¸ë¦­:")
    avg_metrics = summary['average_metrics']
    print(f"  â±ï¸ í‰ê·  ì‘ë‹µì‹œê°„: {avg_metrics['response_time']:.2f}ì´ˆ")
    print(f"  ğŸ“ í‰ê·  ë‹µë³€ê¸¸ì´: {avg_metrics['answer_length']:.0f}ì")
    print(f"  ğŸ“š í‰ê·  ì»¨í…ìŠ¤íŠ¸ ìˆ˜: {avg_metrics['context_count']:.1f}ê°œ")
    print(f"  ğŸ¯ Recall@1: {avg_metrics['recall_at_1']:.3f}")
    print(f"  ğŸ¯ Recall@3: {avg_metrics['recall_at_3']:.3f}")
    print(f"  ğŸ¯ Recall@5: {avg_metrics['recall_at_5']:.3f}")
    print(f"  ğŸ” í‰ê·  ë‹µë³€í‚¤ì›Œë“œë§¤ì¹­: {avg_metrics['answer_keyword_match_score']:.3f}")
    print(f"  â­ í‰ê·  í’ˆì§ˆì ìˆ˜: {avg_metrics['quality_score']:.3f}")
    
    print(f"\nğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥:")
    for category, stats in summary['category_stats'].items():
        print(f"  {category}:")
        print(f"    - ì§ˆë¬¸ ìˆ˜: {stats['count']}ê°œ")
        print(f"    - í’ˆì§ˆì ìˆ˜: {stats['avg_quality_score']:.3f}")
        print(f"    - Recall@5: {stats['avg_recall_at_5']:.3f}")
        print(f"    - ë‹µë³€í‚¤ì›Œë“œë§¤ì¹­: {stats['avg_answer_keyword_match']:.3f}")
        print(f"    - ì‘ë‹µì‹œê°„: {stats['avg_response_time']:.2f}ì´ˆ")
    
    print(f"\nğŸ“ ê²°ê³¼ íŒŒì¼:")
    print(f"  - CSV: eval/out/simple_eval_results.csv")
    print(f"  - JSON: eval/out/simple_eval_results.json")
    print(f"  - ìš”ì•½: eval/out/simple_eval_summary.json")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜."""
    try:
        print("ğŸš€ ê¸°ë³¸ í‰ê°€ ì‹œìŠ¤í…œ ì‹œì‘")
        
        # ì§ˆë¬¸ ë¡œë“œ
        questions = load_questions()
        if not questions:
            logger.error("í‰ê°€í•  ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # RAG ê·¸ë˜í”„ ë¹Œë“œ
        logger.info("RAG ê·¸ë˜í”„ ë¹Œë“œ ì¤‘...")
        graph = build_graph()
        
        # í‰ê°€ ì‹¤í–‰
        results = run_evaluation(questions, graph)
        if not results:
            logger.error("í‰ê°€ ì‹¤í–‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return
        
        # ê²°ê³¼ ì €ì¥
        save_results(results)
        
        # ìš”ì•½ ì¶œë ¥
        print_summary(results)
        
        print("\nâœ… ê¸°ë³¸ í‰ê°€ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main()
