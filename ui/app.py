"""
ì—¬í–‰ìë³´í—˜ RAG ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ UI
Streamlitì„ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ ë° ì¶”ì 
"""

import streamlit as st
import requests
import json
import time
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
from typing import Dict, List, Any, Optional
import uuid
import os
import base64

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì—¬í–‰ìë³´í—˜ RAG ëª¨ë‹ˆí„°ë§",
    page_icon="ğŸ›¡ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# API ê¸°ë³¸ ì„¤ì • - Docker í™˜ê²½ ê°ì§€
API_BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8000")

# CSS ì œê±° - Streamlit ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©

def render_chat_message(message_type: str, content: str, 
                       evidence: List[str] = None, caveats: List[str] = None, 
                       quality_score: float = None, error: bool = False,
                       comparison_table: Dict[str, Any] = None) -> None:
    """ê¸°ë³¸ ì±„íŒ… ë©”ì‹œì§€ ë Œë”ë§ (ë§ˆí¬ë‹¤ìš´ ì§€ì› í¬í•¨)"""
    
    # ë©”ì‹œì§€ íƒ€ì…ì— ë”°ë¥¸ í—¤ë” ê²°ì •
    if message_type == "user":
        header = "ğŸ‘¤ ì‚¬ìš©ì"
    else:  # assistant
        header = "ğŸ¤– AI"
    
    # ì»¨í…Œì´ë„ˆ ìƒì„±
    if message_type == "user":
        with st.container():
            st.markdown(f"**{header}**")
            st.write(content)
    else:  # assistant
        if error:
            st.error(f"**{header}**\n\n{content}")
        else:
            # AI ë‹µë³€ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë Œë”ë§
            st.markdown(f"**{header}**")
            st.markdown(content)  # ë§ˆí¬ë‹¤ìš´ ë Œë”ë§
    
    # ë¹„êµ í‘œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í‘œë¡œ ë Œë”ë§
    if comparison_table and isinstance(comparison_table, dict):
        headers = comparison_table.get("headers", [])
        rows = comparison_table.get("rows", [])
        
        if headers and rows:
            st.markdown("### ğŸ“Š ë¹„êµ í‘œ")
            
            # DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
            import pandas as pd
            df = pd.DataFrame(rows, columns=headers)
            st.dataframe(df, use_container_width=True)
    
    # ì¦ê±° ì •ë³´ í‘œì‹œ
    if evidence:
        with st.expander("ğŸ“‹ ì¦ê±°"):
            for i, ev in enumerate(evidence, 1):
                if isinstance(ev, dict):
                    # evidenceê°€ ê°ì²´ì¸ ê²½ìš°
                    text = ev.get('text', '')
                    source = ev.get('source', '')
                    if source:
                        st.write(f"{i}. **{text}**")
                        st.caption(f"   ì¶œì²˜: {source}")
                    else:
                        st.write(f"{i}. {text}")
                else:
                    # evidenceê°€ ë¬¸ìì—´ì¸ ê²½ìš° (ê¸°ì¡´ í˜¸í™˜ì„±)
                    st.write(f"{i}. {ev}")
    
    # ì£¼ì˜ì‚¬í•­ í‘œì‹œ
    if caveats:
        with st.expander("âš ï¸ ì£¼ì˜ì‚¬í•­"):
            for i, caveat in enumerate(caveats, 1):
                if isinstance(caveat, dict):
                    # caveatì´ ê°ì²´ì¸ ê²½ìš°
                    text = caveat.get('text', '')
                    source = caveat.get('source', '')
                    if source:
                        st.write(f"{i}. **{text}**")
                        st.caption(f"   ì¶œì²˜: {source}")
                    else:
                        st.write(f"{i}. {text}")
                else:
                    # caveatì´ ë¬¸ìì—´ì¸ ê²½ìš° (ê¸°ì¡´ í˜¸í™˜ì„±)
                    st.write(f"{i}. {caveat}")
    
    # í’ˆì§ˆ ì ìˆ˜ í‘œì‹œ
    if quality_score is not None:
        if quality_score >= 0.7:
            st.success(f"í’ˆì§ˆ ì ìˆ˜: {quality_score:.2f}")
        elif quality_score >= 0.4:
            st.warning(f"í’ˆì§ˆ ì ìˆ˜: {quality_score:.2f}")
        else:
            st.error(f"í’ˆì§ˆ ì ìˆ˜: {quality_score:.2f}")
    
    st.markdown("---")

class RAGMonitor:
    """RAG ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.session_id = str(uuid.uuid4())
        self.conversation_history = []
        
    def send_question(self, question: str, include_context: bool = True) -> Dict[str, Any]:
        """ì§ˆë¬¸ì„ RAG APIë¡œ ì „ì†¡í•˜ê³  ê²°ê³¼ ë°˜í™˜"""
        try:
            response = requests.post(
                f"{API_BASE_URL}/rag/ask",
                json={
                    "question": question,
                    "session_id": self.session_id,
                    "include_context": include_context
                },
                timeout=120  # íƒ€ì„ì•„ì›ƒì„ 2ë¶„ìœ¼ë¡œ ì¦ê°€
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            st.error(f"API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            return {}
    
    
    def get_trace(self) -> List[Dict[str, Any]]:
        """ìµœê·¼ ì‹¤í–‰ trace ì •ë³´ ì¡°íšŒ"""
        try:
            response = requests.get(f"{API_BASE_URL}/rag/trace")
            response.raise_for_status()
            data = response.json()
            return data.get("trace", [])
        except requests.exceptions.RequestException as e:
            st.error(f"Trace ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def get_session_info(self) -> Dict[str, Any]:
        """ì„¸ì…˜ ì •ë³´ ì¡°íšŒ"""
        try:
            response = requests.get(f"{API_BASE_URL}/rag/session/{self.session_id}")
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException:
            return {}

def render_pipeline_flow(trace_data: List[Dict[str, Any]]) -> None:
    """íŒŒì´í”„ë¼ì¸ í”Œë¡œìš° ì‹œê°í™”"""
    if not trace_data:
        st.info("ì‹¤í–‰ëœ íŒŒì´í”„ë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë…¸ë“œ ìˆœì„œ ì •ì˜
    node_order = [
        "planner", "websearch", "search", "rank_filter", 
        "verify_refine", "answer_qa", "answer_summary", 
        "answer_compare", "answer_recommend", "reevaluate", "replan"
    ]
    
    # ì‹¤í–‰ëœ ë…¸ë“œë“¤ë§Œ í•„í„°ë§
    executed_nodes = [node for node in trace_data if node.get("node") in node_order]
    
    if not executed_nodes:
        st.warning("ì‹¤í–‰ëœ ë…¸ë“œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í”Œë¡œìš° ì°¨íŠ¸ ìƒì„±
    fig = go.Figure()
    
    # ë…¸ë“œ ìœ„ì¹˜ ê³„ì‚°
    node_positions = {}
    for i, node_name in enumerate(node_order):
        if any(node["node"] == node_name for node in executed_nodes):
            node_positions[node_name] = (i, 0)
    
    # ë…¸ë“œ ê·¸ë¦¬ê¸°
    for node_name, (x, y) in node_positions.items():
        # ë…¸ë“œ ì •ë³´ ì°¾ê¸°
        node_info = next((node for node in executed_nodes if node["node"] == node_name), None)
        
        if node_info:
            # ì‹¤í–‰ ì‹œê°„ì— ë”°ë¥¸ ìƒ‰ìƒ ê²°ì •
            latency = node_info.get("latency_ms", 0)
            if latency > 5000:
                color = "red"
            elif latency > 2000:
                color = "orange"
            else:
                color = "green"
            
            # ë…¸ë“œ ì¶”ê°€
            fig.add_trace(go.Scatter(
                x=[x], y=[y],
                mode='markers+text',
                marker=dict(size=50, color=color, line=dict(width=2, color='black')),
                text=[node_name],
                textposition="middle center",
                name=node_name,
                hovertemplate=f"<b>{node_name}</b><br>" +
                             f"ì‹¤í–‰ì‹œê°„: {latency}ms<br>" +
                             f"ì…ë ¥í† í°: {node_info.get('in_tokens_approx', 0)}<br>" +
                             f"ì¶œë ¥í† í°: {node_info.get('out_tokens_approx', 0)}<extra></extra>"
            ))
    
    # ì—°ê²°ì„  ê·¸ë¦¬ê¸°
    for i in range(len(executed_nodes) - 1):
        current_node = executed_nodes[i]["node"]
        next_node = executed_nodes[i + 1]["node"]
        
        if current_node in node_positions and next_node in node_positions:
            x1, y1 = node_positions[current_node]
            x2, y2 = node_positions[next_node]
            
            fig.add_trace(go.Scatter(
                x=[x1, x2], y=[y1, y2],
                mode='lines',
                line=dict(color='gray', width=2),
                showlegend=False,
                hoverinfo='skip'
            ))
    
    # ë ˆì´ì•„ì›ƒ ì„¤ì •
    fig.update_layout(
        title="RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í”Œë¡œìš°",
        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        showlegend=False,
        height=400,
        margin=dict(l=20, r=20, t=40, b=20)
    )
    
    st.plotly_chart(fig, use_container_width=True)

def render_performance_metrics(trace_data: List[Dict[str, Any]]) -> None:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì‹œê°í™”"""
    if not trace_data:
        return
    
    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    df = pd.DataFrame(trace_data)
    
    # ì‹¤í–‰ ì‹œê°„ ì°¨íŠ¸
    fig_time = px.bar(
        df, 
        x='node', 
        y='latency_ms',
        title="ë…¸ë“œë³„ ì‹¤í–‰ ì‹œê°„ (ms)",
        color='latency_ms',
        color_continuous_scale='RdYlGn_r'
    )
    fig_time.update_layout(xaxis_tickangle=-45)
    st.plotly_chart(fig_time, use_container_width=True)
    
    # í† í° ì‚¬ìš©ëŸ‰ ì°¨íŠ¸
    col1, col2 = st.columns(2)
    
    with col1:
        fig_tokens_in = px.bar(
            df, 
            x='node', 
            y='in_tokens_approx',
            title="ì…ë ¥ í† í° ìˆ˜",
            color='in_tokens_approx',
            color_continuous_scale='Blues'
        )
        fig_tokens_in.update_layout(xaxis_tickangle=-45)
        st.plotly_chart(fig_tokens_in, use_container_width=True)
    
    with col2:
        fig_tokens_out = px.bar(
            df, 
            x='node', 
            y='out_tokens_approx',
            title="ì¶œë ¥ í† í° ìˆ˜",
            color='out_tokens_approx',
            color_continuous_scale='Greens'
        )
        fig_tokens_out.update_layout(xaxis_tickangle=-45)
        st.plotly_chart(fig_tokens_out, use_container_width=True)

def render_document_analysis(passages: List[Dict[str, Any]], search_meta: Dict[str, Any] = None) -> None:
    """ê²€ìƒ‰ëœ ë¬¸ì„œ ë¶„ì„"""
    if not passages:
        st.info("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.subheader("ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ë¶„ì„")
    
    # ë³´í—˜ì‚¬ í•„í„°ë§ ì •ë³´ í‘œì‹œ
    if search_meta:
        insurer_filtered = search_meta.get('insurer_filtered', False)
        insurer_filter = search_meta.get('insurer_filter', [])
        filter_method = search_meta.get('filter_method', 'unknown')
        
        if insurer_filtered and insurer_filter:
            st.success(f"ğŸ¯ ë³´í—˜ì‚¬ í•„í„°ë§ ì ìš©: {', '.join(insurer_filter)}")
            st.info(f"í•„í„°ë§ ë°©ë²•: {filter_method}")
            
            # í•„í„°ë§ëœ ë³´í—˜ì‚¬ ë¬¸ì„œ ìˆ˜ í‘œì‹œ
            filtered_insurer_counts = {}
            for passage in passages:
                insurer = passage.get('insurer', 'Unknown')
                if insurer in insurer_filter:
                    filtered_insurer_counts[insurer] = filtered_insurer_counts.get(insurer, 0) + 1
            
            if filtered_insurer_counts:
                st.write("**í•„í„°ë§ëœ ë³´í—˜ì‚¬ë³„ ë¬¸ì„œ ìˆ˜:**")
                for insurer, count in filtered_insurer_counts.items():
                    st.write(f"  - {insurer}: {count}ê°œ")
        else:
            st.info("â„¹ï¸ ë³´í—˜ì‚¬ í•„í„°ë§ ì—†ìŒ - ì „ì²´ ë¬¸ì„œ ê²€ìƒ‰")
    
    # ë¬¸ì„œ ì†ŒìŠ¤ë³„ ë¶„ë¥˜
    sources = {}
    for passage in passages:
        source = passage.get("source", "unknown")
        if source not in sources:
            sources[source] = []
        sources[source].append(passage)
    
    # ì†ŒìŠ¤ë³„ ë¬¸ì„œ ìˆ˜ í‘œì‹œ
    source_counts = {source: len(docs) for source, docs in sources.items()}
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ì†ŒìŠ¤ë³„ ë¬¸ì„œ ìˆ˜ ì°¨íŠ¸
        fig_sources = px.pie(
            values=list(source_counts.values()),
            names=list(source_counts.keys()),
            title="ë¬¸ì„œ ì†ŒìŠ¤ë³„ ë¶„í¬"
        )
        st.plotly_chart(fig_sources, use_container_width=True)
    
    with col2:
        # ë¬¸ì„œ ì ìˆ˜ ë¶„í¬
        scores = [p.get("score", 0) for p in passages]
        fig_scores = px.histogram(
            x=scores,
            title="ë¬¸ì„œ ê´€ë ¨ì„± ì ìˆ˜ ë¶„í¬",
            nbins=20
        )
        st.plotly_chart(fig_scores, use_container_width=True)
    
    # ë³´í—˜ì‚¬ë³„ ë¬¸ì„œ ë¶„ì„
    insurer_docs = {}
    for passage in passages:
        insurer = passage.get('insurer', 'Unknown')
        if insurer not in insurer_docs:
            insurer_docs[insurer] = []
        insurer_docs[insurer].append(passage)
    
    if insurer_docs:
        st.subheader("ğŸ¢ ë³´í—˜ì‚¬ë³„ ë¬¸ì„œ ë¶„í¬")
        
        # ë³´í—˜ì‚¬ë³„ ë¬¸ì„œ ìˆ˜
        insurer_counts = {insurer: len(docs) for insurer, docs in insurer_docs.items()}
        
        # ë³´í—˜ì‚¬ í•„í„°ë§ì´ ì ìš©ëœ ê²½ìš° ê°•ì¡° í‘œì‹œ
        if search_meta and search_meta.get('insurer_filtered', False):
            target_insurers = search_meta.get('insurer_filter', [])
            st.info(f"ğŸ¯ í•„í„°ë§ëœ ë³´í—˜ì‚¬: {', '.join(target_insurers)}")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # ë³´í—˜ì‚¬ë³„ ë¬¸ì„œ ìˆ˜ ì°¨íŠ¸
            fig_insurers = px.bar(
                x=list(insurer_counts.keys()),
                y=list(insurer_counts.values()),
                title="ë³´í—˜ì‚¬ë³„ ë¬¸ì„œ ìˆ˜",
                labels={'x': 'ë³´í—˜ì‚¬', 'y': 'ë¬¸ì„œ ìˆ˜'}
            )
            st.plotly_chart(fig_insurers, use_container_width=True)
        
        with col2:
            # ë³´í—˜ì‚¬ë³„ í‰ê·  ì ìˆ˜
            insurer_avg_scores = {}
            for insurer, docs in insurer_docs.items():
                avg_score = sum(d.get('score', 0) for d in docs) / len(docs)
                insurer_avg_scores[insurer] = avg_score
            
            fig_scores = px.bar(
                x=list(insurer_avg_scores.keys()),
                y=list(insurer_avg_scores.values()),
                title="ë³´í—˜ì‚¬ë³„ í‰ê·  ê´€ë ¨ì„± ì ìˆ˜",
                labels={'x': 'ë³´í—˜ì‚¬', 'y': 'í‰ê·  ì ìˆ˜'}
            )
            st.plotly_chart(fig_scores, use_container_width=True)
    
    # ìƒì„¸ ë¬¸ì„œ ì •ë³´
    st.subheader("ğŸ“‹ ë¬¸ì„œ ìƒì„¸ ì •ë³´")
    
    # ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_passages = sorted(passages, key=lambda x: x.get('score', 0), reverse=True)
    
    for i, passage in enumerate(sorted_passages[:10]):  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
        # ë³´í—˜ì‚¬ ë¶€ìŠ¤íŠ¸ ì—¬ë¶€ í‘œì‹œ
        title_suffix = ""
        if passage.get('insurer_boost', False):
            title_suffix += " ğŸ¯"
        if passage.get('target_insurer', False):
            title_suffix += " â­"
        
        # ë³´í—˜ì‚¬ í•„í„°ë§ì´ ì ìš©ëœ ê²½ìš° í•´ë‹¹ ë³´í—˜ì‚¬ ë¬¸ì„œ ê°•ì¡°
        insurer = passage.get('insurer', 'N/A')
        if search_meta and search_meta.get('insurer_filtered', False):
            target_insurers = search_meta.get('insurer_filter', [])
            if insurer in target_insurers:
                title_suffix += " ğŸ¯"
        
        with st.expander(f"ë¬¸ì„œ {i+1}: {passage.get('title', 'ì œëª© ì—†ìŒ')} (ì ìˆ˜: {passage.get('score', 0):.3f}){title_suffix}"):
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.write(f"**ì†ŒìŠ¤**: {passage.get('source', 'unknown')}")
                st.write(f"**ì ìˆ˜**: {passage.get('score', 0):.3f}")
            
            with col2:
                st.write(f"**í˜ì´ì§€**: {passage.get('page', 'N/A')}")
                st.write(f"**ë¬¸ì„œID**: {passage.get('doc_id', 'N/A')}")
            
            with col3:
                insurer_display = insurer
                if passage.get('target_insurer', False):
                    insurer_display += " â­"
                
                # ë³´í—˜ì‚¬ í•„í„°ë§ì´ ì ìš©ëœ ê²½ìš° í•´ë‹¹ ë³´í—˜ì‚¬ ê°•ì¡°
                if search_meta and search_meta.get('insurer_filtered', False):
                    target_insurers = search_meta.get('insurer_filter', [])
                    if insurer in target_insurers:
                        insurer_display += " ğŸ¯"
                
                st.write(f"**ë³´í—˜ì‚¬**: {insurer_display}")
                if passage.get('url'):
                    st.write(f"**URL**: {passage.get('url')}")
            
            # ë³´í—˜ì‚¬ ë¶€ìŠ¤íŠ¸ ì •ë³´
            if passage.get('insurer_boost', False):
                st.info("ğŸ¯ ì´ ë¬¸ì„œëŠ” ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰ëœ ë³´í—˜ì‚¬ì˜ ë¬¸ì„œë¡œ ìš°ì„ ìˆœìœ„ê°€ ë¶€ì—¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ë³´í—˜ì‚¬ í•„í„°ë§ ì •ë³´
            if search_meta and search_meta.get('insurer_filtered', False):
                target_insurers = search_meta.get('insurer_filter', [])
                if insurer in target_insurers:
                    st.success(f"ğŸ¯ ì´ ë¬¸ì„œëŠ” í•„í„°ë§ëœ ë³´í—˜ì‚¬({insurer})ì˜ ë¬¸ì„œì…ë‹ˆë‹¤.")
            
            # ë¬¸ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
            text = passage.get('text', '')
            if text:
                st.text_area(
                    "ë¬¸ì„œ ë‚´ìš©",
                    text[:500] + "..." if len(text) > 500 else text,
                    height=100,
                    disabled=True
                )

    

def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
    st.title("ğŸ›¡ï¸ ì—¬í–‰ìë³´í—˜ RAG ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§")
    st.markdown("---")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'monitor' not in st.session_state:
        st.session_state.monitor = RAGMonitor()
    
    monitor = st.session_state.monitor
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # API ì—°ê²° ìƒíƒœ í™•ì¸
        try:
            response = requests.get(f"{API_BASE_URL}/", timeout=5)
            if response.status_code == 200:
                st.success("âœ… API ì—°ê²°ë¨")
            else:
                st.error("âŒ API ì—°ê²° ì‹¤íŒ¨")
        except:
            st.error("âŒ API ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ìƒì„¸ ì§„ë‹¨ ì •ë³´
        if st.button("ğŸ” ìƒì„¸ ì§„ë‹¨", help="API ìƒíƒœ ë° ì„¤ì •ì„ ìì„¸íˆ í™•ì¸í•©ë‹ˆë‹¤"):
            with st.spinner("ì§„ë‹¨ ì¤‘..."):
                try:
                    api_status = requests.get(f"{API_BASE_URL}/api-status", timeout=10).json()
                    
                    st.subheader("ğŸ“Š API ìƒíƒœ ì§„ë‹¨")
                    
                    # Gemini ìƒíƒœ
                    gemini = api_status.get("gemini", {})
                    st.write("**ğŸ¤– Gemini API**")
                    st.write(f"ìƒíƒœ: {gemini.get('status', 'â“')}")
                    st.write(f"ëª¨ë¸: {gemini.get('model', 'â“')}")
                    st.write(f"API í‚¤: {gemini.get('api_key', 'â“')}")
                    
                    if gemini.get('error'):
                        st.error(f"ì˜¤ë¥˜: {gemini.get('error')}")
                        st.info("ğŸ’¡ í•´ê²° ë°©ë²•:")
                        if "API í‚¤" in gemini.get('error', ''):
                            st.write("1. .env íŒŒì¼ì— GEMINI_API_KEYê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸")
                            st.write("2. API í‚¤ê°€ ìœ íš¨í•œì§€ Google AI Studioì—ì„œ í™•ì¸")
                        elif "í• ë‹¹ëŸ‰" in gemini.get('error', ''):
                            st.write("1. Google AI Studioì—ì„œ í• ë‹¹ëŸ‰ í™•ì¸")
                            st.write("2. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„")
                        elif "ê¶Œí•œ" in gemini.get('error', ''):
                            st.write("1. API í‚¤ì— í•´ë‹¹ ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œì´ ìˆëŠ”ì§€ í™•ì¸")
                            st.write("2. ëª¨ë¸ ì´ë¦„ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
                        elif "ë„¤íŠ¸ì›Œí¬" in gemini.get('error', ''):
                            st.write("1. ì¸í„°ë„· ì—°ê²° í™•ì¸")
                            st.write("2. ë°©í™”ë²½ ì„¤ì • í™•ì¸")
                    
                    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸
                    available_models = gemini.get('available_models', [])
                    if available_models:
                        st.write(f"**ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸**: {', '.join(available_models[:5])}")
                        if len(available_models) > 5:
                            st.write(f"... ì™¸ {len(available_models) - 5}ê°œ")
                    
                    # Tavily ìƒíƒœ
                    tavily = api_status.get("tavily", {})
                    st.write("**ğŸŒ Tavily API**")
                    st.write(f"ìƒíƒœ: {tavily.get('status', 'â“')}")
                    
                    # ì „ì²´ ìƒíƒœ
                    overall = api_status.get("overall", "â“")
                    if "âœ…" in overall:
                        st.success(f"ì „ì²´ ìƒíƒœ: {overall}")
                    else:
                        st.error(f"ì „ì²´ ìƒíƒœ: {overall}")
                        
                except Exception as e:
                    st.error(f"ì§„ë‹¨ ì‹¤íŒ¨: {str(e)}")
                    st.info("API ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        st.markdown("---")
        
        # ì„¸ì…˜ ì •ë³´
        st.subheader("ğŸ“Š ì„¸ì…˜ ì •ë³´")
        st.write(f"**ì„¸ì…˜ ID**: {monitor.session_id[:8]}...")
        
        session_info = monitor.get_session_info()
        if session_info:
            context = session_info.get('context', {})
            st.write(f"**ëŒ€í™” ìˆ˜**: {context.get('turn_count', 0)}")
            st.write(f"**ìƒì„± ì‹œê°„**: {context.get('created_at', 'N/A')}")
        
        st.markdown("---")
        
        # ìºì‹œ í†µê³„
        try:
            cache_response = requests.get(f"{API_BASE_URL}/rag/cache/stats")
            if cache_response.status_code == 200:
                cache_data = cache_response.json()
                st.subheader("ğŸ’¾ ìºì‹œ í†µê³„")
                cache_stats = cache_data.get('cache_stats', {})
                st.write(f"**ì„ë² ë”© ìºì‹œ**: {cache_stats.get('embeddings', 0)}")
                st.write(f"**ê²€ìƒ‰ ìºì‹œ**: {cache_stats.get('search', 0)}")
                st.write(f"**LLM ìºì‹œ**: {cache_stats.get('llm_response', 0)}")
        except:
            pass
    
    # ë©”ì¸ ì»¨í…ì¸ 
    tab1, tab2, tab3 = st.tabs(["ğŸ” ì§ˆë¬¸í•˜ê¸°", "ğŸ“Š íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§", "ğŸ“„ ë¬¸ì„œ ë¶„ì„"])
    
    with tab1:
        
        # ê¸°ì¡´ ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
        if monitor.conversation_history:
            for chat in monitor.conversation_history:
                # ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
                render_chat_message(
                    message_type="user",
                    content=chat['question']
                )
                
                # AI ë‹µë³€ í‘œì‹œ
                result = chat['result']
                answer = None
                if 'final_answer' in result and result['final_answer']:
                    answer = result['final_answer']
                elif 'draft_answer' in result and result['draft_answer']:
                    answer = result['draft_answer']
                
                if answer:
                    conclusion = answer.get('conclusion', answer.get('content', 'ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'))
                    evidence = answer.get('evidence', [])
                    caveats = answer.get('caveats', [])
                    quality_score = result.get('quality_score', 0)
                    comparison_table = answer.get('comparison_table', None)
                    
                    render_chat_message(
                        message_type="assistant",
                        content=conclusion,
                        evidence=evidence,
                        caveats=caveats,
                        quality_score=quality_score,
                        comparison_table=comparison_table
                    )
                else:
                    render_chat_message(
                        message_type="assistant",
                        content="ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        error=True
                    )
        
        # ì§ˆë¬¸ ì…ë ¥ í¼
        st.markdown("---")
        
        with st.form("chat_form", clear_on_submit=True):
            col1, col2 = st.columns([4, 1])
            
            with col1:
                question = st.text_area(
                    "ì—¬í–‰ìë³´í—˜ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”:",
                    placeholder="ì˜ˆ: í•´ì™¸ì—¬í–‰ ë³´í—˜ë£ŒëŠ” ì–¼ë§ˆì¸ê°€ìš”?",
                    height=80,
                    key="question_input"
                )
            
            with col2:
                include_context = st.checkbox("ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í¬í•¨", value=True, key="context_checkbox")
                submit_button = st.form_submit_button("ì „ì†¡", type="primary")
        
        if submit_button and question.strip():
            # ì‚¬ìš©ì ì§ˆë¬¸ì„ ì¦‰ì‹œ í‘œì‹œ
            render_chat_message(
                message_type="user",
                content=question
            )
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                start_time = time.time()
                
                # ì§„í–‰ ìƒí™© í‘œì‹œ
                status_text.text("ğŸ” ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
                progress_bar.progress(20)
                
                result = monitor.send_question(question, include_context)
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                status_text.text("ğŸ“Š íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
                progress_bar.progress(60)
                end_time = time.time()
                
                progress_bar.progress(100)
                status_text.text("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ!")
                
                if result:
                    # ë‹µë³€ ë‚´ìš©
                    answer = None
                    if 'final_answer' in result and result['final_answer']:
                        answer = result['final_answer']
                    elif 'draft_answer' in result and result['draft_answer']:
                        answer = result['draft_answer']
                    
                    if answer:
                        conclusion = answer.get('conclusion', answer.get('content', 'ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'))
                        evidence = answer.get('evidence', [])
                        caveats = answer.get('caveats', [])
                        quality_score = result.get('quality_score', 0)
                        comparison_table = answer.get('comparison_table', None)
                        
                        # AI ë‹µë³€ í‘œì‹œ
                        render_chat_message(
                            message_type="assistant",
                            content=conclusion,
                            evidence=evidence,
                            caveats=caveats,
                            quality_score=quality_score,
                            comparison_table=comparison_table
                        )
                    else:
                        render_chat_message(
                            message_type="assistant",
                            content="ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                            error=True
                        )
                    
                    # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                    monitor.conversation_history.append({
                        'question': question,
                        'result': result,
                        'timestamp': datetime.now()
                    })
                    
                    # ì§„í–‰ ìƒí™© ì´ˆê¸°í™”
                    progress_bar.progress(0)
                    status_text.text("")
                    
                    # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ëª¨ë‹ˆí„°ë§ íƒ­ ì—…ë°ì´íŠ¸
                    st.rerun()
                else:
                    render_chat_message(
                        message_type="assistant",
                        content="ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
                        error=True
                    )
                    
            except Exception as e:
                render_chat_message(
                    message_type="assistant",
                    content=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                    error=True
                )
                
                # ì§„í–‰ ìƒí™© ì´ˆê¸°í™”
                progress_bar.progress(0)
                status_text.text("")
        
        elif submit_button and not question.strip():
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
    
    with tab2:
        st.header("íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§")
        
        # ìµœê·¼ trace ì •ë³´ ì¡°íšŒ
        trace_data = monitor.get_trace()
        
        if trace_data:
            # íŒŒì´í”„ë¼ì¸ í”Œë¡œìš°
            st.subheader("ğŸ”„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í”Œë¡œìš°")
            render_pipeline_flow(trace_data)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            st.subheader("ğŸ“ˆ ì„±ëŠ¥ ë©”íŠ¸ë¦­")
            render_performance_metrics(trace_data)
            
            # ìƒì„¸ trace ì •ë³´
            st.subheader("ğŸ” ìƒì„¸ ì‹¤í–‰ ì •ë³´")
            
            # Trace ë°ì´í„°í”„ë ˆì„
            df = pd.DataFrame(trace_data)
            st.dataframe(df, use_container_width=True)
            
            # ì´ ì‹¤í–‰ ì‹œê°„
            total_time = sum(node.get('latency_ms', 0) for node in trace_data)
            total_tokens = sum(node.get('out_tokens_approx', 0) for node in trace_data)
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì´ ì‹¤í–‰ ì‹œê°„", f"{total_time}ms")
            with col2:
                st.metric("ì´ í† í° ìˆ˜", f"{total_tokens:,}")
            with col3:
                st.metric("ì‹¤í–‰ëœ ë…¸ë“œ ìˆ˜", len(trace_data))
            
            # ë³´í—˜ì‚¬ í•„í„°ë§ ì •ë³´
            planner_nodes = [node for node in trace_data if node.get('node_name') == 'planner']
            if planner_nodes:
                st.subheader("ğŸ¯ ë³´í—˜ì‚¬ í•„í„°ë§ ì •ë³´")
                planner_meta = planner_nodes[0]
                insurer_filter = planner_meta.get('insurer_filter', [])
                extracted_insurers = planner_meta.get('extracted_insurers', [])
                owned_insurers = planner_meta.get('owned_insurers', [])
                non_owned_insurers = planner_meta.get('non_owned_insurers', [])
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ì¶”ì¶œëœ ë³´í—˜ì‚¬", f"{len(extracted_insurers)}ê°œ")
                with col2:
                    st.metric("ë³´ìœ  ë³´í—˜ì‚¬", f"{len(owned_insurers)}ê°œ")
                with col3:
                    st.metric("ë¹„ë³´ìœ  ë³´í—˜ì‚¬", f"{len(non_owned_insurers)}ê°œ")
                
                if extracted_insurers:
                    st.info(f"ğŸ” ì§ˆë¬¸ì—ì„œ ì¶”ì¶œëœ ë³´í—˜ì‚¬: {', '.join(extracted_insurers)}")
                
                if insurer_filter:
                    st.success(f"ğŸ¯ ì ìš©ëœ ë³´í—˜ì‚¬ í•„í„°: {', '.join(insurer_filter)}")
                else:
                    st.info("â„¹ï¸ ë³´í—˜ì‚¬ í•„í„°ë§ ì—†ìŒ - ì „ì²´ ë¬¸ì„œ ê²€ìƒ‰")
                
                if non_owned_insurers:
                    st.warning(f"âš ï¸ ë¹„ë³´ìœ  ë³´í—˜ì‚¬ë¡œ ì¸í•œ ì›¹ê²€ìƒ‰ í•„ìš”: {', '.join(non_owned_insurers)}")
            
            # re-evaluate ë…¸ë“œ ì •ë³´
            reevaluate_nodes = [node for node in trace_data if node.get('node_name') == 'reevaluate']
            if reevaluate_nodes:
                st.subheader("ğŸ” ë‹µë³€ í’ˆì§ˆ í‰ê°€")
                reevaluate_meta = reevaluate_nodes[0].get('reevaluate_meta', {})
                quality_score = reevaluate_meta.get('quality_score', 0)
                needs_replan = reevaluate_meta.get('needs_replan', False)
                replan_count = reevaluate_meta.get('replan_count', 0)
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("í’ˆì§ˆ ì ìˆ˜", f"{quality_score:.2f}")
                with col2:
                    st.metric("ì¬ê²€ìƒ‰ í•„ìš”", "âœ…" if needs_replan else "âŒ")
                with col3:
                    st.metric("ì¬ê²€ìƒ‰ íšŸìˆ˜", f"{replan_count}/3")
                
                if needs_replan:
                    st.warning("âš ï¸ ë‹µë³€ í’ˆì§ˆì´ ê¸°ì¤€(0.7) ë¯¸ë§Œìœ¼ë¡œ ì¬ê²€ìƒ‰ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                else:
                    st.success("âœ… ë‹µë³€ í’ˆì§ˆì´ ê¸°ì¤€ì„ ë§Œì¡±í•©ë‹ˆë‹¤.")
        else:
            st.info("ì‹¤í–‰ëœ íŒŒì´í”„ë¼ì¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”.")
    
    with tab3:
        st.header("ë¬¸ì„œ ë¶„ì„")
        
        # ìµœê·¼ ì§ˆë¬¸ì˜ ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„
        if monitor.conversation_history:
            latest_result = monitor.conversation_history[-1]['result']
            passages = latest_result.get('passages', [])
            search_meta = latest_result.get('search_meta', {})
            
            if passages:
                render_document_analysis(passages, search_meta)
            else:
                st.info("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ì§ˆë¬¸ì„ ë¨¼ì € í•´ë³´ì„¸ìš”.")
    

if __name__ == "__main__":
    main()
