"""
LangSmithì™€ í†µí•©ëœ LLM í˜¸ì¶œ ë˜í¼
Google Gemini LLM í˜¸ì¶œì„ LangSmithë¡œ ì¶”ì í•©ë‹ˆë‹¤.
"""

import time
import json
import re
from typing import Dict, Any, Optional, List
import google.generativeai as genai
from app.deps import get_llm
from graph.langsmith_integration import create_langsmith_run, update_langsmith_run, is_langsmith_enabled


class LangSmithLLMWrapper:
    """LangSmith ì¶”ì ì´ í¬í•¨ëœ LLM ë˜í¼ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.base_llm = get_llm()
        self.enabled = is_langsmith_enabled()
    
    def generate_content(self, prompt: str, request_options: Optional[Dict[str, Any]] = None, **kwargs) -> Any:
        """
        LangSmith ì¶”ì ì´ í¬í•¨ëœ LLM ì½˜í…ì¸  ìƒì„±
        
        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            request_options: ìš”ì²­ ì˜µì…˜ (íƒ€ì„ì•„ì›ƒ ë“±)
            **kwargs: ì¶”ê°€ í‚¤ì›Œë“œ ì¸ìˆ˜
            
        Returns:
            LLM ì‘ë‹µ ê°ì²´
        """
        run_id = None
        start_time = time.time()
        
        try:
            # LangSmith ì‹¤í–‰ ì¶”ì  ì‹œì‘
            if self.enabled:
                try:
                    run_id = create_langsmith_run(
                        name="gemini_generate_content",
                        inputs={
                            "prompt": prompt,
                            "request_options": request_options or {},
                            "model": "gemini-1.5-flash"
                        },
                        extra={
                            "metadata": {
                                "provider": "google",
                                "model": "gemini-1.5-flash",
                                "timestamp": start_time
                            }
                        }
                    )
                except Exception as e:
                    print(f"âš ï¸ LangSmith ì‹¤í–‰ ì¶”ì  ì‹œì‘ ì‹¤íŒ¨: {str(e)}")
                    run_id = None
            
            # ì‹¤ì œ LLM í˜¸ì¶œ
            response = self.base_llm.generate_content(prompt, request_options=request_options, **kwargs)
            
            # ì„±ê³µ ì‹œ LangSmith ì—…ë°ì´íŠ¸
            if self.enabled and run_id:
                try:
                    end_time = time.time()
                    update_langsmith_run(
                        run_id=run_id,
                        outputs={
                            "response_text": response.text if hasattr(response, 'text') else str(response),
                            "usage_metadata": getattr(response, 'usage_metadata', {}),
                            "finish_reason": getattr(response, 'finish_reason', 'stop')
                        },
                        extra={
                            "metadata": {
                                "latency_seconds": end_time - start_time,
                                "success": True
                            }
                        }
                    )
                except Exception as e:
                    print(f"âš ï¸ LangSmith ì‹¤í–‰ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            
            return response
            
        except Exception as e:
            # ì—ëŸ¬ ì‹œ LangSmith ì—…ë°ì´íŠ¸
            if self.enabled and run_id:
                try:
                    end_time = time.time()
                    update_langsmith_run(
                        run_id=run_id,
                        error=str(e),
                        extra={
                            "metadata": {
                                "latency_seconds": end_time - start_time,
                                "success": False,
                                "error_type": type(e).__name__
                            }
                        }
                    )
                    print(f"âœ… LangSmith ì—ëŸ¬ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {type(e).__name__}")
                except Exception as update_error:
                    print(f"âš ï¸ LangSmith ì—ëŸ¬ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(update_error)}")
            else:
                print(f"âš ï¸ LangSmith ì‹¤í–‰ IDê°€ ì—†ì–´ ì—ëŸ¬ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤: {str(e)}")
            
            # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—ëŸ¬ì¸ì§€ í™•ì¸
            error_str = str(e).lower()
            if "quota" in error_str or "limit" in error_str or "429" in error_str:
                print("ğŸš« Gemini API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                print(f"ìƒì„¸ ì—ëŸ¬: {str(e)}")
            
            # ì›ë³¸ ì—ëŸ¬ ì¬ë°œìƒ
            raise e
    
    def count_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ê³„ì‚° (LangSmith ì¶”ì  í¬í•¨)"""
        run_id = None
        start_time = time.time()
        
        try:
            # LangSmith ì‹¤í–‰ ì¶”ì  ì‹œì‘
            if self.enabled:
                try:
                    run_id = create_langsmith_run(
                        name="gemini_count_tokens",
                        inputs={"text": text},
                        extra={
                            "metadata": {
                                "provider": "google",
                                "model": "gemini-1.5-flash",
                                "timestamp": start_time
                            }
                        }
                    )
                except Exception as e:
                    print(f"âš ï¸ LangSmith í† í° ê³„ì‚° ì¶”ì  ì‹œì‘ ì‹¤íŒ¨: {str(e)}")
                    run_id = None
            
            # ì‹¤ì œ í† í° ê³„ì‚°
            if hasattr(self.base_llm, 'count_tokens'):
                token_count = self.base_llm.count_tokens(text)
            else:
                # fallback: ê°„ë‹¨í•œ ì¶”ì •
                token_count = len(text.split()) * 1.3  # ëŒ€ëµì ì¸ ì¶”ì •
            
            # ì„±ê³µ ì‹œ LangSmith ì—…ë°ì´íŠ¸
            if self.enabled and run_id:
                try:
                    end_time = time.time()
                    update_langsmith_run(
                        run_id=run_id,
                        outputs={"token_count": token_count},
                        extra={
                            "metadata": {
                                "latency_seconds": end_time - start_time,
                                "success": True
                            }
                        }
                    )
                except Exception as e:
                    print(f"âš ï¸ LangSmith í† í° ê³„ì‚° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            
            return int(token_count)
            
        except Exception as e:
            # ì—ëŸ¬ ì‹œ LangSmith ì—…ë°ì´íŠ¸
            if self.enabled and run_id:
                try:
                    end_time = time.time()
                    update_langsmith_run(
                        run_id=run_id,
                        error=str(e),
                        extra={
                            "metadata": {
                                "latency_seconds": end_time - start_time,
                                "success": False,
                                "error_type": type(e).__name__
                            }
                        }
                    )
                except Exception as update_error:
                    print(f"âš ï¸ LangSmith í† í° ê³„ì‚° ì—ëŸ¬ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(update_error)}")
            
            # ì›ë³¸ ì—ëŸ¬ ì¬ë°œìƒ
            raise e
    
    def with_structured_output(self, response_schema: Any) -> 'LangSmithLLMWrapper':
        """
        Structured outputì„ ìœ„í•œ ë˜í¼ ë°˜í™˜
        Geminiì˜ ê²½ìš° ê¸°ë³¸ì ìœ¼ë¡œ structured outputì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ
        í”„ë¡¬í”„íŠ¸ì— JSON ìŠ¤í‚¤ë§ˆë¥¼ ì¶”ê°€í•˜ì—¬ ì²˜ë¦¬
        """
        return StructuredOutputWrapper(self, response_schema)


class StructuredOutputWrapper:
    """Structured outputì„ ìœ„í•œ ë˜í¼ í´ë˜ìŠ¤"""
    
    def __init__(self, llm_wrapper: LangSmithLLMWrapper, response_schema: Any):
        self.llm_wrapper = llm_wrapper
        self.response_schema = response_schema
    
    def generate_content(self, prompt: str, request_options: Optional[Dict[str, Any]] = None, **kwargs) -> Any:
        """
        Structured outputì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì • ë° ì‘ë‹µ íŒŒì‹±
        """
        # ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
        structured_prompt = self._add_schema_to_prompt(prompt)
        
        # ê¸°ë³¸ LLM í˜¸ì¶œ
        response = self.llm_wrapper.generate_content(
            structured_prompt, 
            request_options=request_options, 
            **kwargs
        )
        
        # ì‘ë‹µì„ structured formatìœ¼ë¡œ íŒŒì‹±
        return self._parse_structured_response(response)
    
    def _add_schema_to_prompt(self, prompt: str) -> str:
        """í”„ë¡¬í”„íŠ¸ì— JSON ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¶”ê°€"""
        # ìŠ¤í‚¤ë§ˆ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ JSON í˜•ì‹ ì œê³µ
        schema_name = getattr(self.response_schema, '__name__', '')
        
        if 'Recommend' in schema_name:
            schema_instruction = """

**ì¤‘ìš”**: ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

```json
{
    "conclusion": "ë‹µë³€ì˜ í•µì‹¬ ë‚´ìš©",
    "evidence": ["ì¦ê±° 1", "ì¦ê±° 2"],
    "caveats": ["ì£¼ì˜ì‚¬í•­ 1", "ì£¼ì˜ì‚¬í•­ 2"],
    "quotes": [
        {
            "text": "ì¸ìš©ëœ í…ìŠ¤íŠ¸",
            "source": "ì¶œì²˜ ì •ë³´"
        }
    ],
    "recommendations": ["ì¶”ì²œ 1", "ì¶”ì²œ 2"],
    "web_info": {
        "sources": ["ì›¹ ì†ŒìŠ¤ 1", "ì›¹ ì†ŒìŠ¤ 2"]
    }
}
```

ìœ„ JSON í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
        elif 'Compare' in schema_name:
            schema_instruction = """

**ì¤‘ìš”**: ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

```json
{
    "conclusion": "ë‹µë³€ì˜ í•µì‹¬ ë‚´ìš©",
    "evidence": ["ì¦ê±° 1", "ì¦ê±° 2"],
    "caveats": ["ì£¼ì˜ì‚¬í•­ 1", "ì£¼ì˜ì‚¬í•­ 2"],
    "quotes": [
        {
            "text": "ì¸ìš©ëœ í…ìŠ¤íŠ¸",
            "source": "ì¶œì²˜ ì •ë³´"
        }
    ],
    "comparison_table": {
        "headers": ["í•­ëª©", "ë³´í—˜ì‚¬ A", "ë³´í—˜ì‚¬ B"],
        "rows": [
            ["í•­ëª©1", "ë‚´ìš©1", "ë‚´ìš©2"],
            ["í•­ëª©2", "ë‚´ìš©3", "ë‚´ìš©4"]
        ]
    }
}
```

ìœ„ JSON í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
        elif 'Quality' in schema_name:
            schema_instruction = """

**ì¤‘ìš”**: ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

```json
{
    "score": 0.8,
    "feedback": "í’ˆì§ˆ í‰ê°€ ì„¤ëª…",
    "needs_replan": false,
    "replan_query": ""
}
```

ìœ„ JSON í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
        elif 'Planner' in schema_name:
            schema_instruction = """

**ì¤‘ìš”**: ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

```json
{
    "intent": "qa",
    "needs_web": false,
    "reasoning": "ë¶„ë¥˜ ê·¼ê±° ì„¤ëª…"
}
```

ìœ„ JSON í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
        elif 'Replan' in schema_name:
            schema_instruction = """

**ì¤‘ìš”**: ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

```json
{
    "new_question": "ê°œì„ ëœ ê²€ìƒ‰ ì§ˆë¬¸",
    "needs_web": false,
    "reasoning": "ì¬ê²€ìƒ‰ ì§ˆë¬¸ ê°œì„  ê·¼ê±°"
}
```

ìœ„ JSON í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
        else:
            # ê¸°ë³¸ QA/Summarize í˜•ì‹
            schema_instruction = """

**ì¤‘ìš”**: ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

```json
{
    "conclusion": "ë‹µë³€ì˜ í•µì‹¬ ë‚´ìš©",
    "evidence": ["ì¦ê±° 1", "ì¦ê±° 2"],
    "caveats": ["ì£¼ì˜ì‚¬í•­ 1", "ì£¼ì˜ì‚¬í•­ 2"],
    "quotes": [
        {
            "text": "ì¸ìš©ëœ í…ìŠ¤íŠ¸",
            "source": "ì¶œì²˜ ì •ë³´"
        }
    ]
}
```

ìœ„ JSON í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
        
        return prompt + schema_instruction
    
    def _parse_structured_response(self, response: Any) -> Any:
        """LLM ì‘ë‹µì„ structured formatìœ¼ë¡œ íŒŒì‹±"""
        try:
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if hasattr(response, 'text'):
                response_text = response.text
            else:
                response_text = str(response)
            
            # JSON ë¸”ë¡ ì¶”ì¶œ
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # JSON ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ JSON ì°¾ê¸°
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                else:
                    raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # JSON íŒŒì‹±
            parsed_data = json.loads(json_str)
            
            # ìŠ¤í‚¤ë§ˆì— ë§ëŠ” ê°ì²´ ìƒì„± (ê°„ë‹¨í•œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê°ì²´)
            class StructuredResponse:
                def __init__(self, data):
                    for key, value in data.items():
                        setattr(self, key, value)
            
            return StructuredResponse(parsed_data)
            
        except Exception as e:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
            schema_name = getattr(self.response_schema, '__name__', '')
            
            if 'Recommend' in schema_name:
                class FallbackResponse:
                    def __init__(self):
                        self.conclusion = "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                        self.evidence = ["ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜"]
                        self.caveats = ["ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."]
                        self.quotes = []
                        self.recommendations = []
                        self.web_info = {}
            elif 'Compare' in schema_name:
                class FallbackResponse:
                    def __init__(self):
                        self.conclusion = "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                        self.evidence = ["ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜"]
                        self.caveats = ["ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."]
                        self.quotes = []
                        self.comparison_table = {
                            "headers": ["í•­ëª©", "ë¹„êµ ê²°ê³¼"],
                            "rows": [["ì˜¤ë¥˜", "íŒŒì‹± ì‹¤íŒ¨"]]
                        }
            elif 'Quality' in schema_name:
                class FallbackResponse:
                    def __init__(self):
                        self.score = 0.5
                        self.feedback = "í’ˆì§ˆ í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                        self.needs_replan = True
                        self.replan_query = ""
            elif 'Planner' in schema_name:
                class FallbackResponse:
                    def __init__(self):
                        self.intent = "qa"
                        self.needs_web = False
                        self.reasoning = "ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            elif 'Replan' in schema_name:
                class FallbackResponse:
                    def __init__(self):
                        self.new_question = "ì¬ê²€ìƒ‰ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                        self.needs_web = False
                        self.reasoning = "ì¬ê²€ìƒ‰ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            else:
                class FallbackResponse:
                    def __init__(self):
                        self.conclusion = "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                        self.evidence = ["ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜"]
                        self.caveats = ["ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."]
                        self.quotes = []
            
            return FallbackResponse()


# ì „ì—­ LLM ë˜í¼ ì¸ìŠ¤í„´ìŠ¤
_langsmith_llm_wrapper = None


def get_langsmith_llm() -> LangSmithLLMWrapper:
    """LangSmith ì¶”ì ì´ í¬í•¨ëœ LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _langsmith_llm_wrapper
    if _langsmith_llm_wrapper is None:
        _langsmith_llm_wrapper = LangSmithLLMWrapper()
    return _langsmith_llm_wrapper


def get_llm_with_tracing():
    """ê¸°ì¡´ get_llm() í•¨ìˆ˜ì˜ LangSmith ì¶”ì  ë²„ì „"""
    return get_langsmith_llm()