# deps.py â€” Gemini 503 & Structured Output hotfixes
# -------------------------------------------------
# ë³€ê²½ ìš”ì•½:
# 1) New SDK ì‚¬ìš© ì‹œ response_schemaì— Pydantic í´ë˜ìŠ¤ ìì²´ë¥¼ ì „ë‹¬(ë”•ì…”ë„ˆë¦¬ ì•„ë‹˜)
#    â†’ additionalProperties ê´€ë ¨ ì˜¤ë¥˜ ë° ë¹ˆ properties ì˜¤ë¥˜ ë°©ì§€
# 2) Legacy SDK ê²½ë¡œëŠ” JSON Schemaë¥¼ ì“°ë˜, $defsë¥¼ í—ˆìš©í•˜ê³ 
#    additionalProperties í‚¤ë¥¼ ëª¨ë“  ê¹Šì´ì—ì„œ ì œê±°í•˜ë„ë¡ sanitizer ê°•í™”
# 3) list_models ì‹¤íŒ¨ ì‹œ [] ëŒ€ì‹  Noneì„ ë°˜í™˜í•˜ì—¬ "ëª¨ë¸ ëª©ë¡ì— ì—†ìŒ" ê²½ê³ ê°€ ì˜ëª» ì°íˆëŠ” ë¬¸ì œ ë°©ì§€
# 4) 503 ì¬ì‹œë„ ì•ˆì •í™”(ë©”ì‹œì§€ì— 'unavailable' ë§¤ì¹­ ì¶”ê°€)

from functools import lru_cache
from pydantic_settings import BaseSettings
from typing import Optional, List, Any
import logging
import redis
import time
import random

logger = logging.getLogger(__name__)

# ìŠ¤í‚¤ë§ˆ Sanitizer - ì§€ì›í•˜ì§€ ì•ŠëŠ” í‚¤ë“¤ì„ ì œê±°
_ALLOWED_SCHEMA_KEYS = {
    "type", "properties", "required", "items", "enum",
    "anyOf", "oneOf", "description", "$ref", "$defs"  # â† $defs ë³´ì¡´ ì¶”ê°€
}


def _sanitize_schema(obj):
    """
    Pydantic model_json_schema() ê²°ê³¼ì—ì„œ ì§€ì›í•˜ì§€ ì•ŠëŠ” í‚¤ë“¤ì„ ì œê±°.
    - ëª¨ë“  ê¹Šì´ì—ì„œ additionalProperties ì œê±°
    - $refì™€ $defsëŠ” ë³´ì¡´ (ì°¸ì¡° ê¹¨ì§ ë°©ì§€)
    """
    if isinstance(obj, dict):
        out = {}
        for k, v in obj.items():
            # ëª¨ë“  ê¹Šì´ì—ì„œ additionalProperties ì œê±°
            if k == "additionalProperties":
                continue
            if k == "properties" and isinstance(v, dict):
                out[k] = {name: _sanitize_schema(s) for name, s in v.items()}
            elif k in _ALLOWED_SCHEMA_KEYS:
                out[k] = _sanitize_schema(v)
            # ê·¸ ì™¸ í‚¤ëŠ” ì œê±°
        return out
    elif isinstance(obj, list):
        return [_sanitize_schema(i) for i in obj]
    else:
        return obj


def _is_retryable_error(error: Exception) -> bool:
    """ì¬ì‹œë„ ê°€ëŠ¥í•œ ì˜¤ë¥˜ì¸ì§€ íŒë‹¨."""
    error_str = str(error).lower()
    retryable_patterns = [
        "503", "502", "504", "500", "429",
        "timeout", "connection", "network",
        "rate limit", "quota", "exceeded",
        "service unavailable", "internal server error", "unavailable",
        "temporarily", "retry", "backoff"  # ì¶”ê°€ íŒ¨í„´
    ]
    is_retryable = any(p in error_str for p in retryable_patterns)
    logger.info(f"ğŸ” ì˜¤ë¥˜ ì¬ì‹œë„ ê°€ëŠ¥ì„± ê²€ì‚¬: '{error_str}' -> {is_retryable}")
    return is_retryable


def _exponential_backoff(attempt: int, base_delay: float = 1.0, max_delay: float = 60.0) -> float:
    delay = min(base_delay * (2 ** attempt) + random.uniform(0, 1), max_delay)
    return delay


def _retry_with_backoff(func, max_retries: int = 3, base_delay: float = 1.0):
    for attempt in range(max_retries + 1):
        try:
            return func()
        except Exception as e:
            error_str = str(e)
            logger.warning(f"ğŸ” API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries + 1}): {error_str}")
            
            if attempt == max_retries or not _is_retryable_error(e):
                logger.error(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ë˜ëŠ” ì¬ì‹œë„ ë¶ˆê°€ëŠ¥í•œ ì˜¤ë¥˜: {error_str}")
                raise e
            
            delay = _exponential_backoff(attempt, base_delay)
            logger.info(f"â³ {delay:.2f}ì´ˆ í›„ ì¬ì‹œë„...")
            time.sleep(delay)


# 1) ìƒˆ SDK ìš°ì„ , êµ¬ SDKëŠ” ë°±ì—…
try:
    from google import genai  # new SDK
    from google.genai import types as genai_types
    _USE_NEW_SDK = True
except Exception:
    import google.generativeai as genai  # legacy SDK
    _USE_NEW_SDK = False


class Settings(BaseSettings):
    ENV: str = "dev"
    REDIS_URL: Optional[str] = "redis://localhost:6379"
    VECTOR_DIR: str = "data/vector_db"
    DOCUMENT_DIR: str = "data/documents"

    GEMINI_API_KEY: str = ""
    GEMINI_MODEL: str = "gemini-2.5-flash"

    TAVILY_API_KEY: str = ""

    REDIS_SESSION_TTL: int = 3600
    REDIS_CACHE_TTL: int = 1800

    class Config:
        env_file = ".env"
        extra = "ignore"


@lru_cache()
def get_settings() -> Settings:
    return Settings()


class LangSmithLLMWrapper:
    def __init__(self, client_or_model, model_name: str):
        self._backend = client_or_model
        self.model_name = model_name

    def with_structured_output(self, response_schema, emergency_fallback: bool = False):
        return StructuredOutputWrapper(self._backend, self.model_name, response_schema, emergency_fallback)

    def generate_content(self, prompt: str, **kwargs):
        if _USE_NEW_SDK:
            resp = self._backend.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=kwargs.get("config")
            )
            return type("Resp", (), {"text": getattr(resp, "text", None) or str(resp)})
        else:
            resp = self._backend.generate_content(prompt, **kwargs)
            return type("Resp", (), {"text": getattr(resp, "text", None) or str(resp)})


class StructuredOutputWrapper:
    def __init__(self, client_or_model, model_name: str, response_schema, emergency_fallback: bool = False):
        self._backend = client_or_model
        self.model_name = model_name
        self.response_schema = response_schema  # Pydantic ëª¨ë¸ í´ë˜ìŠ¤ ê¸°ëŒ€
        self.emergency_fallback = emergency_fallback

    def generate_content(self, prompt: str, **kwargs):
        try:
            print(f"ğŸ” [StructuredOutputWrapper] ì‹œì‘ - ëª¨ë¸: {self.model_name}")
            print(f"ğŸ” [StructuredOutputWrapper] í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")
            print(f"ğŸ” [StructuredOutputWrapper] ìŠ¤í‚¤ë§ˆ: {self.response_schema.__name__}")
            print(f"ğŸ” [StructuredOutputWrapper] ê¸´ê¸‰ íƒˆì¶œ ëª¨ë“œ: {self.emergency_fallback}")

            if self.emergency_fallback:
                def _generate_emergency_content():
                    if _USE_NEW_SDK:
                        return self._backend.models.generate_content(
                            model=self.model_name,
                            contents=prompt,
                            config=kwargs.get("config")
                        )
                    else:
                        return self._backend.generate_content(prompt, **kwargs)
                resp = _retry_with_backoff(_generate_emergency_content, max_retries=2, base_delay=0.5)
                raw_text = getattr(resp, "text", None) or str(resp)
                from json import loads
                try:
                    data = loads(raw_text)
                except Exception:
                    # ë¹„êµ¬ì¡° í…ìŠ¤íŠ¸ë¥¼ í›„ì²˜ë¦¬ ë§¤í•‘ (ê¸°ì¡´ ìœ í‹¸ ì‚¬ìš©)
                    return _parse_unstructured_response(raw_text, self.response_schema)
                return self.response_schema(**data)

            if _USE_NEW_SDK:
                # âœ… New SDK: Pydantic í´ë˜ìŠ¤ ìì²´ë¥¼ ì „ë‹¬ (dict ì•„ë‹˜)
                config = genai_types.GenerateContentConfig(
                    response_mime_type="application/json",
                    response_schema=self.response_schema,
                )
                print("ğŸ” [StructuredOutputWrapper] New SDK ì‚¬ìš© - config ìƒì„± ì™„ë£Œ")

                def _generate_content():
                    return self._backend.models.generate_content(
                        model=self.model_name,
                        contents=prompt,
                        config=config,
                    )
                resp = _retry_with_backoff(_generate_content, max_retries=3, base_delay=1.0)
                raw = getattr(resp, "text", None) or "{}"
                print(f"ğŸ” [StructuredOutputWrapper] New SDK ì‘ë‹µ íƒ€ì…: {type(resp)}")
            else:
                # Legacy SDK: dict(JSON Schema) í•„ìš” â†’ sanitize
                gm = self._backend
                schema = _sanitize_schema(self.response_schema.model_json_schema())
                # quotes ê°™ì€ ë³µì¡ í•„ë“œ ë‹¤ìš´ìºìŠ¤íŒ… í•„ìš” ì‹œ ì—¬ê¸°ì„œ ì²˜ë¦¬
                generation_config = {
                    "response_mime_type": "application/json",
                    "response_schema": schema,
                }
                print("ğŸ” [StructuredOutputWrapper] Legacy SDK ì‚¬ìš© - generation_config ìƒì„± ì™„ë£Œ")
                def _generate_content():
                    return gm.generate_content(prompt, generation_config=generation_config)
                resp = _retry_with_backoff(_generate_content, max_retries=3, base_delay=1.0)
                raw = getattr(resp, "text", None) or "{}"
                print(f"ğŸ” [StructuredOutputWrapper] Legacy SDK ì‘ë‹µ íƒ€ì…: {type(resp)}")

            print(f"ğŸ” [StructuredOutputWrapper] ì›ë³¸ ì‘ë‹µ í…ìŠ¤íŠ¸: '{raw}'")
            print(f"ğŸ” [StructuredOutputWrapper] ì‘ë‹µ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(raw)}ì")
            from json import loads
            data = loads(raw)
            print(f"ğŸ” [StructuredOutputWrapper] JSON íŒŒì‹± ê²°ê³¼: {data}")
            result = self.response_schema(**data)
            print(f"ğŸ” [StructuredOutputWrapper] Pydantic ê°ì²´ ìƒì„± ì„±ê³µ: {result}")
            return result

        except Exception as e:
            print(f"âŒ [StructuredOutputWrapper] ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            import traceback; print(f"âŒ [StructuredOutputWrapper] ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            try:
                print("ğŸ”§ [StructuredOutputWrapper] ê¸°ë³¸ê°’ ìƒì„± ì‹œë„")
                return self.response_schema()
            except Exception:
                # í•„ë“œ íƒ€ì… ê¸°ë°˜ ìˆ˜ë™ ê¸°ë³¸ê°’
                fallback = {}
                for name, f in self.response_schema.model_fields.items():
                    t = f.annotation
                    if t is int:
                        fallback[name] = 0
                    elif t is float:
                        fallback[name] = 0.0
                    elif t is bool:
                        fallback[name] = False
                    elif t is str:
                        fallback[name] = ""
                    elif getattr(t, "__origin__", None) is list:
                        fallback[name] = []
                    elif getattr(t, "__origin__", None) is dict:
                        fallback[name] = {}
                    else:
                        fallback[name] = None
                return self.response_schema(**fallback)


# -----------------------------
# ëª¨ë¸ ì„ íƒ/ìƒì„±
# -----------------------------

def _normalize_candidates(name: str) -> List[str]:
    base = (name or "").strip()
    if base.startswith("projects/") or "generativelanguage" in base.lower():
        base = "gemini-2.5-flash"
    if base.startswith("models/"):
        base = base.split("models/", 1)[1]
    prefer = [
        base,
        "gemini-2.5-flash",
        "gemini-2.5-pro",
        "gemini-2.5-flash-lite",
        "gemini-2.0-flash",
        "gemini-2.0-pro",
        "gemini-1.5-flash-002",
        "gemini-1.5-pro-002",
    ]
    out, seen = [], set()
    for c in prefer:
        if c and c not in seen:
            seen.add(c); out.append(c)
    return out


def _supports_generate_content(meta: Any) -> bool:
    methods = getattr(meta, "supported_generation_methods", None) or \
              getattr(meta, "supported_methods", None)
    if isinstance(methods, (list, tuple, set)):
        return any("generateContent" in m or "generate_content" in m for m in methods)
    return True


def _list_available_model_names(api_key: str) -> Optional[List[str]]:
    try:
        if _USE_NEW_SDK:
            client = genai.Client(api_key=api_key)
            models = list(client.models.list())
            # New SDKì—ì„œëŠ” models/ ì ‘ë‘ì‚¬ê°€ í¬í•¨ëœ ì „ì²´ ì´ë¦„ì„ ë°˜í™˜í•˜ë¯€ë¡œ ì œê±°
            names = []
            for m in models:
                if _supports_generate_content(m):
                    n = getattr(m, "name", "")
                    # models/ ì ‘ë‘ì‚¬ ì œê±°í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
                    clean_name = n.split("models/")[-1] if n.startswith("models/") else n
                    names.append(clean_name)
            return names
        else:
            genai.configure(api_key=api_key)
            models = list(genai.list_models())
            names = []
            for m in models:
                if _supports_generate_content(m):
                    n = getattr(m, "name", "")
                    names.append(n.split("models/")[-1] if n else n)
            return names
    except Exception as e:
        logger.warning(f"list_models ì‹¤íŒ¨: {e}")
        return None  # â† ë¹ˆ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  Noneìœ¼ë¡œ ë°˜í™˜í•´ í•„í„° ë¹„í™œì„±í™”


@lru_cache()
def get_llm():
    s = get_settings()
    if not s.GEMINI_API_KEY:
        raise RuntimeError("GEMINI_API_KEY is not set in .env")

    try:
        if _USE_NEW_SDK:
            client = genai.Client(api_key=s.GEMINI_API_KEY)
        else:
            genai.configure(api_key=s.GEMINI_API_KEY)
            client = None
    except Exception as e:
        logger.error(f"âŒ SDK ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise RuntimeError(f"Gemini SDK ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    try:
        available = _list_available_model_names(s.GEMINI_API_KEY)
        if available:
            logger.info(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {set(available)}")
        else:
            logger.warning("âš ï¸ ëª¨ë¸ ëª©ë¡ì´ ë¹„ì–´ìˆìŒ - API ì—°ê²° ë¬¸ì œì¼ ìˆ˜ ìˆìŒ")
    except Exception as e:
        logger.warning(f"âš ï¸ ëª¨ë¸ ëª©ë¡ í™•ì¸ ì‹¤íŒ¨: {e}")
        available = None

    candidates = _normalize_candidates(s.GEMINI_MODEL)
    logger.info(f"ğŸ” ì‹œë„í•  ëª¨ë¸ í›„ë³´: {candidates}")

    last_err = None
    for cand in candidates:
        # â† ëª¨ë¸ ëª©ë¡ ì¡°íšŒì— ì‹¤íŒ¨(None)ë©´ ìŠ¤í‚µí•˜ì§€ ì•ŠìŒ
        if available is not None and (cand not in set(available)):
            logger.warning(f"âš ï¸ ëª¨ë¸ {cand}ì´ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì— ì—†ìŒ")
            continue
        try:
            if _USE_NEW_SDK:
                logger.info(f"ğŸ”„ ëª¨ë¸ ì‹œë„: {cand}")
                # ê°€ë²¼ìš´ í—¬ìŠ¤ì²´í¬ (ì¬ì‹œë„ ë˜í•‘)
                def _count_tokens():
                    return client.models.count_tokens(model=cand, contents="ping")
                _retry_with_backoff(_count_tokens, max_retries=3, base_delay=0.5)
                logger.info(f"âœ… ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥: {cand}")
                return LangSmithLLMWrapper(client, cand)
            else:
                logger.info(f"ğŸ”„ ëª¨ë¸ ì‹œë„: {cand}")
                gm = genai.GenerativeModel(cand)
                def _count_tokens():
                    return gm.count_tokens("ping")
                _retry_with_backoff(_count_tokens, max_retries=3, base_delay=0.5)
                logger.info(f"âœ… ëª¨ë¸ ìƒì„± ì„±ê³µ: {cand}")
                return LangSmithLLMWrapper(gm, cand)
        except Exception as e:
            last_err = e
            error_str = str(e).lower()
            if "api_key" in error_str or "authentication" in error_str:
                logger.error(f"âŒ API í‚¤ ì¸ì¦ ì‹¤íŒ¨: {e}")
                raise RuntimeError(f"Gemini API í‚¤ ì¸ì¦ ì‹¤íŒ¨: {e}")
            elif "quota" in error_str or "limit" in error_str:
                logger.error(f"âŒ API í• ë‹¹ëŸ‰ ì´ˆê³¼: {e}")
                raise RuntimeError(f"Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼: {e}")
            elif "permission" in error_str or "access" in error_str:
                logger.error(f"âŒ ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œ ì—†ìŒ: {e}")
                raise RuntimeError(f"ëª¨ë¸ {cand}ì— ëŒ€í•œ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤: {e}")
            elif "503" in error_str or "unavailable" in error_str:
                logger.warning(f"âš ï¸ ëª¨ë¸ {cand} ì„œë¹„ìŠ¤ ì¼ì‹œ ì¤‘ë‹¨ (503): {e}")
                logger.info("ğŸ”„ ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì‹œë„í•˜ê±°ë‚˜ ì ì‹œ í›„ ì¬ì‹œë„í•˜ì„¸ìš”")
                continue
            else:
                logger.warning(f"âŒ ëª¨ë¸ {cand} ì‹¤íŒ¨: {e}")
                continue

    error_details = []
    if last_err:
        error_details.append(f"ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_err}")
    if available is None:
        error_details.append("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ í™•ì¸í•˜ì§€ ëª»í•¨(list_models ì‹¤íŒ¨)")
    if not s.GEMINI_API_KEY:
        error_details.append("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")

    error_msg = f"ëª¨ë“  Gemini ëª¨ë¸ ì‹œë„ ì‹¤íŒ¨. {' | '.join(error_details)}"
    logger.error(f"âŒ {error_msg}")
    raise RuntimeError(error_msg)


def get_available_models():
    s = get_settings()
    try:
        return _list_available_model_names(s.GEMINI_API_KEY) or []
    except Exception as e:
        logger.error(f"ëª¨ë¸ ëª©ë¡ í™•ì¸ ì‹¤íŒ¨: {e}")
        return []


@lru_cache()
def get_redis_client():
    s = get_settings()
    if not s.REDIS_URL:
        raise RuntimeError("REDIS_URL is not set in .env")
    try:
        client = redis.from_url(
            s.REDIS_URL,
            decode_responses=False,
            socket_connect_timeout=5,
            socket_timeout=5,
            retry_on_timeout=True,
        )
        client.ping()
        return client
    except Exception as e:
        logger.error(f"Redis ì—°ê²° ì‹¤íŒ¨: {e}")
        return None
