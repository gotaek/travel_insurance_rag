import os, re, json
from pathlib import Path
from typing import List, Dict, Any, Tuple
from tqdm import tqdm
import numpy as np

import fitz  # PyMuPDF
import pdfplumber

try:
    import chromadb
    from chromadb.config import Settings
except Exception:
    chromadb = None

from sklearn.feature_extraction.text import TfidfVectorizer
from retriever.embeddings import embed_texts

DOC_DIR = os.getenv("DOCUMENT_DIR", "data/documents")
OUT_DIR = os.getenv("VECTOR_DIR", "data/vector_db")
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "800"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "120"))

COLLECTION_NAME = "insurance_docs"

# ex) 2025_DBì†ë³´_ì—¬í–‰ìë³´í—˜ì•½ê´€.pdf
FNAME_RE = re.compile(r"(?P<year>\d{4})[_-](?P<insurer>[^_-]+)[_-](?P<title>.+)\.pdf$", re.IGNORECASE)

# í—¤ë”©/ì„¹ì…˜ ê·œì¹™
RE_ARTICLE = re.compile(r"^\s*ì œ\s*\d+\s*ì¡°")
RE_CHAPTER = re.compile(r"^\s*ì œ\s*\d+\s*ê´€")
RE_SPECIAL = re.compile(r"^\s*íŠ¹ë³„ì•½ê´€")
RE_APPENDIX = re.compile(r"^\s*(ë¶€ë¡|ë³„í‘œ)")

KEYWORD_WHITELIST = [
    "ì‚¬ë§", "í›„ìœ ì¥í•´", "ì§ˆë³‘", "ìƒí•´", "ì§€ì—°", "ê²°í•­", "ìˆ˜í•˜ë¬¼", "ìê¸°ë¶€ë‹´",
    "ëŒ€ê¸°ê¸°ê°„", "ë©´ì±…", "ë°°ìƒì±…ì„", "ì‹¤ì†ì˜ë£Œë¹„", "ì‹ì¤‘ë…", "ê°ì—¼ë³‘", "ì—¬ê¶Œë¶„ì‹¤",
    "ë°˜ë ¤ê²¬", "ì§€ìˆ˜í˜•", "íŠ¹ì •ê°ì—¼ë³‘", "ì…ì›", "êµ¬ì¡°ì†¡í™˜"
]

def _parse_filename_meta(fp: Path) -> Dict[str, Any]:
    m = FNAME_RE.search(fp.name)
    if not m:
        return {"version_year": None, "insurer": None, "title": fp.stem}
    d = m.groupdict()
    return {
        "version_year": d["year"],
        "insurer": d["insurer"],
        "title": d["title"].replace("_"," ").replace("-"," ").strip(),
    }

def _section_type_by_heading(text: str) -> str:
    if RE_APPENDIX.search(text): return "appendix"
    if "ë³„í‘œ" in text[:20] or "ë¶€ë¡" in text[:20]: return "appendix"
    return "body"

def _chunk_text(txt: str, size: int, overlap: int) -> List[str]:
    if len(txt) <= size:
        return [txt]
    chunks = []
    i = 0
    while i < len(txt):
        chunks.append(txt[i:i+size])
        i += max(1, size - overlap)
        if i >= len(txt):
            break
    return chunks

def _keyword_tags(docs: List[str], topk: int = 8) -> List[List[str]]:
    if not docs:
        return [[]]
    # TF-IDF + í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë³´ê°•
    vec = TfidfVectorizer(max_df=0.9, min_df=2, ngram_range=(1,2))
    try:
        X = vec.fit_transform(docs)
        terms = np.array(vec.get_feature_names_out())
    except ValueError:
        return [[kw for kw in KEYWORD_WHITELIST if kw in d] for d in docs]

    tags: List[List[str]] = []
    for row in range(X.shape[0]):
        data = X[row].toarray().ravel()
        idx = data.argsort()[::-1][:topk]
        tfidf_tags = [t for t in terms[idx] if len(t) > 1]
        wl = [kw for kw in KEYWORD_WHITELIST if kw in docs[row]]
        # í•©ì¹˜ë˜ë„ë¡ ì¤‘ë³µ ì œê±°
        uniq = []
        for t in wl + tfidf_tags:
            if t not in uniq:
                uniq.append(t)
        tags.append(uniq[:topk])
    return tags

def _extract_tables(pdf_path: Path) -> Dict[int, List[Dict[str, Any]]]:
    """
    í˜ì´ì§€ë³„ í…Œì´ë¸”(í–‰ ë¦¬ìŠ¤íŠ¸) ì¶”ì¶œ.
    - pdfplumber tables: ê° í–‰ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ â†’ í—¤ë”/ì…€ ì •ë¦¬ í›„ JSONí™”
    ë°˜í™˜ êµ¬ì¡°: {page_index(1-base): [ { "rows": [[...],[...]], "bbox": (x0,y0,x1,y1) }, ... ]}
    """
    results: Dict[int, List[Dict[str, Any]]] = {}
    try:
        with pdfplumber.open(str(pdf_path)) as pdf:
            for i, page in enumerate(pdf.pages, start=1):
                try:
                    tables = page.extract_tables()
                except Exception:
                    tables = []
                items = []
                for tb in tables or []:
                    rows = []
                    for r in tb:
                        rows.append([ (c or "").strip() for c in r ])
                    if rows:
                        items.append({"rows": rows})
                if items:
                    results[i] = items
    except Exception:
        pass
    return results

def _blocks_from_pymupdf(pdf_path: Path) -> List[Dict[str, Any]]:
    """
    PyMuPDFë¡œ pageâ†’blocks í…ìŠ¤íŠ¸ ì¶”ì¶œ.
    blockâ†’lineâ†’span ìˆœì„œë¡œ í…ìŠ¤íŠ¸ë¥¼ ì—°ê²°í•˜ë©°, í°íŠ¸/í¬ê¸° ë“±ì€ í•„ìš” ì‹œ í™•ì¥.
    """
    out = []
    meta = _parse_filename_meta(pdf_path)
    doc_id = f"{meta['insurer']}_{meta['version_year']}_{meta['title']}".strip()
    with fitz.open(str(pdf_path)) as doc:
        for pno, page in enumerate(doc, start=1):
            raw = page.get_text("dict")
            for b in raw.get("blocks", []):
                if "lines" not in b:  # ì´ë¯¸ì§€/ê·¸ë¦¼
                    continue
                texts = []
                for ln in b["lines"]:
                    spans = ln.get("spans", [])
                    line_text = "".join([s.get("text","") for s in spans]).strip()
                    if line_text:
                        texts.append(line_text)
                text = "\n".join(texts).strip()
                if not text:
                    continue
                out.append({
                    "doc_id": doc_id,
                    "insurer": meta["insurer"],
                    "version": meta["version_year"],
                    "title": meta["title"],
                    "page": pno,
                    "text": text,
                })
    return out

def _label_sections(blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    í—¤ë”© ê·œì¹™/í‚¤ì›Œë“œë¡œ body/appendix ë¼ë²¨. í‘œëŠ” pdfplumber ê²°ê³¼ì— ë”°ë¡œ í•©ë¥˜.
    """
    labeled = []
    last_heading = None
    for b in blocks:
        t = b["text"].splitlines()[0] if b["text"] else ""
        if RE_SPECIAL.search(t) or RE_CHAPTER.search(t) or RE_ARTICLE.search(t) or RE_APPENDIX.search(t):
            last_heading = t
        section_type = "body"
        if last_heading and RE_APPENDIX.search(last_heading):
            section_type = "appendix"
        elif RE_APPENDIX.search(t):
            section_type = "appendix"
        labeled.append({**b, "section_type": section_type, "heading": last_heading})
    return labeled

def _merge_tables(labeled_blocks: List[Dict[str, Any]], table_map: Dict[int, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    """
    pdfplumberì—ì„œ ë½‘ì€ í…Œì´ë¸”ì„ section_type='table' ë¡œ ì‚½ì….
    - ê° í˜ì´ì§€ ëì— ì¶”ê°€(ê°„ë‹¨). ê³ ê¸‰ ë§¤ì¹­(ì¢Œí‘œ ê·¼ì ‘)ì€ í›„ì† ê°œì„ ì—ì„œ.
    """
    merged: List[Dict[str, Any]] = []
    by_page: Dict[int, List[Dict[str, Any]]] = {}
    for b in labeled_blocks:
        by_page.setdefault(b["page"], []).append(b)

    for pno in sorted(by_page.keys()):
        merged.extend(by_page[pno])
        for tb in table_map.get(pno, []):
            text_rows = [" | ".join(r) for r in tb["rows"]]
            merged.append({**by_page[pno][0], "section_type": "table", "text": "\n".join(text_rows)})
    return merged

def _build_index(chunks_meta: List[Dict[str, Any]]):
    """Chroma DBì— ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• - multilingual-e5-small-ko ëª¨ë¸ ì‚¬ìš©"""
    os.makedirs(OUT_DIR, exist_ok=True)
    texts = [c["text"] for c in chunks_meta]
    if not texts:
        print("âš ï¸ No text chunks parsed. Abort.")
        return
    
    if chromadb is None:
        print("âš ï¸ chromadb not available. Skipping index build.")
        return
    
    try:
        # Chroma DB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = chromadb.PersistentClient(
            path=OUT_DIR,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸ ë° ì²˜ë¦¬
        collection = None
        try:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆëŠ”ì§€ í™•ì¸
            collection = client.get_collection(COLLECTION_NAME)
            print(f"ğŸ“‹ Found existing collection: {COLLECTION_NAME}")
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
            client.delete_collection(COLLECTION_NAME)
            print(f"ğŸ—‘ï¸ Deleted existing collection: {COLLECTION_NAME}")
        except Exception:
            print(f"ğŸ“‹ No existing collection found: {COLLECTION_NAME}")
        
        # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
        collection = client.create_collection(
            name=COLLECTION_NAME,
            metadata={"description": "ì—¬í–‰ìë³´í—˜ ë¬¸ì„œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤"}
        )
        print(f"âœ¨ Created new collection: {COLLECTION_NAME}")
        
        # ë¬¸ì„œ ID ìƒì„± ë° ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        doc_ids = [f"doc_{i}" for i in range(len(chunks_meta))]
        metadatas = []
        
        for chunk in chunks_meta:
            # Chroma DB ë©”íƒ€ë°ì´í„°ëŠ” ë¬¸ìì—´ ê°’ë§Œ í—ˆìš©
            metadata = {}
            for key, value in chunk.items():
                if key != "text" and isinstance(value, (str, int, float, bool)):
                    metadata[key] = str(value)
            metadatas.append(metadata)
        
        # multilingual-e5-small-ko ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ìƒì„±
        print("ğŸ”„ Generating embeddings with multilingual-e5-small-ko model...")
        embeddings = embed_texts(texts)
        
        # ì»¬ë ‰ì…˜ì— ë¬¸ì„œì™€ ì„ë² ë”© ì¶”ê°€
        collection.add(
            documents=texts,
            metadatas=metadatas,
            ids=doc_ids,
            embeddings=embeddings.tolist()
        )
        
        print(f"âœ… Built Chroma DB index: {len(chunks_meta)} chunks â†’ {OUT_DIR}/{COLLECTION_NAME}")
        
    except Exception as e:
        print(f"âŒ Failed to build Chroma DB index: {e}")
        return

def main():
    pdfs = sorted([p for p in Path(DOC_DIR).glob("*.pdf")])
    if not pdfs:
        print(f"âš ï¸ No PDFs under {DOC_DIR}. Place files like '2025_ë³´í—˜ì‚¬_ë¬¸ì„œì œëª©.pdf'")
        return

    all_sections: List[Dict[str, Any]] = []
    for p in tqdm(pdfs, desc="Parsing PDFs (PyMuPDF+pdfplumber)"):
        blocks = _blocks_from_pymupdf(p)
        labeled = _label_sections(blocks)
        tables = _extract_tables(p)
        merged = _merge_tables(labeled, tables)
        all_sections.extend(merged)

    # ì„¹ì…˜ í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ íƒœê¹…
    section_texts = [s["text"] for s in all_sections]
    section_tags = _keyword_tags(section_texts, topk=8)
    for s, tags in zip(all_sections, section_tags):
        s["tags"] = tags

    # ì²­í‚¹(+ë©”íƒ€ ì „ê°œ)
    chunks_meta: List[Dict[str, Any]] = []
    for s in tqdm(all_sections, desc="Chunking"):
        # í‘œëŠ” í–‰ ë‹¨ìœ„ë¡œ ì´ë¯¸ ì§§ì€ í¸ â†’ ë°”ë¡œ ì €ì¥. (í¬ë©´ ì¼ë°˜ ì²­í‚¹)
        if s.get("section_type") == "table" and len(s["text"]) <= CHUNK_SIZE:
            m = dict(s)
            m["chunk_no"] = 1
            chunks_meta.append(m)
        else:
            for i, ch in enumerate(_chunk_text(s["text"], CHUNK_SIZE, CHUNK_OVERLAP), start=1):
                m = dict(s)
                m["text"] = ch
                m["chunk_no"] = i
                chunks_meta.append(m)

    _build_index(chunks_meta)

if __name__ == "__main__":
    main()