import os, re, json
from pathlib import Path
from typing import List, Dict, Any, Tuple
from tqdm import tqdm
import numpy as np

import fitz  # PyMuPDF

try:
    import chromadb
    from chromadb.config import Settings
except Exception:
    chromadb = None

from sklearn.feature_extraction.text import TfidfVectorizer
from retriever.embeddings import embed_texts

# LangChain text splitters ì¶”ê°€
from langchain_text_splitters import RecursiveCharacterTextSplitter

DOC_DIR = os.getenv("DOCUMENT_DIR", "data/documents")
OUT_DIR = os.getenv("VECTOR_DIR", "data/vector_db")
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "800"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "120"))

COLLECTION_NAME = "insurance_docs"

# ex) 2025_DBì†ë³´_ì—¬í–‰ìë³´í—˜ì•½ê´€.pdf
FNAME_RE = re.compile(r"(?P<year>\d{4})[_-](?P<insurer>[^_-]+)[_-](?P<title>.+)\.pdf$", re.IGNORECASE)

# í—¤ë”©/ì„¹ì…˜ ê·œì¹™
RE_ARTICLE = re.compile(r"^\s*ì œ\s*\d+\s*ì¡°")
RE_CHAPTER = re.compile(r"^\s*ì œ\s*\d+\s*ê´€")
RE_SPECIAL = re.compile(r"^\s*íŠ¹ë³„ì•½ê´€")
RE_APPENDIX = re.compile(r"^\s*(ë¶€ë¡|ë³„í‘œ)")

KEYWORD_WHITELIST = [
    "ì‚¬ë§", "í›„ìœ ì¥í•´", "ì§ˆë³‘", "ìƒí•´", "ì§€ì—°", "ê²°í•­", "ìˆ˜í•˜ë¬¼", "ìê¸°ë¶€ë‹´",
    "ëŒ€ê¸°ê¸°ê°„", "ë©´ì±…", "ë°°ìƒì±…ì„", "ì‹¤ì†ì˜ë£Œë¹„", "ì‹ì¤‘ë…", "ê°ì—¼ë³‘", "ì—¬ê¶Œë¶„ì‹¤",
    "ë°˜ë ¤ê²¬", "ì§€ìˆ˜í˜•", "íŠ¹ì •ê°ì—¼ë³‘", "ì…ì›", "êµ¬ì¡°ì†¡í™˜"
]

def _parse_filename_meta(fp: Path) -> Dict[str, Any]:
    m = FNAME_RE.search(fp.name)
    if not m:
        return {"version_year": None, "insurer": None, "title": fp.stem}
    d = m.groupdict()
    return {
        "version_year": d["year"],
        "insurer": d["insurer"],
        "title": d["title"].replace("_"," ").replace("-"," ").strip(),
    }

def _section_type_by_heading(text: str) -> str:
    if RE_APPENDIX.search(text): return "appendix"
    if "ë³„í‘œ" in text[:20] or "ë¶€ë¡" in text[:20]: return "appendix"
    return "body"

def _merge_small_blocks(blocks: List[Dict[str, Any]], min_size: int = 700) -> List[Dict[str, Any]]:
    """
    ì‘ì€ ë¸”ë¡ë“¤ì„ ë³‘í•©í•˜ì—¬ ì˜ë¯¸ ìˆëŠ” í…ìŠ¤íŠ¸ ìƒì„± (ê°œì„ ëœ ë²„ì „)
    
    Args:
        blocks: PDFì—ì„œ ì¶”ì¶œëœ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸
        min_size: ìµœì†Œ ë¸”ë¡ í¬ê¸° (ë¬¸ì ìˆ˜) - 700ìë¡œ ì¦ê°€
        
    Returns:
        ë³‘í•©ëœ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸
    """
    if not blocks:
        return []
    
    merged_blocks = []
    current_block = None
    current_text = ""
    
    for block in blocks:
        block_text = block["text"].strip()
        
        # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ê±´ë„ˆë›°ê¸°
        if not block_text:
            continue
            
        # í˜„ì¬ ë¸”ë¡ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ì‹œì‘
        if current_block is None:
            current_block = dict(block)
            current_text = block_text
        else:
            # í˜„ì¬ í…ìŠ¤íŠ¸ì™€ í•©ì³¤ì„ ë•Œ ìµœì†Œ í¬ê¸° ë¯¸ë§Œì´ë©´ ë³‘í•©
            if len(current_text) + len(block_text) < min_size * 1.5:  # 1.5ë°°ë¡œ ì œí•œ ì™„í™”
                current_text += "\n" + block_text
            else:
                # í˜„ì¬ê¹Œì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ë¸”ë¡ìœ¼ë¡œ ì €ì¥
                current_block["text"] = current_text
                merged_blocks.append(current_block)
                
                # ìƒˆ ë¸”ë¡ ì‹œì‘
                current_block = dict(block)
                current_text = block_text
    
    # ë§ˆì§€ë§‰ ë¸”ë¡ ì²˜ë¦¬
    if current_block and current_text:
        current_block["text"] = current_text
        merged_blocks.append(current_block)
    
    # ğŸ”§ ê°œì„ : ìµœì¢… ê²€ì¦ - ë„ˆë¬´ ì‘ì€ ë¸”ë¡ì€ ë‹¤ìŒ ë¸”ë¡ê³¼ ê°•ì œ ë³‘í•©
    final_blocks = []
    i = 0
    while i < len(merged_blocks):
        current_block = merged_blocks[i]
        current_text = current_block["text"]
        
        if len(current_text) < min_size and i + 1 < len(merged_blocks):
            # ë‹¤ìŒ ë¸”ë¡ê³¼ ê°•ì œ ë³‘í•©
            next_block = merged_blocks[i + 1]
            merged_text = current_text + "\n" + next_block["text"]
            
            # ë³‘í•©ëœ ë¸”ë¡ì´ ë„ˆë¬´ í¬ë©´ ë¶„í• 
            if len(merged_text) > min_size * 2:
                # ì¤‘ê°„ì ì—ì„œ ë¶„í• 
                mid_point = len(merged_text) // 2
                # ë¬¸ì¥ ê²½ê³„ì—ì„œ ë¶„í• ì  ì°¾ê¸°
                for offset in range(0, len(merged_text) // 4):
                    if mid_point - offset > 0:
                        if merged_text[mid_point - offset:mid_point - offset + 2] == '. ':
                            mid_point = mid_point - offset + 2
                            break
                    if mid_point + offset < len(merged_text):
                        if merged_text[mid_point + offset:mid_point + offset + 2] == '. ':
                            mid_point = mid_point + offset + 2
                            break
                
                # ë¶„í•  ì‹¤í–‰
                if mid_point > 0 and mid_point < len(merged_text):
                    final_blocks.append({
                        **current_block,
                        "text": merged_text[:mid_point]
                    })
                    final_blocks.append({
                        **next_block,
                        "text": merged_text[mid_point:]
                    })
                else:
                    final_blocks.append({
                        **current_block,
                        "text": merged_text
                    })
            else:
                final_blocks.append({
                    **current_block,
                    "text": merged_text
                })
            i += 2  # ë‘ ë¸”ë¡ì„ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ 2 ì¦ê°€
        else:
            final_blocks.append(current_block)
            i += 1
    
    return final_blocks

def _create_recursive_text_splitter(chunk_size: int, chunk_overlap: int) -> RecursiveCharacterTextSplitter:
    """
    í•œêµ­ì–´ì— ìµœì í™”ëœ Recursive Text Splitter ìƒì„±
    
    Args:
        chunk_size: ì²­í¬ í¬ê¸°
        chunk_overlap: ì²­í¬ ì˜¤ë²„ë©
        
    Returns:
        RecursiveCharacterTextSplitter ì¸ìŠ¤í„´ìŠ¤
    """
    # í•œêµ­ì–´ì— ìµœì í™”ëœ êµ¬ë¶„ì ì„¤ì •
    separators = [
        "\n\n",      # ë¬¸ë‹¨ êµ¬ë¶„
        "\n",        # ì¤„ êµ¬ë¶„  
        ". ",        # ë¬¸ì¥ êµ¬ë¶„
        "ã€‚",        # í•œêµ­ì–´ ë¬¸ì¥ êµ¬ë¶„
        " ",         # ë‹¨ì–´ êµ¬ë¶„
        ""           # ë¬¸ì êµ¬ë¶„
    ]
    
    return RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=separators,
        length_function=len,
        is_separator_regex=False
    )

def _uniform_chunking_with_recursive_splitter(txt: str, target_size: int, overlap: int, tolerance: float = 0.15) -> List[str]:
    """
    RecursiveCharacterTextSplitterì™€ í›„ì²˜ë¦¬ë¥¼ ê²°í•©í•œ ê· ë“± ì²­í‚¹ (ê°œì„ ëœ ë²„ì „)
    
    Args:
        txt: ì²­í‚¹í•  í…ìŠ¤íŠ¸
        target_size: ëª©í‘œ ì²­í¬ í¬ê¸°
        overlap: ì²­í¬ ì˜¤ë²„ë©
        tolerance: í—ˆìš© ì˜¤ì°¨ (0.15 = 15%)
        
    Returns:
        ê· ë“±í•œ í¬ê¸°ì˜ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    if len(txt) <= target_size:
        return [txt]
    
    # ğŸ”§ ê°œì„ : ìµœì†Œ í¬ê¸° ë³´ì¥ ë¡œì§ ì¶”ê°€
    if len(txt) < target_size * 0.8:  # 80% ë¯¸ë§Œì´ë©´
        # ì‘ì€ í…ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì§€ ë§ê³  ê°•ì œ ë¶„í•  ì‹œë„
        if len(txt) < target_size * 0.5:  # 50% ë¯¸ë§Œì´ë©´
            return [txt]  # ë„ˆë¬´ ì‘ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        else:
            # 50-80% ë²”ìœ„ë©´ ê°•ì œë¡œ ëª©í‘œ í¬ê¸°ì— ë§ì¶° ë¶„í• 
            return _force_chunk_to_target_size(txt, target_size, overlap)
    
    # 1ë‹¨ê³„: RecursiveCharacterTextSplitterë¡œ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• 
    text_splitter = _create_recursive_text_splitter(target_size, overlap)
    initial_chunks = text_splitter.split_text(txt)
    
    # 2ë‹¨ê³„: ì²­í¬ í¬ê¸° ê· ë“±í™”
    min_size = int(target_size * (1 - tolerance))
    max_size = int(target_size * (1 + tolerance))
    
    balanced_chunks = []
    i = 0
    
    while i < len(initial_chunks):
        current_chunk = initial_chunks[i]
        chunk_len = len(current_chunk)
        
        if min_size <= chunk_len <= max_size:
            # ì ì ˆí•œ í¬ê¸°ì˜ ì²­í¬ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
            balanced_chunks.append(current_chunk)
            i += 1
        elif chunk_len < min_size:
            # ë„ˆë¬´ ì‘ì€ ì²­í¬ëŠ” ë‹¤ìŒ ì²­í¬ì™€ ë³‘í•©
            if i + 1 < len(initial_chunks):
                next_chunk = initial_chunks[i + 1]
                merged = current_chunk + "\n" + next_chunk
                
                if len(merged) <= max_size:
                    # ë³‘í•©ëœ ì²­í¬ê°€ ìµœëŒ€ í¬ê¸° ì´ë‚´ë©´ ì‚¬ìš©
                    balanced_chunks.append(merged)
                    i += 2
                else:
                    # ë³‘í•©í•˜ë©´ ë„ˆë¬´ í¬ë©´ í˜„ì¬ ì²­í¬ë§Œ ì‚¬ìš©
                    balanced_chunks.append(current_chunk)
                    i += 1
            else:
                # ë§ˆì§€ë§‰ ì²­í¬ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
                balanced_chunks.append(current_chunk)
                i += 1
        else:
            # ë„ˆë¬´ í° ì²­í¬ëŠ” ì¬ë¶„í• 
            sub_chunks = _split_large_chunk(current_chunk, target_size, overlap)
            balanced_chunks.extend(sub_chunks)
            i += 1
    
    # 3ë‹¨ê³„: ë§ˆì§€ë§‰ ìµœì í™” - ë„ˆë¬´ ì‘ì€ ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
    if len(balanced_chunks) > 1 and len(balanced_chunks[-1]) < min_size:
        # ë§ˆì§€ë§‰ ì²­í¬ë¥¼ ì´ì „ ì²­í¬ì™€ ë³‘í•©
        last_chunk = balanced_chunks.pop()
        balanced_chunks[-1] += "\n" + last_chunk
    
    # ğŸ”§ ê°œì„ : ìµœì¢… ê²€ì¦ - ëª©í‘œ í¬ê¸° ë‹¬ì„± ì—¬ë¶€ í™•ì¸
    final_chunks = []
    for chunk in balanced_chunks:
        if len(chunk) < target_size * 0.7:  # 70% ë¯¸ë§Œì´ë©´
            # ë‹¤ìŒ ì²­í¬ì™€ ê°•ì œ ë³‘í•© ì‹œë„
            if len(final_chunks) > 0:
                # ì´ì „ ì²­í¬ì™€ ë³‘í•©
                final_chunks[-1] += "\n" + chunk
            else:
                final_chunks.append(chunk)
        else:
            final_chunks.append(chunk)
    
    return final_chunks

def _force_chunk_to_target_size(txt: str, target_size: int, overlap: int) -> List[str]:
    """
    ì‘ì€ í…ìŠ¤íŠ¸ë¥¼ ëª©í‘œ í¬ê¸°ì— ë§ì¶° ê°•ì œ ë¶„í• 
    
    Args:
        txt: ë¶„í• í•  í…ìŠ¤íŠ¸
        target_size: ëª©í‘œ ì²­í¬ í¬ê¸°
        overlap: ì²­í¬ ì˜¤ë²„ë©
        
    Returns:
        ê°•ì œ ë¶„í• ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    if len(txt) <= target_size:
        return [txt]
    
    chunks = []
    start = 0
    
    while start < len(txt):
        end = min(start + target_size, len(txt))
        
        # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´ ì ì ˆí•œ ë¶„í• ì  ì°¾ê¸°
        if end < len(txt):
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ë¶„í•  ì‹œë„
            for sep in ['. ', '.\n', '\n\n', '\n']:
                sep_pos = txt.rfind(sep, start, end)
                if sep_pos > start + target_size * 0.7:  # 70% ì´ìƒ ìœ„ì¹˜ì—ì„œ ë°œê²¬
                    end = sep_pos + len(sep)
                    break
        
        chunk_text = txt[start:end].strip()
        if chunk_text:
            chunks.append(chunk_text)
        
        start = max(start + 1, end - overlap)
    
    return chunks

def _split_large_chunk(chunk: str, target_size: int, overlap: int) -> List[str]:
    """
    í° ì²­í¬ë¥¼ ëª©í‘œ í¬ê¸°ë¡œ ë¶„í• 
    """
    if len(chunk) <= target_size:
        return [chunk]
    
    chunks = []
    start = 0
    
    while start < len(chunk):
        end = min(start + target_size, len(chunk))
        
        # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´ ì ì ˆí•œ ë¶„í• ì  ì°¾ê¸°
        if end < len(chunk):
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ë¶„í•  ì‹œë„
            for sep in ['. ', '.\n', '\n\n', '\n']:
                sep_pos = chunk.rfind(sep, start, end)
                if sep_pos > start + target_size * 0.7:  # 70% ì´ìƒ ìœ„ì¹˜ì—ì„œ ë°œê²¬
                    end = sep_pos + len(sep)
                    break
        
        chunk_text = chunk[start:end].strip()
        if chunk_text:
            chunks.append(chunk_text)
        
        start = max(start + 1, end - overlap)
    
    return chunks

def _keyword_tags(docs: List[str], topk: int = 8) -> List[List[str]]:
    if not docs:
        return [[]]
    # TF-IDF + í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë³´ê°•
    vec = TfidfVectorizer(max_df=0.9, min_df=2, ngram_range=(1,2))
    try:
        X = vec.fit_transform(docs)
        terms = np.array(vec.get_feature_names_out())
    except ValueError:
        return [[kw for kw in KEYWORD_WHITELIST if kw in d] for d in docs]

    tags: List[List[str]] = []
    for row in range(X.shape[0]):
        data = X[row].toarray().ravel()
        idx = data.argsort()[::-1][:topk]
        tfidf_tags = [t for t in terms[idx] if len(t) > 1]
        wl = [kw for kw in KEYWORD_WHITELIST if kw in docs[row]]
        # í•©ì¹˜ë˜ë„ë¡ ì¤‘ë³µ ì œê±°
        uniq = []
        for t in wl + tfidf_tags:
            if t not in uniq:
                uniq.append(t)
        tags.append(uniq[:topk])
    return tags

def _extract_tables_pymupdf(pdf_path: Path) -> Dict[int, List[Dict[str, Any]]]:
    """
    PyMuPDFë¥¼ ì‚¬ìš©í•œ í…Œì´ë¸” ì¶”ì¶œ (ê°„ë‹¨í•œ í˜•íƒœ)
    - í…Œì´ë¸” í˜•íƒœì˜ í…ìŠ¤íŠ¸ë¥¼ ê°ì§€í•˜ì—¬ êµ¬ì¡°í™”
    """
    results: Dict[int, List[Dict[str, Any]]] = {}
    try:
        with fitz.open(str(pdf_path)) as doc:
            for pno, page in enumerate(doc, start=1):
                # í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ë¸”ë¡ ë‹¨ìœ„ë¡œ ì¶”ì¶œ
                blocks = page.get_text("dict").get("blocks", [])
                tables = []
                
                for block in blocks:
                    if "lines" not in block:
                        continue
                    
                    # ë¸”ë¡ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    block_text = ""
                    for line in block["lines"]:
                        line_text = "".join([span.get("text", "") for span in line.get("spans", [])])
                        block_text += line_text + "\n"
                    
                    # í…Œì´ë¸” íŒ¨í„´ ê°ì§€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
                    if _is_table_pattern(block_text):
                        rows = _parse_table_text(block_text)
                        if rows:
                            tables.append({"rows": rows})
                
                if tables:
                    results[pno] = tables
    except Exception as e:
        print(f"âš ï¸ í…Œì´ë¸” ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return results

def _is_table_pattern(text: str) -> bool:
    """
    í…ìŠ¤íŠ¸ê°€ í…Œì´ë¸” íŒ¨í„´ì¸ì§€ íŒë‹¨
    """
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    if len(lines) < 2:
        return False
    
    # ì—¬ëŸ¬ êµ¬ë¶„ìë¡œ ë¶„ë¦¬ëœ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
    separators = ['|', '\t', '  ', '    ']  # íŒŒì´í”„, íƒ­, 2ê°œ ì´ìƒ ê³µë°±
    
    for sep in separators:
        if sep in text:
            # êµ¬ë¶„ìë¡œ ë¶„ë¦¬ëœ í–‰ì´ 2ê°œ ì´ìƒ ìˆëŠ”ì§€ í™•ì¸
            separated_lines = [line for line in lines if sep in line]
            if len(separated_lines) >= 2:
                return True
    
    return False

def _parse_table_text(text: str) -> List[List[str]]:
    """
    í…Œì´ë¸” í˜•íƒœì˜ í…ìŠ¤íŠ¸ë¥¼ í–‰/ì—´ êµ¬ì¡°ë¡œ íŒŒì‹±
    """
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    rows = []
    
    for line in lines:
        # ê°€ì¥ ì ì ˆí•œ êµ¬ë¶„ì ì°¾ê¸°
        if '|' in line:
            cells = [cell.strip() for cell in line.split('|')]
        elif '\t' in line:
            cells = [cell.strip() for cell in line.split('\t')]
        else:
            # 2ê°œ ì´ìƒì˜ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
            cells = [cell.strip() for cell in re.split(r'\s{2,}', line)]
        
        # ë¹ˆ ì…€ ì œê±°í•˜ê³  ìœ íš¨í•œ í–‰ë§Œ ì¶”ê°€
        cells = [cell for cell in cells if cell]
        if cells:
            rows.append(cells)
    
    return rows

def _blocks_from_pymupdf(pdf_path: Path) -> List[Dict[str, Any]]:
    """
    PyMuPDFë¡œ pageâ†’blocks í…ìŠ¤íŠ¸ ì¶”ì¶œ.
    blockâ†’lineâ†’span ìˆœì„œë¡œ í…ìŠ¤íŠ¸ë¥¼ ì—°ê²°í•˜ë©°, í°íŠ¸/í¬ê¸° ë“±ì€ í•„ìš” ì‹œ í™•ì¥.
    """
    out = []
    meta = _parse_filename_meta(pdf_path)
    doc_id = f"{meta['insurer']}_{meta['version_year']}_{meta['title']}".strip()
    with fitz.open(str(pdf_path)) as doc:
        for pno, page in enumerate(doc, start=1):
            raw = page.get_text("dict")
            for b in raw.get("blocks", []):
                if "lines" not in b:  # ì´ë¯¸ì§€/ê·¸ë¦¼
                    continue
                texts = []
                for ln in b["lines"]:
                    spans = ln.get("spans", [])
                    line_text = "".join([s.get("text","") for s in spans]).strip()
                    if line_text:
                        texts.append(line_text)
                text = "\n".join(texts).strip()
                if not text:
                    continue
                out.append({
                    "doc_id": doc_id,
                    "insurer": meta["insurer"],
                    "version": meta["version_year"],
                    "title": meta["title"],
                    "page": pno,
                    "text": text,
                })
    return out

def _label_sections(blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    í—¤ë”© ê·œì¹™/í‚¤ì›Œë“œë¡œ body/appendix ë¼ë²¨. í‘œëŠ” pdfplumber ê²°ê³¼ì— ë”°ë¡œ í•©ë¥˜.
    """
    labeled = []
    last_heading = None
    for b in blocks:
        t = b["text"].splitlines()[0] if b["text"] else ""
        if RE_SPECIAL.search(t) or RE_CHAPTER.search(t) or RE_ARTICLE.search(t) or RE_APPENDIX.search(t):
            last_heading = t
        section_type = "body"
        if last_heading and RE_APPENDIX.search(last_heading):
            section_type = "appendix"
        elif RE_APPENDIX.search(t):
            section_type = "appendix"
        labeled.append({**b, "section_type": section_type, "heading": last_heading})
    return labeled

def _merge_tables(labeled_blocks: List[Dict[str, Any]], table_map: Dict[int, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    """
    PyMuPDFë¡œ ì¶”ì¶œí•œ í…Œì´ë¸”ì„ section_type='table' ë¡œ ì‚½ì….
    - ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ í…Œì´ë¸”ì´ ìˆëŠ” í˜ì´ì§€ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œ í…Œì´ë¸” íŒ¨í„´ ì œì™¸
    """
    merged: List[Dict[str, Any]] = []
    by_page: Dict[int, List[Dict[str, Any]]] = {}
    
    # í˜ì´ì§€ë³„ë¡œ ë¸”ë¡ ê·¸ë£¹í™”
    for b in labeled_blocks:
        by_page.setdefault(b["page"], []).append(b)

    for pno in sorted(by_page.keys()):
        page_blocks = by_page[pno]
        
        # í•´ë‹¹ í˜ì´ì§€ì— í…Œì´ë¸”ì´ ìˆëŠ”ì§€ í™•ì¸
        has_tables = pno in table_map and table_map[pno]
        
        if has_tables:
            # í…Œì´ë¸”ì´ ìˆëŠ” í˜ì´ì§€ëŠ” í…Œì´ë¸” íŒ¨í„´ì„ ì œì™¸í•œ ì¼ë°˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ê°€
            for block in page_blocks:
                if not _is_table_pattern(block["text"]):
                    merged.append(block)
            
            # êµ¬ì¡°í™”ëœ í…Œì´ë¸” ì¶”ê°€
            for tb in table_map[pno]:
                text_rows = [" | ".join(r) for r in tb["rows"]]
                merged.append({
                    **page_blocks[0], 
                    "section_type": "table", 
                    "text": "\n".join(text_rows)
                })
        else:
            # í…Œì´ë¸”ì´ ì—†ëŠ” í˜ì´ì§€ëŠ” ê·¸ëŒ€ë¡œ ì¶”ê°€
            merged.extend(page_blocks)
    
    return merged

def _build_index(chunks_meta: List[Dict[str, Any]]):
    """Chroma DBì— ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• - multilingual-e5-small-ko ëª¨ë¸ ì‚¬ìš©"""
    os.makedirs(OUT_DIR, exist_ok=True)
    texts = [c["text"] for c in chunks_meta]
    if not texts:
        print("âš ï¸ No text chunks parsed. Abort.")
        return
    
    if chromadb is None:
        print("âš ï¸ chromadb not available. Skipping index build.")
        return
    
    try:
        # Chroma DB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = chromadb.PersistentClient(
            path=OUT_DIR,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸ ë° ì²˜ë¦¬
        collection = None
        try:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆëŠ”ì§€ í™•ì¸
            collection = client.get_collection(COLLECTION_NAME)
            print(f"ğŸ“‹ Found existing collection: {COLLECTION_NAME}")
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
            client.delete_collection(COLLECTION_NAME)
            print(f"ğŸ—‘ï¸ Deleted existing collection: {COLLECTION_NAME}")
        except Exception:
            print(f"ğŸ“‹ No existing collection found: {COLLECTION_NAME}")
        
        # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
        collection = client.create_collection(
            name=COLLECTION_NAME,
            metadata={"description": "ì—¬í–‰ìë³´í—˜ ë¬¸ì„œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤"}
        )
        print(f"âœ¨ Created new collection: {COLLECTION_NAME}")
        
        # ë¬¸ì„œ ID ìƒì„± ë° ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        doc_ids = [f"doc_{i}" for i in range(len(chunks_meta))]
        metadatas = []
        
        for chunk in chunks_meta:
            # Chroma DB ë©”íƒ€ë°ì´í„°ëŠ” ë¬¸ìì—´ ê°’ë§Œ í—ˆìš©
            metadata = {}
            for key, value in chunk.items():
                if key != "text" and isinstance(value, (str, int, float, bool)):
                    metadata[key] = str(value)
            metadatas.append(metadata)
        
        # multilingual-e5-small-ko ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ìƒì„±
        print("ğŸ”„ Generating embeddings with multilingual-e5-small-ko model...")
        embeddings = embed_texts(texts)
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì»¬ë ‰ì…˜ì— ë¬¸ì„œì™€ ì„ë² ë”© ì¶”ê°€ (Chroma DB ë°°ì¹˜ í¬ê¸° ì œí•œ ëŒ€ì‘)
        BATCH_SIZE = 5000  # Chroma DB ìµœëŒ€ ë°°ì¹˜ í¬ê¸°ë³´ë‹¤ ì‘ê²Œ ì„¤ì •
        total_chunks = len(chunks_meta)
        
        print(f"ğŸ”„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë²¡í„° DBì— ì €ì¥ ì¤‘... (ì´ {total_chunks}ê°œ ì²­í¬)")
        
        for i in range(0, total_chunks, BATCH_SIZE):
            end_idx = min(i + BATCH_SIZE, total_chunks)
            batch_texts = texts[i:end_idx]
            batch_metadatas = metadatas[i:end_idx]
            batch_ids = doc_ids[i:end_idx]
            batch_embeddings = embeddings[i:end_idx].tolist()
            
            print(f"  ğŸ“¦ ë°°ì¹˜ {i//BATCH_SIZE + 1}: {len(batch_texts)}ê°œ ì²­í¬ ì €ì¥ ì¤‘...")
            
            collection.add(
                documents=batch_texts,
                metadatas=batch_metadatas,
                ids=batch_ids,
                embeddings=batch_embeddings
            )
        
        print(f"âœ… Built Chroma DB index: {len(chunks_meta)} chunks â†’ {OUT_DIR}/{COLLECTION_NAME}")
        
    except Exception as e:
        print(f"âŒ Failed to build Chroma DB index: {e}")
        return

def _remove_duplicate_chunks(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ì¤‘ë³µëœ ì²­í¬ ì œê±°
    """
    seen_texts = set()
    unique_chunks = []
    duplicate_count = 0
    
    for chunk in chunks:
        # í…ìŠ¤íŠ¸ ì •ê·œí™” (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬)
        normalized = re.sub(r'\s+', ' ', chunk["text"].strip())
        
        if normalized and normalized not in seen_texts:
            seen_texts.add(normalized)
            unique_chunks.append(chunk)
        else:
            duplicate_count += 1
            if duplicate_count <= 5:  # ì²˜ìŒ 5ê°œë§Œ ë¡œê·¸ ì¶œë ¥
                print(f"âš ï¸ ì¤‘ë³µ ì²­í¬ ì œê±°: {chunk.get('doc_id', 'unknown')} - {normalized[:50]}...")
    
    if duplicate_count > 5:
        print(f"âš ï¸ ì´ {duplicate_count}ê°œì˜ ì¤‘ë³µ ì²­í¬ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return unique_chunks

def _filter_empty_chunks(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ë¹ˆ ì²­í¬ ë° ë„ˆë¬´ ì§§ì€ ì²­í¬ í•„í„°ë§
    """
    filtered_chunks = []
    empty_count = 0
    
    for chunk in chunks:
        text = chunk.get("text", "").strip()
        
        # ë¹ˆ í…ìŠ¤íŠ¸ ë˜ëŠ” ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œê±°
        if not text or len(text) < 10:
            empty_count += 1
            continue
        
        # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ í¬í•¨ (ìˆ«ì, í•œê¸€, ì˜ë¬¸ì´ í¬í•¨ëœ ê²½ìš°)
        if re.search(r'[ê°€-í£a-zA-Z0-9]', text):
            filtered_chunks.append(chunk)
        else:
            empty_count += 1
    
    if empty_count > 0:
        print(f"âš ï¸ {empty_count}ê°œì˜ ë¹ˆ/ë¬´ì˜ë¯¸í•œ ì²­í¬ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return filtered_chunks

def _analyze_chunk_sizes(chunks: List[Dict[str, Any]], target_size: int = 800) -> None:
    """ì²­í¬ í¬ê¸° ë¶„ì„ ë° í†µê³„ ì¶œë ¥ (ê°œì„ ëœ ë²„ì „)"""
    sizes = [len(chunk["text"]) for chunk in chunks]
    if not sizes:
        return
    
    avg_size = sum(sizes) / len(sizes)
    min_size = min(sizes)
    max_size = max(sizes)
    std_dev = (sum((x - avg_size) ** 2 for x in sizes) / len(sizes)) ** 0.5
    
    # ëª©í‘œ í¬ê¸° ê¸°ì¤€ í†µê³„
    target_range_count = sum(1 for s in sizes if target_size * 0.8 <= s <= target_size * 1.2)
    target_ratio = target_range_count / len(sizes) * 100
    
    # í¬ê¸° ë¶„í¬ ë¶„ì„
    size_ranges = [
        (0, target_size * 0.5, "ë§¤ìš° ì‘ìŒ"),
        (target_size * 0.5, target_size * 0.8, "ì‘ìŒ"),
        (target_size * 0.8, target_size * 1.2, "ì ì ˆí•¨"),
        (target_size * 1.2, target_size * 1.5, "í¼"),
        (target_size * 1.5, float('inf'), "ë§¤ìš° í¼")
    ]
    
    print(f"\nğŸ“Š ì²­í¬ í¬ê¸° ë¶„ì„ (ê°œì„ ëœ ë²„ì „):")
    print(f"  - ì´ ì²­í¬ ìˆ˜: {len(chunks)}")
    print(f"  - í‰ê·  í¬ê¸°: {avg_size:.1f}ì")
    print(f"  - ìµœì†Œ/ìµœëŒ€ í¬ê¸°: {min_size}/{max_size}ì")
    print(f"  - í‘œì¤€í¸ì°¨: {std_dev:.1f}ì")
    print(f"  - ëª©í‘œ í¬ê¸°({target_size}Â±20%) ë²”ìœ„: {target_range_count}ê°œ ({target_ratio:.1f}%)")
    
    print(f"\nğŸ“ˆ í¬ê¸° ë¶„í¬:")
    for start, end, label in size_ranges:
        count = sum(1 for s in sizes if start <= s < end)
        if count > 0:
            ratio = count / len(sizes) * 100
            print(f"  - {label}: {count}ê°œ ({ratio:.1f}%)")
    
    # í’ˆì§ˆ í‰ê°€
    quality_score = 0
    if target_ratio >= 80:
        quality_score = 5  # ìš°ìˆ˜
    elif target_ratio >= 60:
        quality_score = 4  # ì–‘í˜¸
    elif target_ratio >= 40:
        quality_score = 3  # ë³´í†µ
    elif target_ratio >= 20:
        quality_score = 2  # ë¯¸í¡
    else:
        quality_score = 1  # ë¶ˆëŸ‰
    
    quality_labels = ["ë¶ˆëŸ‰", "ë¯¸í¡", "ë³´í†µ", "ì–‘í˜¸", "ìš°ìˆ˜"]
    print(f"\nğŸ¯ ì²­í‚¹ í’ˆì§ˆ: {quality_labels[quality_score-1]} ({quality_score}/5)")
    
    # ê°œì„  ì œì•ˆ
    if target_ratio < 60:
        print(f"\nğŸ’¡ ê°œì„  ì œì•ˆ:")
        if target_ratio < 40:
            print(f"  - ë¸”ë¡ ë³‘í•© ì„ê³„ê°’ì„ ë” í¬ê²Œ ì„¤ì • (í˜„ì¬: 700ì)")
            print(f"  - ìµœì†Œ ì²­í¬ í¬ê¸° ê°•ì œ ì ìš© í•„ìš”")
        if std_dev > target_size * 0.3:
            print(f"  - ì²­í¬ í¬ê¸° ì¼ê´€ì„± ê°œì„  í•„ìš” (í‘œì¤€í¸ì°¨: {std_dev:.1f}ì)")
        if min_size < target_size * 0.5:
            print(f"  - ë„ˆë¬´ ì‘ì€ ì²­í¬ë“¤ ê°•ì œ ë³‘í•© í•„ìš”")

def main():
    pdfs = sorted([p for p in Path(DOC_DIR).glob("*.pdf")])
    if not pdfs:
        print(f"âš ï¸ No PDFs under {DOC_DIR}. Place files like '2025_ë³´í—˜ì‚¬_ë¬¸ì„œì œëª©.pdf'")
        return

    all_sections: List[Dict[str, Any]] = []
    for p in tqdm(pdfs, desc="Parsing PDFs (PyMuPDF only)"):
        blocks = _blocks_from_pymupdf(p)
        labeled = _label_sections(blocks)
        tables = _extract_tables_pymupdf(p)
        merged = _merge_tables(labeled, tables)
        
        # ğŸ”§ ê°œì„ : ì‘ì€ ë¸”ë¡ ë³‘í•© ì ìš© (700ì ì„ê³„ê°’)
        print(f"ğŸ“„ {p.name}: {len(merged)}ê°œ ë¸”ë¡ ì¶”ì¶œ")
        merged_blocks = _merge_small_blocks(merged, min_size=700)
        print(f"ğŸ“„ {p.name}: {len(merged_blocks)}ê°œ ë¸”ë¡ìœ¼ë¡œ ë³‘í•© (ë³‘í•©ë¥ : {len(merged_blocks)/len(merged)*100:.1f}%)")
        
        # ğŸ”§ ê°œì„ : ë³‘í•©ëœ ë¸”ë¡ í¬ê¸° ê²€ì¦
        avg_block_size = sum(len(block["text"]) for block in merged_blocks) / len(merged_blocks) if merged_blocks else 0
        print(f"ğŸ“„ {p.name}: í‰ê·  ë¸”ë¡ í¬ê¸° {avg_block_size:.1f}ì")
        
        all_sections.extend(merged_blocks)

    print(f"ğŸ“„ ì´ {len(all_sections)}ê°œì˜ ì„¹ì…˜ì´ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ì„¹ì…˜ í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ íƒœê¹…
    section_texts = [s["text"] for s in all_sections]
    section_tags = _keyword_tags(section_texts, topk=8)
    for s, tags in zip(all_sections, section_tags):
        s["tags"] = tags

    # ğŸ”§ ê°œì„ : RecursiveCharacterTextSplitter + ê· ë“±í™”ë¥¼ ì‚¬ìš©í•œ ì¼ì •í•œ í¬ê¸° ì²­í¬ ìƒì„±
    chunks_meta: List[Dict[str, Any]] = []
    for s in tqdm(all_sections, desc="Uniform Chunking with Recursive Splitter"):
        # í‘œëŠ” í–‰ ë‹¨ìœ„ë¡œ ì´ë¯¸ ì§§ì€ í¸ â†’ ë°”ë¡œ ì €ì¥. (í¬ë©´ ì¼ë°˜ ì²­í‚¹)
        if s.get("section_type") == "table" and len(s["text"]) <= CHUNK_SIZE:
            m = dict(s)
            m["chunk_no"] = 1
            chunks_meta.append(m)
        else:
            # RecursiveCharacterTextSplitter + ê· ë“±í™” ì‚¬ìš©
            chunked_texts = _uniform_chunking_with_recursive_splitter(
                s["text"], CHUNK_SIZE, CHUNK_OVERLAP
            )
            for i, ch in enumerate(chunked_texts, start=1):
                m = dict(s)
                m["text"] = ch
                m["chunk_no"] = i
                chunks_meta.append(m)

    print(f"ğŸ”§ ì´ {len(chunks_meta)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ì¤‘ë³µ ë° ë¹ˆ ì²­í¬ ì œê±°
    chunks_meta = _remove_duplicate_chunks(chunks_meta)
    chunks_meta = _filter_empty_chunks(chunks_meta)
    
    # ì²­í¬ í¬ê¸° ë¶„ì„
    _analyze_chunk_sizes(chunks_meta, CHUNK_SIZE)
    
    print(f"âœ… ìµœì¢… {len(chunks_meta)}ê°œì˜ ì²­í¬ê°€ ë²¡í„° DBì— ì €ì¥ë©ë‹ˆë‹¤.")

    _build_index(chunks_meta)

if __name__ == "__main__":
    main()