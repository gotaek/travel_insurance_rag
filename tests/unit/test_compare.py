"""
Compare ë…¸ë“œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
compare_nodeì˜ í•µì‹¬ ê¸°ëŠ¥ê³¼ ì—ëŸ¬ ì²˜ë¦¬ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import sys
import os
import pytest
from unittest.mock import Mock, patch
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from graph.nodes.answerers.compare import compare_node, _format_context, _parse_llm_response


@pytest.mark.unit
class TestCompareNode:
    """Compare ë…¸ë“œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def test_format_context_empty_passages(self):
        """ë¹ˆ íŒ¨ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        result = _format_context([])
        assert result == "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        print("âœ… ë¹ˆ íŒ¨ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬")
    
    def test_format_context_with_passages(self):
        """íŒ¨ì‹œì§€ í¬ë§·íŒ… í…ŒìŠ¤íŠ¸"""
        passages = [
            {
                "doc_id": "DBì†í•´ë³´í—˜_ì—¬í–‰ìë³´í—˜ì•½ê´€",
                "page": 15,
                "text": "ì‚¬ë§ë³´ì¥ í•œë„ëŠ” 1ì–µì›ì´ë©°, ìƒí•´ë³´ì¥ì€ 5ì²œë§Œì›ì…ë‹ˆë‹¤."
            },
            {
                "doc_id": "ì¹´ì¹´ì˜¤í˜ì´_ì—¬í–‰ìë³´í—˜ì•½ê´€", 
                "page": 12,
                "text": "ì‚¬ë§ë³´ì¥ í•œë„ëŠ” 5ì²œë§Œì›ì´ë©°, ìƒí•´ë³´ì¥ì€ 3ì²œë§Œì›ì…ë‹ˆë‹¤."
            }
        ]
        
        result = _format_context(passages)
        
        # ê²°ê³¼ì— í•„ìš”í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        assert "DBì†í•´ë³´í—˜_ì—¬í–‰ìë³´í—˜ì•½ê´€" in result
        assert "í˜ì´ì§€ 15" in result
        assert "ì¹´ì¹´ì˜¤í˜ì´_ì—¬í–‰ìë³´í—˜ì•½ê´€" in result
        assert "í˜ì´ì§€ 12" in result
        assert "ì‚¬ë§ë³´ì¥" in result
        assert "ìƒí•´ë³´ì¥" in result
        
        print("âœ… íŒ¨ì‹œì§€ í¬ë§·íŒ…")
    
    def test_format_context_long_text_truncation(self):
        """ê¸´ í…ìŠ¤íŠ¸ ìë™ ì˜ë¦¼ í…ŒìŠ¤íŠ¸"""
        long_text = "ì—¬í–‰ìë³´í—˜" * 200  # ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸
        passages = [{
            "doc_id": "í…ŒìŠ¤íŠ¸_ë¬¸ì„œ",
            "page": 1,
            "text": long_text
        }]
        
        result = _format_context(passages)
        
        # 500ìë¡œ ì œí•œë˜ì–´ì•¼ í•¨
        assert len(result) < len(long_text)
        assert "ì—¬í–‰ìë³´í—˜" in result
        
        print("âœ… ê¸´ í…ìŠ¤íŠ¸ ìë™ ì˜ë¦¼")
    
    def test_format_context_max_passages_limit(self):
        """ìµœëŒ€ íŒ¨ì‹œì§€ ìˆ˜ ì œí•œ í…ŒìŠ¤íŠ¸"""
        # 10ê°œì˜ íŒ¨ì‹œì§€ ìƒì„±
        passages = []
        for i in range(10):
            passages.append({
                "doc_id": f"ë¬¸ì„œ_{i}",
                "page": i + 1,
                "text": f"í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ {i}"
            })
        
        result = _format_context(passages)
        
        # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©ë˜ì–´ì•¼ í•¨
        assert "ë¬¸ì„œ_0" in result
        assert "ë¬¸ì„œ_4" in result
        assert "ë¬¸ì„œ_5" not in result  # 6ë²ˆì§¸ë¶€í„°ëŠ” ì œì™¸
        
        print("âœ… ìµœëŒ€ íŒ¨ì‹œì§€ ìˆ˜ ì œí•œ")
    
    def test_parse_llm_response_valid_json(self):
        """ìœ íš¨í•œ JSON ì‘ë‹µ íŒŒì‹± í…ŒìŠ¤íŠ¸"""
        valid_response = {
            "conclusion": "DBì†ë³´ê°€ ì¹´ì¹´ì˜¤í˜ì´ë³´ë‹¤ ë³´ì¥í•œë„ê°€ ë†’ìŠµë‹ˆë‹¤",
            "evidence": ["ì‚¬ë§ë³´ì¥ 1ì–µì› vs 5ì²œë§Œì›", "ìƒí•´ë³´ì¥ 5ì²œë§Œì› vs 3ì²œë§Œì›"],
            "caveats": ["ë³´í—˜ë£Œ ì°¨ì´ ê³ ë ¤ í•„ìš”"],
            "quotes": [],
            "comparison_table": {
                "headers": ["í•­ëª©", "DBì†ë³´", "ì¹´ì¹´ì˜¤í˜ì´"],
                "rows": [
                    ["ì‚¬ë§ë³´ì¥", "1ì–µì›", "5ì²œë§Œì›"],
                    ["ìƒí•´ë³´ì¥", "5ì²œë§Œì›", "3ì²œë§Œì›"]
                ]
            }
        }
        
        json_text = json.dumps(valid_response, ensure_ascii=False)
        result = _parse_llm_response(json_text)
        
        assert result["conclusion"] == "DBì†ë³´ê°€ ì¹´ì¹´ì˜¤í˜ì´ë³´ë‹¤ ë³´ì¥í•œë„ê°€ ë†’ìŠµë‹ˆë‹¤"
        assert "ì‚¬ë§ë³´ì¥ 1ì–µì› vs 5ì²œë§Œì›" in result["evidence"]
        assert "ë³´í—˜ë£Œ ì°¨ì´ ê³ ë ¤ í•„ìš”" in result["caveats"]
        assert "comparison_table" in result
        assert result["comparison_table"]["headers"] == ["í•­ëª©", "DBì†ë³´", "ì¹´ì¹´ì˜¤í˜ì´"]
        
        print("âœ… ìœ íš¨í•œ JSON ì‘ë‹µ íŒŒì‹±")
    
    def test_parse_llm_response_markdown_json(self):
        """ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ê°ì‹¸ì§„ JSON íŒŒì‹± í…ŒìŠ¤íŠ¸"""
        valid_response = {
            "conclusion": "í…ŒìŠ¤íŠ¸ ê²°ë¡ ",
            "evidence": ["í…ŒìŠ¤íŠ¸ ì¦ê±°"],
            "caveats": ["í…ŒìŠ¤íŠ¸ ì£¼ì˜ì‚¬í•­"],
            "quotes": [],
            "comparison_table": {
                "headers": ["í•­ëª©", "ê²°ê³¼"],
                "rows": [["í…ŒìŠ¤íŠ¸", "ì„±ê³µ"]]
            }
        }
        
        markdown_json = f"```json\n{json.dumps(valid_response, ensure_ascii=False)}\n```"
        result = _parse_llm_response(markdown_json)
        
        assert result["conclusion"] == "í…ŒìŠ¤íŠ¸ ê²°ë¡ "
        assert "comparison_table" in result
        
        print("âœ… ë§ˆí¬ë‹¤ìš´ JSON íŒŒì‹±")
    
    def test_parse_llm_response_missing_comparison_table(self):
        """comparison_table í•„ë“œê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        response_without_table = {
            "conclusion": "í…ŒìŠ¤íŠ¸ ê²°ë¡ ",
            "evidence": ["í…ŒìŠ¤íŠ¸ ì¦ê±°"],
            "caveats": ["í…ŒìŠ¤íŠ¸ ì£¼ì˜ì‚¬í•­"],
            "quotes": []
        }
        
        json_text = json.dumps(response_without_table, ensure_ascii=False)
        result = _parse_llm_response(json_text)
        
        # comparison_table í•„ë“œê°€ ìë™ìœ¼ë¡œ ì¶”ê°€ë˜ì–´ì•¼ í•¨
        assert "comparison_table" in result
        assert result["comparison_table"]["headers"] == ["í•­ëª©", "ë¹„êµ ê²°ê³¼"]
        assert result["comparison_table"]["rows"] == [["ë¹„êµ ì •ë³´", "í‘œ í˜•íƒœë¡œ ì œê³µë˜ì§€ ì•ŠìŒ"]]
        
        print("âœ… ëˆ„ë½ëœ comparison_table í•„ë“œ ì²˜ë¦¬")
    
    def test_parse_llm_response_invalid_json(self):
        """ì˜ëª»ëœ JSON ì‘ë‹µ fallback í…ŒìŠ¤íŠ¸"""
        invalid_response = "ì´ê²ƒì€ JSONì´ ì•„ë‹™ë‹ˆë‹¤"
        result = _parse_llm_response(invalid_response)
        
        # fallback êµ¬ì¡° í™•ì¸
        assert "conclusion" in result
        assert "evidence" in result
        assert "caveats" in result
        assert "quotes" in result
        assert "comparison_table" in result
        assert "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤" in result["conclusion"]
        assert result["comparison_table"]["rows"] == [["ì˜¤ë¥˜", "íŒŒì‹± ì‹¤íŒ¨"]]
        
        print("âœ… ì˜ëª»ëœ JSON fallback")
    
    @patch('graph.nodes.answerers.compare.get_llm')
    def test_compare_node_success(self, mock_get_llm):
        """compare_node ì„±ê³µ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
        # Mock LLM ì„¤ì •
        mock_llm = Mock()
        mock_response = Mock()
        mock_response.text = json.dumps({
            "conclusion": "DBì†ë³´ê°€ ì¹´ì¹´ì˜¤í˜ì´ë³´ë‹¤ ë³´ì¥í•œë„ê°€ ë†’ìŠµë‹ˆë‹¤",
            "evidence": ["ì‚¬ë§ë³´ì¥ 1ì–µì› vs 5ì²œë§Œì›", "ìƒí•´ë³´ì¥ 5ì²œë§Œì› vs 3ì²œë§Œì›"],
            "caveats": ["ë³´í—˜ë£Œ ì°¨ì´ ê³ ë ¤ í•„ìš”"],
            "quotes": [],
            "comparison_table": {
                "headers": ["í•­ëª©", "DBì†ë³´", "ì¹´ì¹´ì˜¤í˜ì´"],
                "rows": [
                    ["ì‚¬ë§ë³´ì¥", "1ì–µì›", "5ì²œë§Œì›"],
                    ["ìƒí•´ë³´ì¥", "5ì²œë§Œì›", "3ì²œë§Œì›"]
                ]
            }
        }, ensure_ascii=False)
        mock_llm.generate_content.return_value = mock_response
        mock_get_llm.return_value = mock_llm
        
        # í…ŒìŠ¤íŠ¸ state
        state = {
            "question": "DBì†ë³´ì™€ ì¹´ì¹´ì˜¤í˜ì´ ì—¬í–‰ìë³´í—˜ ì°¨ì´ ë¹„êµ",
            "passages": [
                {
                    "doc_id": "DBì†í•´ë³´í—˜_ì—¬í–‰ìë³´í—˜ì•½ê´€",
                    "page": 15,
                    "text": "ì‚¬ë§ë³´ì¥ í•œë„ëŠ” 1ì–µì›ì´ë©°, ìƒí•´ë³´ì¥ì€ 5ì²œë§Œì›ì…ë‹ˆë‹¤."
                },
                {
                    "doc_id": "ì¹´ì¹´ì˜¤í˜ì´_ì—¬í–‰ìë³´í—˜ì•½ê´€",
                    "page": 12,
                    "text": "ì‚¬ë§ë³´ì¥ í•œë„ëŠ” 5ì²œë§Œì›ì´ë©°, ìƒí•´ë³´ì¥ì€ 3ì²œë§Œì›ì…ë‹ˆë‹¤."
                }
            ]
        }
        
        result = compare_node(state)
        
        # ê²°ê³¼ ê²€ì¦
        assert "draft_answer" in result
        assert "final_answer" in result
        assert result["draft_answer"]["conclusion"] == "DBì†ë³´ê°€ ì¹´ì¹´ì˜¤í˜ì´ë³´ë‹¤ ë³´ì¥í•œë„ê°€ ë†’ìŠµë‹ˆë‹¤"
        assert "ì‚¬ë§ë³´ì¥ 1ì–µì› vs 5ì²œë§Œì›" in result["draft_answer"]["evidence"]
        assert "comparison_table" in result["draft_answer"]
        assert result["draft_answer"]["comparison_table"]["headers"] == ["í•­ëª©", "DBì†ë³´", "ì¹´ì¹´ì˜¤í˜ì´"]
        
        # LLM í˜¸ì¶œ í™•ì¸
        mock_llm.generate_content.assert_called_once()
        
        print("âœ… compare_node ì„±ê³µ ì¼€ì´ìŠ¤")
    
    @patch('graph.nodes.answerers.compare.get_llm')
    def test_compare_node_llm_failure(self, mock_get_llm):
        """compare_node LLM í˜¸ì¶œ ì‹¤íŒ¨ í…ŒìŠ¤íŠ¸"""
        # Mock LLMì´ ì˜ˆì™¸ ë°œìƒí•˜ë„ë¡ ì„¤ì •
        mock_llm = Mock()
        mock_llm.generate_content.side_effect = Exception("LLM í˜¸ì¶œ ì‹¤íŒ¨")
        mock_get_llm.return_value = mock_llm
        
        # í…ŒìŠ¤íŠ¸ state
        state = {
            "question": "DBì†ë³´ì™€ ì¹´ì¹´ì˜¤í˜ì´ ì—¬í–‰ìë³´í—˜ ì°¨ì´ ë¹„êµ",
            "passages": []
        }
        
        result = compare_node(state)
        
        # fallback ë‹µë³€ í™•ì¸
        assert "draft_answer" in result
        assert "final_answer" in result
        assert "ë¹„êµ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤" in result["draft_answer"]["conclusion"]
        assert "LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤." in result["draft_answer"]["evidence"]
        assert "ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤." in result["draft_answer"]["caveats"]
        assert "comparison_table" in result["draft_answer"]
        assert result["draft_answer"]["comparison_table"]["rows"] == [["ì˜¤ë¥˜", "LLM í˜¸ì¶œ ì‹¤íŒ¨"]]
        
        print("âœ… compare_node LLM ì‹¤íŒ¨ ì²˜ë¦¬")
    
    def test_compare_node_empty_question(self):
        """ë¹ˆ ì§ˆë¬¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        state = {
            "question": "",
            "passages": []
        }
        
        # LLM í˜¸ì¶œ ì—†ì´ í…ŒìŠ¤íŠ¸ (ì‹¤ì œë¡œëŠ” Mockì´ í•„ìš”í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” êµ¬ì¡°ë§Œ í™•ì¸)
        with patch('graph.nodes.answerers.compare.get_llm') as mock_get_llm:
            mock_llm = Mock()
            mock_response = Mock()
            mock_response.text = json.dumps({
                "conclusion": "ë¹„êµ ë¶„ì„ì„ ìœ„í•´ ì§ˆë¬¸ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤",
                "evidence": [],
                "caveats": [],
                "quotes": [],
                "comparison_table": {
                    "headers": ["í•­ëª©", "ë¹„êµ ê²°ê³¼"],
                    "rows": [["ì§ˆë¬¸", "ë¹ˆ ì§ˆë¬¸"]]
                }
            }, ensure_ascii=False)
            mock_llm.generate_content.return_value = mock_response
            mock_get_llm.return_value = mock_llm
            
            result = compare_node(state)
            
            assert "draft_answer" in result
            assert "final_answer" in result
            assert "comparison_table" in result["draft_answer"]
            
        print("âœ… ë¹ˆ ì§ˆë¬¸ ì²˜ë¦¬")
    
    def test_compare_node_quotes_generation(self):
        """ì¶œì²˜ ì •ë³´ ìë™ ìƒì„± í…ŒìŠ¤íŠ¸"""
        with patch('graph.nodes.answerers.compare.get_llm') as mock_get_llm:
            mock_llm = Mock()
            mock_response = Mock()
            mock_response.text = json.dumps({
                "conclusion": "í…ŒìŠ¤íŠ¸ ê²°ë¡ ",
                "evidence": ["í…ŒìŠ¤íŠ¸ ì¦ê±°"],
                "caveats": ["í…ŒìŠ¤íŠ¸ ì£¼ì˜ì‚¬í•­"],
                "quotes": [],
                "comparison_table": {
                    "headers": ["í•­ëª©", "ê²°ê³¼"],
                    "rows": [["í…ŒìŠ¤íŠ¸", "ì„±ê³µ"]]
                }
            }, ensure_ascii=False)
            mock_llm.generate_content.return_value = mock_response
            mock_get_llm.return_value = mock_llm
            
            state = {
                "question": "í…ŒìŠ¤íŠ¸ ì§ˆë¬¸",
                "passages": [
                    {
                        "doc_id": "DBì†í•´ë³´í—˜_ì—¬í–‰ìë³´í—˜ì•½ê´€",
                        "page": 15,
                        "text": "ì‚¬ë§ë³´ì¥ í•œë„ëŠ” 1ì–µì›ì´ë©°, ìƒí•´ë³´ì¥ì€ 5ì²œë§Œì›ì…ë‹ˆë‹¤."
                    },
                    {
                        "doc_id": "ì¹´ì¹´ì˜¤í˜ì´_ì—¬í–‰ìë³´í—˜ì•½ê´€",
                        "page": 12,
                        "text": "ì‚¬ë§ë³´ì¥ í•œë„ëŠ” 5ì²œë§Œì›ì´ë©°, ìƒí•´ë³´ì¥ì€ 3ì²œë§Œì›ì…ë‹ˆë‹¤."
                    }
                ]
            }
            
            result = compare_node(state)
            
            # quotes ìë™ ìƒì„± í™•ì¸
            quotes = result["draft_answer"]["quotes"]
            assert len(quotes) == 2  # ìƒìœ„ 2ê°œë§Œ
            assert "DBì†í•´ë³´í—˜_ì—¬í–‰ìë³´í—˜ì•½ê´€_í˜ì´ì§€15" in quotes[0]["source"]
            assert "ì¹´ì¹´ì˜¤í˜ì´_ì—¬í–‰ìë³´í—˜ì•½ê´€_í˜ì´ì§€12" in quotes[1]["source"]
            
        print("âœ… ì¶œì²˜ ì •ë³´ ìë™ ìƒì„±")
    
    def test_compare_node_comparison_table_structure(self):
        """comparison_table êµ¬ì¡° ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        with patch('graph.nodes.answerers.compare.get_llm') as mock_get_llm:
            mock_llm = Mock()
            mock_response = Mock()
            mock_response.text = json.dumps({
                "conclusion": "í…ŒìŠ¤íŠ¸ ê²°ë¡ ",
                "evidence": ["í…ŒìŠ¤íŠ¸ ì¦ê±°"],
                "caveats": ["í…ŒìŠ¤íŠ¸ ì£¼ì˜ì‚¬í•­"],
                "quotes": [],
                "comparison_table": {
                    "headers": ["í•­ëª©", "DBì†ë³´", "ì¹´ì¹´ì˜¤í˜ì´", "ì°¨ì´ì "],
                    "rows": [
                        ["ì‚¬ë§ë³´ì¥", "1ì–µì›", "5ì²œë§Œì›", "DBì†ë³´ 2ë°° ë†’ìŒ"],
                        ["ìƒí•´ë³´ì¥", "5ì²œë§Œì›", "3ì²œë§Œì›", "DBì†ë³´ 1.7ë°° ë†’ìŒ"]
                    ]
                }
            }, ensure_ascii=False)
            mock_llm.generate_content.return_value = mock_response
            mock_get_llm.return_value = mock_llm
            
            state = {
                "question": "í…ŒìŠ¤íŠ¸ ì§ˆë¬¸",
                "passages": []
            }
            
            result = compare_node(state)
            
            # comparison_table êµ¬ì¡° í™•ì¸
            table = result["draft_answer"]["comparison_table"]
            assert "headers" in table
            assert "rows" in table
            assert len(table["headers"]) == 4
            assert len(table["rows"]) == 2
            assert table["headers"] == ["í•­ëª©", "DBì†ë³´", "ì¹´ì¹´ì˜¤í˜ì´", "ì°¨ì´ì "]
            assert table["rows"][0] == ["ì‚¬ë§ë³´ì¥", "1ì–µì›", "5ì²œë§Œì›", "DBì†ë³´ 2ë°° ë†’ìŒ"]
            
        print("âœ… comparison_table êµ¬ì¡° ê²€ì¦")


@pytest.mark.benchmark
def test_compare_node_performance_benchmark():
    """Compare ë…¸ë“œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ¯ Compare ë…¸ë“œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("="*60)
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
    test_cases = [
        {
            "question": "DBì†ë³´ì™€ ì¹´ì¹´ì˜¤í˜ì´ ì—¬í–‰ìë³´í—˜ ì°¨ì´ ë¹„êµ",
            "passages": [
                {
                    "doc_id": "DBì†í•´ë³´í—˜_ì—¬í–‰ìë³´í—˜ì•½ê´€",
                    "page": 15,
                    "text": "ì‚¬ë§ë³´ì¥ í•œë„ëŠ” 1ì–µì›ì´ë©°, ìƒí•´ë³´ì¥ì€ 5ì²œë§Œì›ì…ë‹ˆë‹¤."
                },
                {
                    "doc_id": "ì¹´ì¹´ì˜¤í˜ì´_ì—¬í–‰ìë³´í—˜ì•½ê´€",
                    "page": 12,
                    "text": "ì‚¬ë§ë³´ì¥ í•œë„ëŠ” 5ì²œë§Œì›ì´ë©°, ìƒí•´ë³´ì¥ì€ 3ì²œë§Œì›ì…ë‹ˆë‹¤."
                }
            ]
        },
        {
            "question": "ë³´í—˜ì‚¬ë³„ ì—¬í–‰ìë³´í—˜ ê°€ê²© ë¹„êµ",
            "passages": [
                {
                    "doc_id": "KBì†í•´ë³´í—˜_ì—¬í–‰ìë³´í—˜ì•½ê´€",
                    "page": 20,
                    "text": "1ì¼ ë³´í—˜ë£ŒëŠ” 3,000ì›ë¶€í„° ì‹œì‘ë©ë‹ˆë‹¤."
                },
                {
                    "doc_id": "ì‚¼ì„±í™”ì¬_ì—¬í–‰ìë³´í—˜ì•½ê´€",
                    "page": 18,
                    "text": "1ì¼ ë³´í—˜ë£ŒëŠ” 2,500ì›ë¶€í„° ì‹œì‘ë©ë‹ˆë‹¤."
                }
            ]
        },
        {
            "question": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ ë‚´ìš© ì°¨ì´ì ",
            "passages": [
                {
                    "doc_id": "í˜„ëŒ€í•´ìƒ_ì—¬í–‰ìë³´í—˜ì•½ê´€",
                    "page": 25,
                    "text": "ì˜ë£Œë¹„ ë³´ìƒ í•œë„ëŠ” 1ì–µì›ê¹Œì§€ ë³´ì¥í•©ë‹ˆë‹¤."
                },
                {
                    "doc_id": "DBì†í•´ë³´í—˜_ì—¬í–‰ìë³´í—˜ì•½ê´€",
                    "page": 30,
                    "text": "ì˜ë£Œë¹„ ë³´ìƒ í•œë„ëŠ” 5ì²œë§Œì›ê¹Œì§€ ë³´ì¥í•©ë‹ˆë‹¤."
                }
            ]
        }
    ]
    
    success_count = 0
    total_count = len(test_cases)
    
    for i, case in enumerate(test_cases, 1):
        try:
            with patch('graph.nodes.answerers.compare.get_llm') as mock_get_llm:
                mock_llm = Mock()
                mock_response = Mock()
                mock_response.text = json.dumps({
                    "conclusion": f"í…ŒìŠ¤íŠ¸ {i} ë¹„êµ ê²°ê³¼",
                    "evidence": [f"í…ŒìŠ¤íŠ¸ {i} ì¦ê±°"],
                    "caveats": [f"í…ŒìŠ¤íŠ¸ {i} ì£¼ì˜ì‚¬í•­"],
                    "quotes": [],
                    "comparison_table": {
                        "headers": ["í•­ëª©", "ë³´í—˜ì‚¬A", "ë³´í—˜ì‚¬B"],
                        "rows": [["í…ŒìŠ¤íŠ¸í•­ëª©", "ê°’A", "ê°’B"]]
                    }
                }, ensure_ascii=False)
                mock_llm.generate_content.return_value = mock_response
                mock_get_llm.return_value = mock_llm
                
                result = compare_node(case)
                
                # ê¸°ë³¸ êµ¬ì¡° í™•ì¸
                assert "draft_answer" in result
                assert "final_answer" in result
                assert "conclusion" in result["draft_answer"]
                assert "comparison_table" in result["draft_answer"]
                
                success_count += 1
                print(f"âœ… í…ŒìŠ¤íŠ¸ {i}: {case['question'][:30]}...")
                
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ {i}: {case['question'][:30]}... - {str(e)}")
    
    success_rate = (success_count / total_count) * 100
    print(f"\nğŸ“Š ì„±ê³µë¥ : {success_count}/{total_count} ({success_rate:.1f}%)")
    
    if success_rate >= 90:
        print("ğŸ‰ ìš°ìˆ˜í•œ ì„±ëŠ¥!")
    elif success_rate >= 80:
        print("ğŸ‘ ì–‘í˜¸í•œ ì„±ëŠ¥")
    else:
        print("âš ï¸ ê°œì„  í•„ìš”")
    
    return success_rate


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
    test_compare_node_performance_benchmark()
