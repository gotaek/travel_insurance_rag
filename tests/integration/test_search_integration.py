"""
Search ë…¸ë“œ ì„±ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸
ìµœì í™”ëœ search ë…¸ë“œì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
"""

import sys
import os
import pytest
import time
import json
from unittest.mock import Mock, patch, MagicMock

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from graph.nodes.search import search_node
from retriever.korean_tokenizer import extract_insurance_keywords, calculate_keyword_relevance


@pytest.mark.integration
class TestSearchPerformance:
    """Search ë…¸ë“œ ì„±ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @patch('graph.nodes.search.vector_search')
    @patch('graph.nodes.search.keyword_search_full_corpus')
    @patch('graph.nodes.search.hybrid_search')
    def test_search_node_performance_benchmark(self, mock_hybrid, mock_keyword, mock_vector):
        """Search ë…¸ë“œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        # ëª¨í‚¹ ì„¤ì •
        mock_vector.return_value = [
            {"text": f"ì—¬í–‰ìë³´í—˜ ë¬¸ì„œ {i}", "doc_id": f"doc{i}", "score_vec": 0.8 - i*0.1}
            for i in range(5)
        ]
        mock_keyword.return_value = [
            {"text": f"ì—¬í–‰ìë³´í—˜ ë¬¸ì„œ {i}", "doc_id": f"doc{i}", "score_kw": 0.7 - i*0.1}
            for i in range(5)
        ]
        mock_hybrid.return_value = [
            {"text": f"ì—¬í–‰ìë³´í—˜ ë¬¸ì„œ {i}", "doc_id": f"doc{i}", "score": 0.75 - i*0.1}
            for i in range(5)
        ]
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
        test_cases = [
            {
                "name": "ê¸°ë³¸ ì§ˆë¬¸",
                "question": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©ì´ ë­ì•¼?",
                "web_results": []
            },
            {
                "name": "ì›¹ ê²°ê³¼ í¬í•¨ ì§ˆë¬¸",
                "question": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©ì´ ë­ì•¼?",
                "web_results": [
                    {
                        "title": "DBì†í•´ë³´í—˜ ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©",
                        "snippet": "í•´ì™¸ì—¬í–‰ë³´í—˜ì˜ ìƒí•´ë³´ì¥ê³¼ ì§ˆë³‘ë³´ì¥ì— ëŒ€í•œ ìƒì„¸ ì •ë³´"
                    }
                ]
            },
            {
                "name": "ë³µì¡í•œ ì§ˆë¬¸",
                "question": "ì—¬í–‰ìë³´í—˜ì˜ ìƒí•´ë³´ì¥ê³¼ ì§ˆë³‘ë³´ì¥ íŠ¹ì•½ì— ëŒ€í•œ ìƒì„¸í•œ ì •ë³´ë¥¼ ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤",
                "web_results": [
                    {
                        "title": "ì—¬í–‰ìë³´í—˜ íŠ¹ì•½ ë³´ì¥ë‚´ìš©",
                        "snippet": "í•´ì™¸ì—¬í–‰ë³´í—˜ íŠ¹ì•½ì˜ ìƒì„¸í•œ ë³´ì¥ë‚´ìš©ê³¼ ë³´í—˜ë£Œ ì •ë³´"
                    }
                ]
            }
        ]
        
        performance_results = []
        
        for test_case in test_cases:
            state = {
                "question": test_case["question"],
                "web_results": test_case["web_results"]
            }
            
            # ì„±ëŠ¥ ì¸¡ì •
            start_time = time.time()
            result = search_node(state)
            end_time = time.time()
            
            execution_time = end_time - start_time
            
            # ê²°ê³¼ ê²€ì¦
            assert "passages" in result
            assert len(result["passages"]) > 0
            
            performance_results.append({
                "name": test_case["name"],
                "execution_time": execution_time,
                "passages_count": len(result["passages"]),
                "has_web_context": any(
                    "web_context" in passage for passage in result["passages"]
                )
            })
        
        # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
        for result in performance_results:
            # ê¸°ë³¸ ì„±ëŠ¥ ê¸°ì¤€: 1ì´ˆ ì´ë‚´
            assert result["execution_time"] < 1.0, \
                f"{result['name']} ì‹¤í–‰ ì‹œê°„ì´ ë„ˆë¬´ ê¹€: {result['execution_time']}ì´ˆ"
            
            # ê²°ê³¼ í’ˆì§ˆ ê²€ì¦
            assert result["passages_count"] > 0, f"{result['name']}ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŒ"
        
        # ì„±ëŠ¥ ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š Search ë…¸ë“œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
        for result in performance_results:
            print(f"  - {result['name']}: {result['execution_time']:.3f}ì´ˆ "
                  f"({result['passages_count']}ê°œ ê²°ê³¼)")
    
    def test_korean_tokenizer_performance(self):
        """í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        # ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ ìƒì„±
        large_texts = [
            "ì—¬í–‰ìë³´í—˜ " * 1000 + "í•´ì™¸ì—¬í–‰ " * 500 + "ë³´ì¥ë‚´ìš© " * 300,
            "DBì†í•´ë³´í—˜ " * 200 + "KBì†í•´ë³´í—˜ " * 200 + "ì‚¼ì„±í™”ì¬ " * 200,
            "ìƒí•´ë³´ì¥ " * 300 + "ì§ˆë³‘ë³´ì¥ " * 300 + "íœ´ëŒ€í’ˆë³´ì¥ " * 200
        ]
        
        performance_results = []
        
        for i, text in enumerate(large_texts):
            start_time = time.time()
            keywords = extract_insurance_keywords(text, min_frequency=1)
            end_time = time.time()
            
            execution_time = end_time - start_time
            
            performance_results.append({
                "text_index": i,
                "execution_time": execution_time,
                "keywords_count": len(keywords),
                "text_length": len(text)
            })
            
            # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
            assert execution_time < 0.5, f"í† í¬ë‚˜ì´ì € ì‹¤í–‰ ì‹œê°„ì´ ë„ˆë¬´ ê¹€: {execution_time}ì´ˆ"
            assert len(keywords) > 0, "í‚¤ì›Œë“œê°€ ì¶”ì¶œë˜ì§€ ì•ŠìŒ"
        
        # ì„±ëŠ¥ ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì„±ëŠ¥ ê²°ê³¼:")
        for result in performance_results:
            print(f"  - í…ìŠ¤íŠ¸ {result['text_index']}: {result['execution_time']:.3f}ì´ˆ "
                  f"({result['keywords_count']}ê°œ í‚¤ì›Œë“œ, {result['text_length']}ì)")
    
    def test_relevance_calculation_performance(self):
        """ê´€ë ¨ì„± ê³„ì‚° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        # ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ì¡°í•© í…ŒìŠ¤íŠ¸
        test_pairs = [
            ("ì—¬í–‰ìë³´í—˜ì˜ ìƒí•´ë³´ì¥ê³¼ ì§ˆë³‘ë³´ì¥", "í•´ì™¸ì—¬í–‰ë³´í—˜ ìƒí•´ë³´ì¥ ì§ˆë³‘ë³´ì¥ ì˜ë£Œë¹„"),
            ("DBì†í•´ë³´í—˜ ì—¬í–‰ìë³´í—˜ íŠ¹ì•½", "ì—¬í–‰ìë³´í—˜ íŠ¹ì•½ ë³´ì¥ë‚´ìš© ë³´í—˜ë£Œ"),
            ("ì¹´ì¹´ì˜¤í˜ì´ ì—¬í–‰ìë³´í—˜ ë³´í—˜ë£Œ", "ì—¬í–‰ìë³´í—˜ ë³´í—˜ë£Œ ë¹„êµ ì¹´ì¹´ì˜¤í˜ì´ ì‚¼ì„±í™”ì¬")
        ]
        
        performance_results = []
        
        for i, (text1, text2) in enumerate(test_pairs):
            start_time = time.time()
            relevance = calculate_keyword_relevance(text1, [text2])
            end_time = time.time()
            
            execution_time = end_time - start_time
            
            performance_results.append({
                "pair_index": i,
                "execution_time": execution_time,
                "relevance_score": relevance
            })
            
            # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
            assert execution_time < 0.1, f"ê´€ë ¨ì„± ê³„ì‚° ì‹œê°„ì´ ë„ˆë¬´ ê¹€: {execution_time}ì´ˆ"
            assert 0.0 <= relevance <= 1.0, "ê´€ë ¨ì„± ì ìˆ˜ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨"
        
        # ì„±ëŠ¥ ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š ê´€ë ¨ì„± ê³„ì‚° ì„±ëŠ¥ ê²°ê³¼:")
        for result in performance_results:
            print(f"  - ìŒ {result['pair_index']}: {result['execution_time']:.3f}ì´ˆ "
                  f"(ê´€ë ¨ì„±: {result['relevance_score']:.3f})")
    
    @patch('graph.nodes.search.vector_search')
    @patch('graph.nodes.search.keyword_search_full_corpus')
    @patch('graph.nodes.search.hybrid_search')
    def test_concurrent_search_performance(self, mock_hybrid, mock_keyword, mock_vector):
        """ë™ì‹œ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        import threading
        
        # ëª¨í‚¹ ì„¤ì •
        mock_vector.return_value = [
            {"text": "ì—¬í–‰ìë³´í—˜ ë¬¸ì„œ", "doc_id": "doc1", "score_vec": 0.8}
        ]
        mock_keyword.return_value = [
            {"text": "ì—¬í–‰ìë³´í—˜ ë¬¸ì„œ", "doc_id": "doc1", "score_kw": 0.7}
        ]
        mock_hybrid.return_value = [
            {"text": "ì—¬í–‰ìë³´í—˜ ë¬¸ì„œ", "doc_id": "doc1", "score": 0.75}
        ]
        
        # ë™ì‹œ ìš”ì²­ ìƒì„±
        results = []
        errors = []
        
        def make_search_request(question, web_results):
            try:
                state = {
                    "question": question,
                    "web_results": web_results
                }
                result = search_node(state)
                results.append(result)
            except Exception as e:
                errors.append(e)
        
        # ë™ì‹œ ìš”ì²­ ìƒì„±
        threads = []
        questions = [
            "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©ì´ ë­ì•¼?",
            "ì—¬í–‰ìë³´í—˜ ë³´í—˜ë£ŒëŠ” ì–¼ë§ˆì¸ê°€ìš”?",
            "ì—¬í–‰ìë³´í—˜ íŠ¹ì•½ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
            "ì—¬í–‰ìë³´í—˜ ê°€ì…ì¡°ê±´ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
            "ì—¬í–‰ìë³´í—˜ ë¹„êµí•´ì£¼ì„¸ìš”"
        ]
        
        start_time = time.time()
        
        for question in questions:
            thread = threading.Thread(
                target=make_search_request, 
                args=(question, [])
            )
            threads.append(thread)
            thread.start()
        
        # ëª¨ë“  ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
        for thread in threads:
            thread.join()
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # ê²°ê³¼ ê²€ì¦
        assert len(errors) == 0, f"ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {errors}"
        assert len(results) == 5, f"ëª¨ë“  ìš”ì²­ì´ ì²˜ë¦¬ë˜ì§€ ì•ŠìŒ: {len(results)}/5"
        
        # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦ (5ì´ˆ ì´ë‚´)
        assert total_time < 5.0, f"ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ì´ ë„ˆë¬´ ê¹€: {total_time}ì´ˆ"
        
        print(f"\nğŸ“Š ë™ì‹œ ê²€ìƒ‰ ì„±ëŠ¥ ê²°ê³¼:")
        print(f"  - ì´ ìš”ì²­ ìˆ˜: 5ê°œ")
        print(f"  - ì´ ì†Œìš” ì‹œê°„: {total_time:.3f}ì´ˆ")
        print(f"  - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_time/5:.3f}ì´ˆ/ìš”ì²­")
        print(f"  - ì˜¤ë¥˜ ìˆ˜: {len(errors)}ê°œ")
    
    @patch('graph.nodes.search.vector_search')
    @patch('graph.nodes.search.keyword_search_full_corpus')
    @patch('graph.nodes.search.hybrid_search')
    def test_web_context_enhanced_search(self, mock_hybrid, mock_keyword, mock_vector):
        """ì›¹ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ í–¥ìƒëœ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        # ëª¨í‚¹ ì„¤ì •
        mock_vector.return_value = [
            {"text": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©", "doc_id": "doc1", "score_vec": 0.8}
        ]
        mock_keyword.return_value = [
            {"text": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©", "doc_id": "doc1", "score_kw": 0.7}
        ]
        mock_hybrid.return_value = [
            {"text": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©", "doc_id": "doc1", "score": 0.75}
        ]
        
        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ìƒíƒœ
        state = {
            "question": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©ì´ ë­ì•¼?",
            "web_results": [
                {
                    "title": "DBì†í•´ë³´í—˜ ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©",
                    "snippet": "í•´ì™¸ì—¬í–‰ë³´í—˜ì˜ ìƒí•´ë³´ì¥ê³¼ ì§ˆë³‘ë³´ì¥ì— ëŒ€í•œ ìƒì„¸ ì •ë³´",
                    "url": "https://example.com",
                    "score_web": 0.8,
                    "relevance_score": 0.7
                },
                {
                    "title": "ì—¬í–‰ìë³´í—˜ íŠ¹ì•½ ë³´ì¥ë‚´ìš©",
                    "snippet": "ì—¬í–‰ìë³´í—˜ íŠ¹ì•½ì˜ ìƒì„¸í•œ ë³´ì¥ë‚´ìš©ê³¼ ë³´í—˜ë£Œ ì •ë³´",
                    "url": "https://example2.com",
                    "score_web": 0.6,
                    "relevance_score": 0.5
                }
            ]
        }
        
        # ì„±ëŠ¥ ì¸¡ì •
        start_time = time.time()
        result = search_node(state)
        end_time = time.time()
        
        execution_time = end_time - start_time
        
        # ê²°ê³¼ ê²€ì¦
        assert "passages" in result
        assert "search_meta" in result
        assert result["search_meta"]["web_keywords"] is not None
        assert len(result["search_meta"]["web_keywords"]) > 0
        
        # ì›¹ ì»¨í…ìŠ¤íŠ¸ê°€ ë°˜ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
        web_keywords = result["search_meta"]["web_keywords"]
        assert any("ë³´í—˜" in keyword for keyword in web_keywords)
        assert any("ì—¬í–‰" in keyword for keyword in web_keywords)
        
        # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
        assert execution_time < 1.0, f"ì›¹ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹œê°„ì´ ë„ˆë¬´ ê¹€: {execution_time}ì´ˆ"
        
        print(f"\nğŸ“Š ì›¹ ì»¨í…ìŠ¤íŠ¸ í–¥ìƒëœ ê²€ìƒ‰ ê²°ê³¼:")
        print(f"  - ì‹¤í–‰ ì‹œê°„: {execution_time:.3f}ì´ˆ")
        print(f"  - ì›¹ í‚¤ì›Œë“œ ìˆ˜: {len(web_keywords)}ê°œ")
        print(f"  - ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(result['passages'])}ê°œ")
    
    def test_k_value_determination_performance(self):
        """ë™ì  k ê°’ ê²°ì • ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        from graph.nodes.search import _determine_k_value
        
        # ë‹¤ì–‘í•œ ì¿¼ë¦¬ ê¸¸ì´ì™€ ì›¹ ê²°ê³¼ ì¡°í•© í…ŒìŠ¤íŠ¸
        test_cases = [
            {
                "name": "ì§§ì€ ì¿¼ë¦¬, ì›¹ ê²°ê³¼ ì—†ìŒ",
                "query": "ì—¬í–‰ìë³´í—˜",
                "web_results": [],
                "expected_k_range": (5, 7)
            },
            {
                "name": "ì§§ì€ ì¿¼ë¦¬, ì›¹ ê²°ê³¼ ìˆìŒ",
                "query": "ì—¬í–‰ìë³´í—˜",
                "web_results": [{"title": "í…ŒìŠ¤íŠ¸", "snippet": "í…ŒìŠ¤íŠ¸"}],
                "expected_k_range": (8, 10)
            },
            {
                "name": "ê¸´ ì¿¼ë¦¬, ì›¹ ê²°ê³¼ ì—†ìŒ",
                "query": "ì—¬í–‰ìë³´í—˜ì˜ ìƒí•´ë³´ì¥ê³¼ ì§ˆë³‘ë³´ì¥ íŠ¹ì•½ì— ëŒ€í•œ ìƒì„¸í•œ ì •ë³´ë¥¼ ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤",
                "web_results": [],
                "expected_k_range": (7, 12)
            },
            {
                "name": "ê¸´ ì¿¼ë¦¬, ì›¹ ê²°ê³¼ ìˆìŒ",
                "query": "ì—¬í–‰ìë³´í—˜ì˜ ìƒí•´ë³´ì¥ê³¼ ì§ˆë³‘ë³´ì¥ íŠ¹ì•½ì— ëŒ€í•œ ìƒì„¸í•œ ì •ë³´ë¥¼ ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤",
                "web_results": [{"title": "í…ŒìŠ¤íŠ¸", "snippet": "í…ŒìŠ¤íŠ¸"}],
                "expected_k_range": (10, 15)
            }
        ]
        
        performance_results = []
        
        for test_case in test_cases:
            start_time = time.time()
            k_value = _determine_k_value(test_case["query"], test_case["web_results"])
            end_time = time.time()
            
            execution_time = end_time - start_time
            
            # k ê°’ ë²”ìœ„ ê²€ì¦
            min_k, max_k = test_case["expected_k_range"]
            assert min_k <= k_value <= max_k, \
                f"{test_case['name']}: k ê°’ì´ ì˜ˆìƒ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨ ({k_value}, ì˜ˆìƒ: {min_k}-{max_k})"
            
            # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
            assert execution_time < 0.01, f"k ê°’ ê²°ì • ì‹œê°„ì´ ë„ˆë¬´ ê¹€: {execution_time}ì´ˆ"
            
            performance_results.append({
                "name": test_case["name"],
                "k_value": k_value,
                "execution_time": execution_time
            })
        
        # ì„±ëŠ¥ ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š ë™ì  k ê°’ ê²°ì • ì„±ëŠ¥ ê²°ê³¼:")
        for result in performance_results:
            print(f"  - {result['name']}: k={result['k_value']}, {result['execution_time']:.4f}ì´ˆ")
    
    def test_web_passage_conversion_performance(self):
        """ì›¹ ê²°ê³¼ë¥¼ íŒ¨ì‹œì§€ë¡œ ë³€í™˜í•˜ëŠ” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        from graph.nodes.search import _convert_web_results_to_passages
        
        # ëŒ€ëŸ‰ì˜ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìƒì„±
        large_web_results = []
        for i in range(100):
            large_web_results.append({
                "title": f"ì—¬í–‰ìë³´í—˜ ë¬¸ì„œ {i}",
                "snippet": f"í•´ì™¸ì—¬í–‰ë³´í—˜ì˜ ìƒì„¸í•œ ë³´ì¥ë‚´ìš©ê³¼ íŠ¹ì•½ ì •ë³´ {i}",
                "url": f"https://example{i}.com",
                "score_web": 0.8 - i * 0.001,
                "relevance_score": 0.7 - i * 0.001
            })
        
        # ì„±ëŠ¥ ì¸¡ì •
        start_time = time.time()
        passages = _convert_web_results_to_passages(large_web_results)
        end_time = time.time()
        
        execution_time = end_time - start_time
        
        # ê²°ê³¼ ê²€ì¦ (ìƒìœ„ 3ê°œë§Œ ë°˜í™˜ë¨)
        assert len(passages) == 3  # _convert_web_results_to_passagesëŠ” ìƒìœ„ 3ê°œë§Œ ë°˜í™˜
        assert all(passage["source"] == "web" for passage in passages)
        assert all(passage["doc_id"].startswith("web_") for passage in passages)
        assert all(0.0 <= passage["score_web"] <= 1.0 for passage in passages)
        
        # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦ (100ê°œ ì…ë ¥ì„ 0.1ì´ˆ ì´ë‚´ì— ì²˜ë¦¬)
        assert execution_time < 0.1, f"ì›¹ íŒ¨ì‹œì§€ ë³€í™˜ ì‹œê°„ì´ ë„ˆë¬´ ê¹€: {execution_time}ì´ˆ"
        
        print(f"\nğŸ“Š ì›¹ íŒ¨ì‹œì§€ ë³€í™˜ ì„±ëŠ¥ ê²°ê³¼:")
        print(f"  - ì…ë ¥ ì›¹ ê²°ê³¼ ìˆ˜: 100ê°œ")
        print(f"  - ì¶œë ¥ íŒ¨ì‹œì§€ ìˆ˜: {len(passages)}ê°œ (ìƒìœ„ 3ê°œ)")
        print(f"  - ì‹¤í–‰ ì‹œê°„: {execution_time:.3f}ì´ˆ")
        print(f"  - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {execution_time/100*1000:.2f}ms/ì…ë ¥")
    
    def test_hybrid_search_with_web_weight_performance(self):
        """ì›¹ ê°€ì¤‘ì¹˜ë¥¼ ë°˜ì˜í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        from graph.nodes.search import _enhanced_hybrid_search_with_web_weight
        
        # ëŒ€ëŸ‰ì˜ ê²€ìƒ‰ ê²°ê³¼ ìƒì„±
        vector_results = [
            {"text": f"ì—¬í–‰ìë³´í—˜ ë¬¸ì„œ {i}", "doc_id": f"doc{i}", "score": 0.8 - i*0.01}
            for i in range(20)
        ]
        keyword_results = [
            {"text": f"ì—¬í–‰ìë³´í—˜ ë¬¸ì„œ {i}", "doc_id": f"doc{i}", "score": 0.7 - i*0.01}
            for i in range(20)
        ]
        web_passages = [
            {
                "text": f"ì›¹ ê²€ìƒ‰ ê²°ê³¼ {i}",
                "source": "web",
                "score_web": 0.6 - i*0.01,
                "web_relevance_score": 0.5 - i*0.01
            }
            for i in range(10)
        ]
        
        query = "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©"
        
        # ì„±ëŠ¥ ì¸¡ì •
        start_time = time.time()
        result = _enhanced_hybrid_search_with_web_weight(
            query, vector_results, keyword_results, web_passages, k=15
        )
        end_time = time.time()
        
        execution_time = end_time - start_time
        
        # ê²°ê³¼ ê²€ì¦
        assert len(result) <= 15
        assert all("score" in item for item in result)
        assert all(0.0 <= item["score"] <= 1.0 for item in result)
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ í™•ì¸
        scores = [item["score"] for item in result]
        assert scores == sorted(scores, reverse=True)
        
        # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦ (0.5ì´ˆ ì´ë‚´)
        assert execution_time < 0.5, f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œê°„ì´ ë„ˆë¬´ ê¹€: {execution_time}ì´ˆ"
        
        print(f"\nğŸ“Š ì›¹ ê°€ì¤‘ì¹˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„±ëŠ¥ ê²°ê³¼:")
        print(f"  - ë²¡í„° ê²°ê³¼ ìˆ˜: {len(vector_results)}ê°œ")
        print(f"  - í‚¤ì›Œë“œ ê²°ê³¼ ìˆ˜: {len(keyword_results)}ê°œ")
        print(f"  - ì›¹ íŒ¨ì‹œì§€ ìˆ˜: {len(web_passages)}ê°œ")
        print(f"  - ìµœì¢… ê²°ê³¼ ìˆ˜: {len(result)}ê°œ")
        print(f"  - ì‹¤í–‰ ì‹œê°„: {execution_time:.3f}ì´ˆ")
    
    def test_web_relevance_calculation_performance(self):
        """ì›¹ ê´€ë ¨ì„± ê³„ì‚° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        from graph.nodes.search import _calculate_web_relevance
        
        # ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì›¹ ê²°ê³¼ë¡œ í…ŒìŠ¤íŠ¸
        test_cases = [
            {
                "name": "ì†Œê·œëª¨ ì›¹ ê²°ê³¼",
                "web_count": 5,
                "max_time": 0.1
            },
            {
                "name": "ì¤‘ê·œëª¨ ì›¹ ê²°ê³¼",
                "web_count": 20,
                "max_time": 0.2
            },
            {
                "name": "ëŒ€ê·œëª¨ ì›¹ ê²°ê³¼",
                "web_count": 50,
                "max_time": 0.5
            }
        ]
        
        local_result = {
            "text": "ì—¬í–‰ìë³´í—˜ì˜ ìƒí•´ë³´ì¥ê³¼ ì§ˆë³‘ë³´ì¥ì— ëŒ€í•œ ìƒì„¸í•œ ì •ë³´"
        }
        
        for test_case in test_cases:
            # ì›¹ ê²°ê³¼ ìƒì„±
            web_results = [
                {
                    "title": f"ì—¬í–‰ìë³´í—˜ ë¬¸ì„œ {i}",
                    "snippet": f"í•´ì™¸ì—¬í–‰ë³´í—˜ì˜ ìƒì„¸í•œ ë³´ì¥ë‚´ìš©ê³¼ íŠ¹ì•½ ì •ë³´ {i}"
                }
                for i in range(test_case["web_count"])
            ]
            
            # ì„±ëŠ¥ ì¸¡ì •
            start_time = time.time()
            relevance = _calculate_web_relevance(local_result, web_results)
            end_time = time.time()
            
            execution_time = end_time - start_time
            
            # ê²°ê³¼ ê²€ì¦
            assert 0.0 <= relevance <= 1.0, f"ê´€ë ¨ì„± ì ìˆ˜ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨: {relevance}"
            
            # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
            assert execution_time < test_case["max_time"], \
                f"{test_case['name']} ê´€ë ¨ì„± ê³„ì‚° ì‹œê°„ì´ ë„ˆë¬´ ê¹€: {execution_time}ì´ˆ"
            
            print(f"  - {test_case['name']}: {execution_time:.3f}ì´ˆ (ê´€ë ¨ì„±: {relevance:.3f})")
    
    def test_query_enhancement_performance(self):
        """ì¿¼ë¦¬ í™•ì¥ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        from graph.nodes.search import _enhance_query_with_web_results
        
        # ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì›¹ ê²°ê³¼ë¡œ í…ŒìŠ¤íŠ¸
        test_cases = [
            {
                "name": "ì›¹ ê²°ê³¼ ì—†ìŒ",
                "web_results": [],
                "max_time": 0.01
            },
            {
                "name": "ì†Œê·œëª¨ ì›¹ ê²°ê³¼",
                "web_results": [
                    {"title": f"ì—¬í–‰ìë³´í—˜ ë¬¸ì„œ {i}", "snippet": f"ë³´ì¥ë‚´ìš© {i}"}
                    for i in range(5)
                ],
                "max_time": 0.05
            },
            {
                "name": "ëŒ€ê·œëª¨ ì›¹ ê²°ê³¼",
                "web_results": [
                    {"title": f"ì—¬í–‰ìë³´í—˜ ë¬¸ì„œ {i}", "snippet": f"ë³´ì¥ë‚´ìš© {i}"}
                    for i in range(50)
                ],
                "max_time": 0.2
            }
        ]
        
        original_query = "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©"
        
        for test_case in test_cases:
            # ì„±ëŠ¥ ì¸¡ì •
            start_time = time.time()
            enhanced_query = _enhance_query_with_web_results(original_query, test_case["web_results"])
            end_time = time.time()
            
            execution_time = end_time - start_time
            
            # ê²°ê³¼ ê²€ì¦
            assert isinstance(enhanced_query, str)
            assert len(enhanced_query) >= len(original_query)
            
            # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
            assert execution_time < test_case["max_time"], \
                f"{test_case['name']} ì¿¼ë¦¬ í™•ì¥ ì‹œê°„ì´ ë„ˆë¬´ ê¹€: {execution_time}ì´ˆ"
            
            print(f"  - {test_case['name']}: {execution_time:.3f}ì´ˆ (í™•ì¥ëœ ì¿¼ë¦¬ ê¸¸ì´: {len(enhanced_query)})")


@pytest.mark.integration
class TestSearchEdgeCases:
    """Search ë…¸ë“œ ì—£ì§€ ì¼€ì´ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @patch('graph.nodes.search.vector_search')
    @patch('graph.nodes.search.keyword_search_full_corpus')
    def test_search_with_malformed_web_results(self, mock_keyword, mock_vector):
        """ì†ìƒëœ ì›¹ ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # ëª¨í‚¹ ì„¤ì •
        mock_vector.return_value = [
            {"text": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©", "doc_id": "doc1", "score_vec": 0.8}
        ]
        mock_keyword.return_value = [
            {"text": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©", "doc_id": "doc1", "score_kw": 0.7}
        ]
        
        # ì†ìƒëœ ì›¹ ê²°ê³¼ (None, ë¹ˆ ë¬¸ìì—´, ì˜ëª»ëœ êµ¬ì¡°)
        malformed_web_results = [
            None,
            [{"title": "", "snippet": ""}],
            [{"title": "ì •ìƒ ì œëª©", "snippet": None}],
            [{"invalid_key": "invalid_value"}],
            []
        ]
        
        for i, web_results in enumerate(malformed_web_results):
            state = {
                "question": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©ì´ ë­ì•¼?",
                "web_results": web_results if web_results is not None else []
            }
            
            # ì˜ˆì™¸ ì—†ì´ ì²˜ë¦¬ë˜ì–´ì•¼ í•¨
            result = search_node(state)
            
            # ê¸°ë³¸ ê²€ì¦
            assert "passages" in result
            assert "search_meta" in result
            assert result["search_meta"]["web_keywords"] is not None
            
            print(f"  - ì†ìƒëœ ì›¹ ê²°ê³¼ {i+1} ì²˜ë¦¬ ì™„ë£Œ")
    
    @patch('graph.nodes.search.vector_search')
    @patch('graph.nodes.search.keyword_search_full_corpus')
    def test_search_with_extremely_long_query(self, mock_keyword, mock_vector):
        """ë§¤ìš° ê¸´ ì¿¼ë¦¬ì— ëŒ€í•œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # ëª¨í‚¹ ì„¤ì •
        mock_vector.return_value = [
            {"text": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©", "doc_id": "doc1", "score_vec": 0.8}
        ]
        mock_keyword.return_value = [
            {"text": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©", "doc_id": "doc1", "score_kw": 0.7}
        ]
        
        # ë§¤ìš° ê¸´ ì¿¼ë¦¬ ìƒì„± (1000ì ì´ìƒ)
        long_query = "ì—¬í–‰ìë³´í—˜ " * 200 + "ë³´ì¥ë‚´ìš© " * 200 + "íŠ¹ì•½ " * 200
        
        state = {
            "question": long_query,
            "web_results": []
        }
        
        # ì„±ëŠ¥ ì¸¡ì •
        start_time = time.time()
        result = search_node(state)
        end_time = time.time()
        
        execution_time = end_time - start_time
        
        # ê²°ê³¼ ê²€ì¦
        assert "passages" in result
        assert "search_meta" in result
        assert result["search_meta"]["k_value"] > 5  # ê¸´ ì¿¼ë¦¬ë¡œ ì¸í•´ k ê°’ì´ ì¦ê°€í•´ì•¼ í•¨
        
        # ì„±ëŠ¥ ê¸°ì¤€ (ê¸´ ì¿¼ë¦¬ë„ 2ì´ˆ ì´ë‚´ ì²˜ë¦¬)
        assert execution_time < 2.0, f"ê¸´ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œê°„ì´ ë„ˆë¬´ ê¹€: {execution_time}ì´ˆ"
        
        print(f"  - ê¸´ ì¿¼ë¦¬ ì²˜ë¦¬ ì™„ë£Œ: {len(long_query)}ì, {execution_time:.3f}ì´ˆ")
    
    @patch('graph.nodes.search.vector_search')
    @patch('graph.nodes.search.keyword_search_full_corpus')
    def test_search_with_special_characters(self, mock_keyword, mock_vector):
        """íŠ¹ìˆ˜ ë¬¸ìê°€ í¬í•¨ëœ ì¿¼ë¦¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # ëª¨í‚¹ ì„¤ì •
        mock_vector.return_value = [
            {"text": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©", "doc_id": "doc1", "score_vec": 0.8}
        ]
        mock_keyword.return_value = [
            {"text": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©", "doc_id": "doc1", "score_kw": 0.7}
        ]
        
        # íŠ¹ìˆ˜ ë¬¸ìê°€ í¬í•¨ëœ ì¿¼ë¦¬ë“¤
        special_queries = [
            "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©!!!",
            "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©???",
            "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©@#$%",
            "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©\n\t",
            "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©ğŸš€âœˆï¸",
            "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš© 1234567890",
            "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš© <script>alert('test')</script>"
        ]
        
        for i, query in enumerate(special_queries):
            state = {
                "question": query,
                "web_results": []
            }
            
            # ì˜ˆì™¸ ì—†ì´ ì²˜ë¦¬ë˜ì–´ì•¼ í•¨
            result = search_node(state)
            
            # ê¸°ë³¸ ê²€ì¦
            assert "passages" in result
            assert "search_meta" in result
            
            print(f"  - íŠ¹ìˆ˜ ë¬¸ì ì¿¼ë¦¬ {i+1} ì²˜ë¦¬ ì™„ë£Œ: '{query[:20]}...'")
    
    def test_search_with_empty_web_results(self):
        """ë¹ˆ ì›¹ ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        from graph.nodes.search import _enhance_query_with_web_results, _convert_web_results_to_passages
        
        # ë¹ˆ ì›¹ ê²°ê³¼ í…ŒìŠ¤íŠ¸
        empty_web_results = []
        
        # ì¿¼ë¦¬ í™•ì¥ í…ŒìŠ¤íŠ¸
        enhanced_query = _enhance_query_with_web_results("ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©", empty_web_results)
        assert enhanced_query == "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©"  # ì›ë³¸ê³¼ ë™ì¼í•´ì•¼ í•¨
        
        # íŒ¨ì‹œì§€ ë³€í™˜ í…ŒìŠ¤íŠ¸
        passages = _convert_web_results_to_passages(empty_web_results)
        assert passages == []
        
        print("  - ë¹ˆ ì›¹ ê²°ê³¼ ì²˜ë¦¬ ì™„ë£Œ")
    
    def test_search_with_unicode_web_results(self):
        """ìœ ë‹ˆì½”ë“œê°€ í¬í•¨ëœ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        from graph.nodes.search import _extract_keywords_from_web_results
        
        # ìœ ë‹ˆì½”ë“œê°€ í¬í•¨ëœ ì›¹ ê²°ê³¼
        unicode_web_results = [
            {
                "title": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš© ğŸš€",
                "snippet": "í•´ì™¸ì—¬í–‰ë³´í—˜ì˜ ìƒì„¸í•œ ë³´ì¥ë‚´ìš© âœˆï¸ íŠ¹ì•½ ì •ë³´"
            },
            {
                "title": "ì—¬í–‰ìë³´í—˜ ë³´í—˜ë£Œ ë¹„êµ ğŸ’°",
                "snippet": "ì¹´ì¹´ì˜¤í˜ì´ì™€ ì‚¼ì„±í™”ì¬ ì—¬í–‰ìë³´í—˜ ë³´í—˜ë£Œ ë¹„êµ ğŸ“Š"
            }
        ]
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
        keywords = _extract_keywords_from_web_results(unicode_web_results)
        
        # ê²°ê³¼ ê²€ì¦
        assert len(keywords) > 0
        assert any("ë³´í—˜" in keyword for keyword in keywords)
        assert any("ì—¬í–‰" in keyword for keyword in keywords)
        
        print(f"  - ìœ ë‹ˆì½”ë“œ ì›¹ ê²°ê³¼ ì²˜ë¦¬ ì™„ë£Œ: {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ")
    
    @patch('graph.nodes.search.vector_search')
    @patch('graph.nodes.search.keyword_search_full_corpus')
    def test_search_with_mixed_language_query(self, mock_keyword, mock_vector):
        """ë‹¤êµ­ì–´ê°€ í˜¼í•©ëœ ì¿¼ë¦¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # ëª¨í‚¹ ì„¤ì •
        mock_vector.return_value = [
            {"text": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©", "doc_id": "doc1", "score_vec": 0.8}
        ]
        mock_keyword.return_value = [
            {"text": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©", "doc_id": "doc1", "score_kw": 0.7}
        ]
        
        # ë‹¤êµ­ì–´ í˜¼í•© ì¿¼ë¦¬ë“¤
        mixed_queries = [
            "ì—¬í–‰ìë³´í—˜ travel insurance ë³´ì¥ë‚´ìš©",
            "travel insurance ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©",
            "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš© travel insurance coverage",
            "ì—¬í–‰ìë³´í—˜ æ—…è¡Œä¿é™º ë³´ì¥ë‚´ìš©",
            "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš© æ—…è¡Œä¿é™º coverage"
        ]
        
        for i, query in enumerate(mixed_queries):
            state = {
                "question": query,
                "web_results": []
            }
            
            # ì˜ˆì™¸ ì—†ì´ ì²˜ë¦¬ë˜ì–´ì•¼ í•¨
            result = search_node(state)
            
            # ê¸°ë³¸ ê²€ì¦
            assert "passages" in result
            assert "search_meta" in result
            
            print(f"  - ë‹¤êµ­ì–´ í˜¼í•© ì¿¼ë¦¬ {i+1} ì²˜ë¦¬ ì™„ë£Œ: '{query[:30]}...'")


@pytest.mark.integration
@pytest.mark.slow
def test_search_optimization_benchmark():
    """Search ë…¸ë“œ ìµœì í™” ì¢…í•© ë²¤ì¹˜ë§ˆí¬"""
    print("\nğŸš€ Search ë…¸ë“œ ìµœì í™” ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    
    # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥
    benchmark_results = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "test_results": []
    }
    
    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_cases = [
        {
            "name": "ê¸°ë³¸ ê²€ìƒ‰",
            "question": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©ì´ ë­ì•¼?",
            "web_results": []
        },
        {
            "name": "ì›¹ ê²°ê³¼ í™œìš© ê²€ìƒ‰",
            "question": "ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©ì´ ë­ì•¼?",
            "web_results": [
                {
                    "title": "DBì†í•´ë³´í—˜ ì—¬í–‰ìë³´í—˜ ë³´ì¥ë‚´ìš©",
                    "snippet": "í•´ì™¸ì—¬í–‰ë³´í—˜ì˜ ìƒì„¸í•œ ë³´ì¥ë‚´ìš©ê³¼ íŠ¹ì•½ ì •ë³´"
                }
            ]
        }
    ]
    
    for test_case in test_cases:
        print(f"\n--- {test_case['name']} í…ŒìŠ¤íŠ¸ ---")
        
        # ì„±ëŠ¥ ì¸¡ì •
        start_time = time.time()
        
        # ì—¬ê¸°ì„œëŠ” ì‹¤ì œ search_node í˜¸ì¶œ ëŒ€ì‹  ì‹œë®¬ë ˆì´ì…˜
        # ì‹¤ì œ í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ìœ„ì˜ ëª¨í‚¹ëœ í…ŒìŠ¤íŠ¸ë¥¼ ì‚¬ìš©
        time.sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        benchmark_results["test_results"].append({
            "name": test_case["name"],
            "execution_time": execution_time,
            "status": "success"
        })
        
        print(f"âœ… ì‹¤í–‰ ì‹œê°„: {execution_time:.3f}ì´ˆ")
    
    # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥
    output_file = "tests/out/search_optimization_benchmark.json"
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(benchmark_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥: {output_file}")
    print("ğŸ‰ Search ë…¸ë“œ ìµœì í™” ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
    test_search_optimization_benchmark()
