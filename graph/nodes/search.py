from typing import Dict, Any, List, Optional
import os
import re
from collections import Counter

from retriever.vector import vector_search
from retriever.keyword import keyword_search, keyword_search_full_corpus
from retriever.hybrid import hybrid_search
from retriever.korean_tokenizer import (
    extract_insurance_keywords, 
    calculate_keyword_relevance,
    get_keyword_weights
)
from app.deps import get_settings

def search_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•œ ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
    - ë²¡í„° ê²€ìƒ‰: Chroma DB ê¸°ë°˜
    - í‚¤ì›Œë“œ ê²€ìƒ‰: ì „ì²´ ì½”í¼ìŠ¤ BM25 ê¸°ë°˜ (ë‹¤ì–‘ì„± í–¥ìƒ)
    - í•˜ì´ë¸Œë¦¬ë“œ: ë‘ ê²°ê³¼ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ë³‘í•© + ì›¹ ì»¨í…ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜
    - ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•œ ì¿¼ë¦¬ í™•ì¥ ë° ì»¨í…ìŠ¤íŠ¸ ê°œì„ 
    - ë³´í—˜ì‚¬ í•„í„°ë§ ë° ê°€ì¤‘ì¹˜ ë¶€ì—¬
    """
    q = state.get("question", "")
    web_results = state.get("web_results", [])
    insurer_filter = state.get("insurer_filter", None)  # Plannerì—ì„œ ì „ë‹¬ëœ ë³´í—˜ì‚¬ í•„í„°
    s = get_settings()
    
    # ë¹ˆ ì§ˆë¬¸ ê°€ë“œ
    if not q or not q.strip():
        return {
            **state, 
            "passages": [],
            "search_meta": {
                "reason": "empty_question",
                "k_value": 0,
                "candidates_count": 0,
                "used_query": "",
                "web_keywords": [],
                "from_cache": False
            }
        }

    # Chroma DB ê²½ë¡œ ì„¤ì •
    db_path = s.VECTOR_DIR
    collection_name = "insurance_docs"

    # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•œ ì¿¼ë¦¬ í™•ì¥
    enhanced_query = _enhance_query_with_web_results(q, web_results)
    
    # í™•ì¥ëœ ì¿¼ë¦¬ ê¸¸ì´ ê¸°ë°˜ k ê°’ ì¡°ì •
    k = _determine_k_value(enhanced_query, web_results)
    
    # ê²€ìƒ‰ ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
    search_meta = {
        "k_value": k,
        "candidates_count": 0,
        "used_query": enhanced_query,
        "web_keywords": _extract_keywords_from_web_results(web_results)[:5],  # ìƒìœ„ 5ê°œ
        "from_cache": False,
        "rerank_candidates": True,  # ë¦¬ë­í¬ë¥¼ ìœ„í•œ ëŒ€ëŸ‰ í›„ë³´ ì œê³µ
        "vector_candidates": 0,
        "keyword_candidates": 0
    }

    try:
        # ë³´í—˜ì‚¬ í•„í„°ë§ ì •ë³´ ë¡œê¹…
        if insurer_filter:
            print(f"ğŸ” ë³´í—˜ì‚¬ í•„í„°ë§ ì ìš©: {insurer_filter}")
        else:
            print("â„¹ï¸ ë³´í—˜ì‚¬ í•„í„°ë§ ì—†ìŒ - ì „ì²´ ë¬¸ì„œ ê²€ìƒ‰")
        
        # ë²¡í„° ê²€ìƒ‰ (Chroma DB ì‚¬ìš©) - ë¦¬ë­í¬ë¥¼ ìœ„í•œ ëŒ€ëŸ‰ í›„ë³´ ê²€ìƒ‰
        vec_k = min(k * 20, 200)  # ë²¡í„° ê²€ìƒ‰: 20ë°° í™•ì¥ (ìµœëŒ€ 200ê°œ)
        vec_results = vector_search(enhanced_query, db_path, collection_name, k=vec_k, insurer_filter=insurer_filter)
        
        # ì „ì²´ ì½”í¼ìŠ¤ì—ì„œ BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ (ë‹¤ì–‘ì„± í–¥ìƒ)
        kw_k = min(k * 15, 150)  # í‚¤ì›Œë“œ ê²€ìƒ‰: 15ë°° í™•ì¥ (ìµœëŒ€ 150ê°œ)
        kw_results = keyword_search_full_corpus(enhanced_query, k=kw_k, insurer_filter=insurer_filter)
        
        # ê²€ìƒ‰ ê²°ê³¼ ë¡œê¹…
        print(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: ë²¡í„° {len(vec_results)}ê°œ, í‚¤ì›Œë“œ {len(kw_results)}ê°œ")
        
        # ë””ë²„ê¹…: ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ì˜ ë³´í—˜ì‚¬ ë¶„í¬ í™•ì¸
        if vec_results:
            vec_insurer_counts = {}
            for result in vec_results:
                insurer = result.get("insurer", "Unknown")
                vec_insurer_counts[insurer] = vec_insurer_counts.get(insurer, 0) + 1
            print(f"ğŸ” ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ë³´í—˜ì‚¬ ë¶„í¬: {dict(sorted(vec_insurer_counts.items(), key=lambda x: x[1], reverse=True)[:10])}")
        
        # ë””ë²„ê¹…: í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ì˜ ë³´í—˜ì‚¬ ë¶„í¬ í™•ì¸
        if kw_results:
            kw_insurer_counts = {}
            for result in kw_results:
                insurer = result.get("insurer", "Unknown")
                kw_insurer_counts[insurer] = kw_insurer_counts.get(insurer, 0) + 1
            print(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ë³´í—˜ì‚¬ ë¶„í¬: {dict(sorted(kw_insurer_counts.items(), key=lambda x: x[1], reverse=True)[:10])}")
        
        # ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ/ì›¹ë§Œìœ¼ë¡œ ì§„í–‰
        if not vec_results and not kw_results:
            return {
                **state,
                "passages": [],
                "search_meta": {
                    **search_meta,
                    "reason": "no_search_results",
                    "candidates_count": 0
                }
            }
        
        # ì›¹ ê²°ê³¼ë¥¼ ì§ì ‘ íŒ¨ì‹œì§€ í›„ë³´ë¡œ í¬í•¨
        web_passages = _convert_web_results_to_passages(web_results)
        
        # í–¥ìƒëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì›¹ ì»¨í…ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ ë°˜ì˜)
        # ë¦¬ë­í¬ë¥¼ ìœ„í•´ ë” ë§ì€ í›„ë³´ë¥¼ rank_filterë¡œ ì „ë‹¬
        merged = _enhanced_hybrid_search_with_web_weight(
            enhanced_query, 
            vec_results, 
            kw_results, 
            web_passages,
            k=k * 10,  # rank_filterì—ì„œ ë¦¬ë­í¬í•  ìˆ˜ ìˆë„ë¡ 10ë°° í™•ì¥
            insurer_filter=insurer_filter
        )
        
        # ë””ë²„ê¹…: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ì˜ ë³´í—˜ì‚¬ ë¶„í¬ í™•ì¸
        if merged:
            merged_insurer_counts = {}
            for result in merged:
                insurer = result.get("insurer", "Unknown")
                merged_insurer_counts[insurer] = merged_insurer_counts.get(insurer, 0) + 1
            print(f"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ë³´í—˜ì‚¬ ë¶„í¬: {dict(sorted(merged_insurer_counts.items(), key=lambda x: x[1], reverse=True)[:10])}")
            print(f"ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ì´ {len(merged)}ê°œ")
        
        # ë³´í—˜ì‚¬ í•„í„°ë§ ë©”íƒ€ë°ì´í„° ì„¤ì • (ì´ë¯¸ Retriever í•¨ìˆ˜ë“¤ì—ì„œ í•„í„°ë§ ì ìš©ë¨)
        if insurer_filter:
            search_meta["insurer_filtered"] = True
            search_meta["insurer_filter"] = insurer_filter
            search_meta["filter_method"] = "retriever_level_filtering"
            search_meta["filtered_insurers"] = insurer_filter
            print(f"âœ… ë³´í—˜ì‚¬ í•„í„°ë§ ì™„ë£Œ: {insurer_filter}")
        else:
            search_meta["insurer_filtered"] = False
            search_meta["insurer_filter"] = None
            search_meta["filter_method"] = "no_filter"
            search_meta["filtered_insurers"] = []
            print("â„¹ï¸ ë³´í—˜ì‚¬ í•„í„°ë§ ì—†ìŒ")
        
        search_meta["candidates_count"] = len(merged)
        search_meta["vector_candidates"] = len(vec_results)
        search_meta["keyword_candidates"] = len(kw_results)
        
        return {**state, "passages": merged, "search_meta": search_meta}
        
    except Exception as e:
        # ì˜ˆì™¸ ë°œìƒ ì‹œ ë¹ˆ ê²°ê³¼ì™€ ì—ëŸ¬ ë©”íƒ€ë°ì´í„° ë°˜í™˜
        print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        if insurer_filter:
            print(f"ğŸ” ë³´í—˜ì‚¬ í•„í„°ë§ ì‹œë„ ì¤‘ ì˜¤ë¥˜: {insurer_filter}")
        
        return {
            **state,
            "passages": [],
            "search_meta": {
                **search_meta,
                "reason": f"search_error: {str(e)}",
                "candidates_count": 0,
                "insurer_filter": insurer_filter,
                "error_details": str(e)
            }
        }

def _enhance_query_with_web_results(original_query: str, web_results: List[Dict[str, Any]]) -> str:
    """
    ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ í™•ì¥í•©ë‹ˆë‹¤.
    
    Args:
        original_query: ì›ë³¸ ì§ˆë¬¸
        web_results: ì›¹ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        í™•ì¥ëœ ê²€ìƒ‰ ì¿¼ë¦¬
    """
    if not web_results:
        return original_query
    
    # ì›¹ ê²°ê³¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    web_keywords = _extract_keywords_from_web_results(web_results)
    
    # ì›ë³¸ ì¿¼ë¦¬ì™€ ì›¹ í‚¤ì›Œë“œ ê²°í•©
    if web_keywords:
        # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ì¶”ê°€í•˜ì—¬ ë…¸ì´ì¦ˆ ë°©ì§€
        top_keywords = web_keywords[:3]
        enhanced_query = f"{original_query} {' '.join(top_keywords)}"
        return enhanced_query
    
    return original_query

def _extract_keywords_from_web_results(web_results: List[Dict[str, Any]]) -> List[str]:
    """
    ì›¹ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        web_results: ì›¹ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ë¹ˆë„ìˆœ ì •ë ¬)
    """
    if not web_results:
        return []
    
    # ì›¹ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    all_text = []
    for result in web_results:
        title = result.get("title", "")
        snippet = result.get("snippet", "")
        if title:
            all_text.append(title)
        if snippet:
            all_text.append(snippet)
    
    # ê°œì„ ëœ í•œêµ­ì–´ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
    insurance_keywords = extract_insurance_keywords(" ".join(all_text), min_frequency=1)
    
    return insurance_keywords


def _determine_k_value(query: str, web_results: List[Dict[str, Any]]) -> int:
    """
    í™•ì¥ëœ ì¿¼ë¦¬ ê¸¸ì´ì™€ ì›¹ ê²€ìƒ‰ ê²°ê³¼ì— ë”°ë¼ ë™ì ìœ¼ë¡œ k ê°’ì„ ì¡°ì •í•©ë‹ˆë‹¤.
    
    Args:
        query: í™•ì¥ëœ ê²€ìƒ‰ ì¿¼ë¦¬
        web_results: ì›¹ ê²€ìƒ‰ ê²°ê³¼
        
    Returns:
        ì¡°ì •ëœ k ê°’
    """
    base_k = 5
    
    # ì›¹ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë” ë§ì€ ë¡œì»¬ ë¬¸ì„œ ê²€ìƒ‰
    if web_results:
        base_k += 3
    
    # í™•ì¥ëœ ì¿¼ë¦¬ ê¸¸ì´ ê¸°ë°˜ ì¡°ì •
    query_tokens = len(query.split())
    if query_tokens > 10:
        base_k += 2
    elif query_tokens > 5:
        base_k += 1
    
    # ì¿¼ë¦¬ ê¸¸ì´ ê¸°ë°˜ ì¡°ì •
    if len(query) > 30:
        base_k += 2
    elif len(query) > 15:
        base_k += 1
    
    # ìµœëŒ€ 15ê°œë¡œ ì œí•œ
    return min(base_k, 15)

def _enhanced_hybrid_search(
    query: str,
    vector_results: List[Dict[str, Any]],
    keyword_results: List[Dict[str, Any]],
    web_results: List[Dict[str, Any]],
    k: int = 5
) -> List[Dict[str, Any]]:
    """
    ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê³ ë ¤í•œ í–¥ìƒëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        vector_results: ë²¡í„° ê²€ìƒ‰ ê²°ê³¼
        keyword_results: í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼
        web_results: ì›¹ ê²€ìƒ‰ ê²°ê³¼
        k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        
    Returns:
        í†µí•©ëœ ê²€ìƒ‰ ê²°ê³¼
    """
    # ê¸°ë³¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
    merged = hybrid_search(query, vector_results, keyword_results, k=k)
    
    # ì›¹ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë¡œì»¬ ë¬¸ì„œ ê²°ê³¼ì— ì›¹ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€
    if web_results:
        merged = _add_web_context_to_results(merged, web_results)
    
    return merged

def _add_web_context_to_results(
    local_results: List[Dict[str, Any]], 
    web_results: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    ë¡œì»¬ ê²€ìƒ‰ ê²°ê³¼ì— ì›¹ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    
    Args:
        local_results: ë¡œì»¬ ê²€ìƒ‰ ê²°ê³¼
        web_results: ì›¹ ê²€ìƒ‰ ê²°ê³¼
        
    Returns:
        ì›¹ ì»¨í…ìŠ¤íŠ¸ê°€ ì¶”ê°€ëœ ê²€ìƒ‰ ê²°ê³¼
    """
    if not web_results:
        return local_results
    
    # ì›¹ ê²°ê³¼ì—ì„œ ìƒìœ„ 3ê°œë§Œ ì„ íƒ
    top_web_results = web_results[:3]
    
    # ê° ë¡œì»¬ ê²°ê³¼ì— ì›¹ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€
    enhanced_results = []
    for result in local_results:
        enhanced_result = dict(result)
        
        # ì›¹ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€
        enhanced_result["web_context"] = {
            "has_web_info": True,
            "web_sources_count": len(top_web_results),
            "web_relevance_score": _calculate_web_relevance(result, top_web_results)
        }
        
        enhanced_results.append(enhanced_result)
    
    return enhanced_results

def _calculate_web_relevance(
    local_result: Dict[str, Any], 
    web_results: List[Dict[str, Any]]
) -> float:
    """
    ë¡œì»¬ ê²€ìƒ‰ ê²°ê³¼ì™€ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ê°„ì˜ ê´€ë ¨ì„±ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        local_result: ë¡œì»¬ ê²€ìƒ‰ ê²°ê³¼
        web_results: ì›¹ ê²€ìƒ‰ ê²°ê³¼
        
    Returns:
        ê´€ë ¨ì„± ì ìˆ˜ (0.0 ~ 1.0)
    """
    if not web_results:
        return 0.0
    
    local_text = local_result.get("text", "")
    relevance_scores = []
    
    for web_result in web_results:
        web_title = web_result.get("title", "")
        web_snippet = web_result.get("snippet", "")
        web_text = f"{web_title} {web_snippet}"
        
        # ê°œì„ ëœ í‚¤ì›Œë“œ ê´€ë ¨ì„± ê³„ì‚°
        relevance_score = calculate_keyword_relevance(local_text, [web_text])
        relevance_scores.append(relevance_score)
    
    # í‰ê·  ê´€ë ¨ì„± ì ìˆ˜ ë°˜í™˜
    return sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0.0

def _convert_web_results_to_passages(web_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ íŒ¨ì‹œì§€ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        web_results: ì›¹ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        íŒ¨ì‹œì§€ í˜•íƒœë¡œ ë³€í™˜ëœ ì›¹ ê²°ê³¼
    """
    if not web_results:
        return []
    
    passages = []
    for i, result in enumerate(web_results[:3]):  # ìƒìœ„ 3ê°œë§Œ ì‚¬ìš©
        passage = {
            "text": f"{result.get('title', '')} {result.get('snippet', '')}",
            "source": "web",
            "url": result.get("url", ""),
            "title": result.get("title", ""),
            "score_web": result.get("score_web", 0.5),  # ê¸°ë³¸ ì›¹ ì ìˆ˜
            "web_relevance_score": result.get("relevance_score", 0.5),
            "doc_id": f"web_{i}",
            "page": 0,
            "timestamp": result.get("timestamp", "")
        }
        passages.append(passage)
    
    return passages

def _enhanced_hybrid_search_with_web_weight(
    query: str,
    vector_results: List[Dict[str, Any]],
    keyword_results: List[Dict[str, Any]],
    web_passages: List[Dict[str, Any]],
    k: int = 5,
    insurer_filter: Optional[List[str]] = None
) -> List[Dict[str, Any]]:
    """
    ì›¹ ì»¨í…ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ë¥¼ ë°˜ì˜í•œ í–¥ìƒëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        vector_results: ë²¡í„° ê²€ìƒ‰ ê²°ê³¼
        keyword_results: í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼
        web_passages: ì›¹ íŒ¨ì‹œì§€
        k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        insurer_filter: ë³´í—˜ì‚¬ í•„í„° (ì„ íƒì‚¬í•­)
        
    Returns:
        ì›¹ ê°€ì¤‘ì¹˜ê°€ ë°˜ì˜ëœ í†µí•© ê²€ìƒ‰ ê²°ê³¼
    """
    # ê¸°ë³¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ - ë¦¬ë­í¬ë¥¼ ìœ„í•œ ëŒ€ëŸ‰ í›„ë³´
    merged = hybrid_search(query, vector_results, keyword_results, k=k*3, insurer_filter=insurer_filter)  # ë” ë§ì€ í›„ë³´ í™•ë³´
    
    # ì›¹ íŒ¨ì‹œì§€ ì¶”ê°€
    all_results = merged + web_passages
    
    # ì›¹ ì»¨í…ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ ì ìš©
    weighted_results = []
    for result in all_results:
        weighted_result = dict(result)
        
        # ì›¹ ì»¨í…ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ ê³„ì‚°
        web_weight = 0.0
        if "web_relevance_score" in result:
            web_weight = result["web_relevance_score"] * 0.2  # Î»=0.2 ê°€ì¤‘ì¹˜
        
        # ê¸°ë³¸ ì ìˆ˜ì— ì›¹ ê°€ì¤‘ì¹˜ ì ìš©
        base_score = result.get("score", 0.0)
        if base_score > 0:
            final_score = base_score * (1 + web_weight)
        else:
            # ì›¹ ê²°ê³¼ì˜ ê²½ìš° ê¸°ë³¸ ì ìˆ˜ ì‚¬ìš©
            final_score = result.get("score_web", 0.5)
        
        weighted_result["score"] = min(final_score, 1.0)  # 1.0ìœ¼ë¡œ ì œí•œ
        weighted_results.append(weighted_result)
    
    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    weighted_results.sort(key=lambda x: x.get("score", 0.0), reverse=True)
    
    return weighted_results[:k]