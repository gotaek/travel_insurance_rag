from typing import Dict, Any, List
import json
import logging
from app.deps import get_llm
from app.langsmith_llm import get_llm_with_tracing
from graph.models import QualityEvaluationResponse

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ìƒìˆ˜ ì •ì˜
QUALITY_THRESHOLD = 0.7
MAX_REPLAN_ATTEMPTS = 3

def reevaluate_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    LLM ê¸°ë°˜ ë‹µë³€ í’ˆì§ˆ í‰ê°€ ë° ì¬ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨ (ë¬´í•œë£¨í”„ ë°©ì§€ í¬í•¨)
    """
    question = state.get("question", "")
    answer = state.get("draft_answer", {})
    citations = state.get("citations", [])
    passages = state.get("refined", [])
    replan_count = state.get("replan_count", 0)
    max_attempts = state.get("max_replan_attempts", MAX_REPLAN_ATTEMPTS)
    
    # ë‹µë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ - ë‹¤ì–‘í•œ ë‹µë³€ êµ¬ì¡° ì§€ì›
    if isinstance(answer, dict):
        # conclusion í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš© (summarize, qa, compare, recommend ë…¸ë“œ)
        answer_text = answer.get("conclusion", "")
        # conclusionì´ ì—†ìœ¼ë©´ text í•„ë“œ ì‚¬ìš© (ê¸°ì¡´ í˜¸í™˜ì„±)
        if not answer_text:
            answer_text = answer.get("text", "")
        # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ì „ì²´ ë‹µë³€ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        if not answer_text:
            answer_text = str(answer)
    else:
        answer_text = str(answer)
    
    # ì„±ëŠ¥ ìµœì í™”: 3ë²ˆì§¸ ì‚¬ì´í´ë¶€í„°ëŠ” í’ˆì§ˆ í‰ê°€ ì—†ì´ ë°”ë¡œ ë‹µë³€ ì œê³µ
    if replan_count >= 3:
        logger.info(f"ì¬ê²€ìƒ‰ íšŸìˆ˜ê°€ {replan_count}íšŒì— ë„ë‹¬ - í’ˆì§ˆ í‰ê°€ ì—†ì´ ë‹µë³€ ì™„ë£Œ")
        return {
            **state,
            "needs_replan": False,
            "final_answer": answer,
            "quality_feedback": f"ì¬ê²€ìƒ‰ íšŸìˆ˜({replan_count}íšŒ) ì´ˆê³¼ë¡œ ë‹µë³€ì„ ì™„ë£Œí•©ë‹ˆë‹¤.",
            "replan_count": replan_count
        }
    
    # LLMì„ ì‚¬ìš©í•œ í’ˆì§ˆ í‰ê°€ (3ë²ˆì§¸ ì‚¬ì´í´ ì´ì „ì—ë§Œ)
    logger.info(f"ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì‹œì‘ - ì§ˆë¬¸: {question[:50]}... (ì¬ê²€ìƒ‰ íšŸìˆ˜: {replan_count})")
    logger.debug(f"ì¶”ì¶œëœ ë‹µë³€ í…ìŠ¤íŠ¸: {answer_text[:100]}..." if answer_text else "ë‹µë³€ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŒ")
    logger.debug(f"ë‹µë³€ ì›ë³¸ íƒ€ì…: {type(answer)}, ë‚´ìš©: {str(answer)[:100]}...")
    quality_result = _evaluate_answer_quality(question, answer_text, citations, passages)
    
    # ì¬ê²€ìƒ‰ íšŸìˆ˜ ì²´í¬ ë° ë¬´í•œë£¨í”„ ë°©ì§€
    needs_replan = quality_result["needs_replan"] and replan_count < max_attempts
    
    # ë¬´í•œë£¨í”„ ë°©ì§€: ìµœëŒ€ ì‹œë„ íšŸìˆ˜ì— ë„ë‹¬í•˜ë©´ ê°•ì œë¡œ ë‹µë³€ ì™„ë£Œ (3ë²ˆì§¸ ì‚¬ì´í´ ì´í›„)
    if replan_count >= max_attempts:
        needs_replan = False
        logger.warning(f"ğŸš¨ ìµœëŒ€ ì¬ê²€ìƒ‰ íšŸìˆ˜({max_attempts})ì— ë„ë‹¬í•˜ì—¬ ë‹µë³€ì„ ì™„ë£Œí•©ë‹ˆë‹¤.")
        quality_result["score"] = max(quality_result["score"], 0.5)  # ìµœì†Œ 0.5ì  ë³´ì¥
        print(f"ğŸš¨ reevaluateì—ì„œ ê°•ì œ ì™„ë£Œ - replan_count: {replan_count}, max_attempts: {max_attempts}")
    
    # ë‹µë³€ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ê²½ìš° ìµœì†Œ ì ìˆ˜ ë³´ì¥
    if answer_text and answer_text.strip() and quality_result["score"] < 0.3:
        logger.warning(f"ë‹µë³€ì´ ì¡´ì¬í•˜ì§€ë§Œ ë‚®ì€ ì ìˆ˜({quality_result['score']:.2f}) - ìµœì†Œ ì ìˆ˜ 0.3ìœ¼ë¡œ ì¡°ì •")
        quality_result["score"] = 0.3
        if quality_result["score"] >= QUALITY_THRESHOLD:
            needs_replan = False
    
    # ì¶”ê°€ ì•ˆì „ì¥ì¹˜: ì¬ê²€ìƒ‰ íšŸìˆ˜ê°€ 2íšŒ ì´ìƒì´ë©´ ë” ê´€ëŒ€í•˜ê²Œ í‰ê°€ (3ë²ˆì§¸ ì‚¬ì´í´ ì´ì „ì—ë§Œ)
    if replan_count >= 2 and replan_count < 3 and answer_text and answer_text.strip():
        logger.warning(f"ì¬ê²€ìƒ‰ íšŸìˆ˜ê°€ {replan_count}íšŒë¡œ ë†’ìŒ - ë” ê´€ëŒ€í•˜ê²Œ í‰ê°€")
        quality_result["score"] = max(quality_result["score"], 0.6)
        if quality_result["score"] >= QUALITY_THRESHOLD:
            needs_replan = False
    
    logger.info(f"í’ˆì§ˆ ì ìˆ˜: {quality_result['score']:.2f}, ì¬ê²€ìƒ‰ í•„ìš”: {needs_replan}, ì¬ê²€ìƒ‰ íšŸìˆ˜: {replan_count}/{max_attempts}")
    
    return {
        **state,
        "quality_score": quality_result["score"],
        "quality_feedback": quality_result["feedback"],
        "needs_replan": needs_replan,
        "replan_query": quality_result["replan_query"],
        "final_answer": answer if quality_result["score"] >= QUALITY_THRESHOLD or replan_count >= max_attempts else None
    }

def _evaluate_answer_quality(question: str, answer: str, citations: List[Dict[str, Any]], passages: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•˜ê³  ì¬ê²€ìƒ‰ í•„ìš”ì„±ì„ íŒë‹¨
    """
    prompt = f"""
ë‹¤ìŒì€ ì—¬í–‰ìë³´í—˜ RAG ì‹œìŠ¤í…œì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì…ë‹ˆë‹¤. ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ê³  ì¬ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: "{question}"

ë‹µë³€: "{answer}"

ì¸ìš© ì •ë³´: {len(citations)}ê°œ
ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(passages)}ê°œ

**ì¤‘ìš”**: ë‹µë³€ì´ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°, ë‚´ìš©ì„ ì¶©ë¶„íˆ ê³ ë ¤í•˜ì—¬ í‰ê°€í•´ì£¼ì„¸ìš”. 
ë‹µë³€ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ê³  ì§ˆë¬¸ì— ê´€ë ¨ëœ ë‚´ìš©ì´ ìˆë‹¤ë©´ 0ì ì„ ì£¼ì§€ ë§ˆì„¸ìš”.

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:

1. **ì •í™•ì„± (0-1)**: ë‹µë³€ì´ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µí•˜ê³  ìˆëŠ”ê°€?
2. **ì™„ì „ì„± (0-1)**: ë‹µë³€ì´ ì¶©ë¶„íˆ ìƒì„¸í•˜ê³  ì™„ì „í•œê°€?
3. **ê´€ë ¨ì„± (0-1)**: ë‹µë³€ì´ ì—¬í–‰ìë³´í—˜ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì¸ê°€?
4. **ì¸ìš© í’ˆì§ˆ (0-1)**: ì ì ˆí•œ ì¸ìš©ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?

ì´ ì ìˆ˜ëŠ” 0-1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ, 0.7 ì´ìƒì´ë©´ ì–‘í˜¸í•œ ë‹µë³€ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.

**ì¬ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°**:
- ë‹µë³€ì´ ì‹¤ì œë¡œ ë¹„ì–´ìˆê±°ë‚˜ ì˜ë¯¸ê°€ ì—†ëŠ” ê²½ìš°
- ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì „í˜€ ê´€ë ¨ì´ ì—†ëŠ” ê²½ìš°
- ë‹µë³€ì´ ë¶ˆì™„ì „í•˜ê±°ë‚˜ ë¶€ì •í™•í•œ ê²½ìš°
- ë” ìµœì‹  ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš°

ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
- score: 0.0-1.0 ì‚¬ì´ì˜ í’ˆì§ˆ ì ìˆ˜
- feedback: í’ˆì§ˆ í‰ê°€ ìƒì„¸ ì„¤ëª…
- needs_replan: ì¬ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ (true/false)
- replan_query: ì¬ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš° ìƒˆë¡œìš´ ê²€ìƒ‰ ì§ˆë¬¸ (ì—†ìœ¼ë©´ null)
"""

    try:
        logger.debug("LLMì„ ì‚¬ìš©í•œ í’ˆì§ˆ í‰ê°€ ì‹œì‘ (structured output)")
        llm = get_llm_with_tracing()
        
        # structured output ì‚¬ìš©
        structured_llm = llm.with_structured_output(QualityEvaluationResponse)
        response = structured_llm.generate_content(prompt, request_options={"timeout": 10})
        
        logger.debug(f"Structured LLM ì‘ë‹µ: {response}")
        
        # ìœ íš¨ì„± ê²€ì¦
        score = response.score
        if not isinstance(score, (int, float)) or score < 0 or score > 1:
            logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì ìˆ˜: {score}, ê¸°ë³¸ê°’ 0.5 ì‚¬ìš©")
            score = 0.5
            
        needs_replan = response.needs_replan
        if not isinstance(needs_replan, bool):
            needs_replan = score < QUALITY_THRESHOLD
            logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ needs_replan: {needs_replan}, ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì„¤ì •")
            
        replan_query = response.replan_query
        if replan_query == "null" or replan_query is None:
            replan_query = ""
            
        return {
            "score": float(score),
            "feedback": response.feedback,
            "needs_replan": needs_replan,
            "replan_query": replan_query
        }
        
    except Exception as e:
        logger.error(f"LLM í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨, fallback ì‚¬ìš©: {str(e)}")
        return _fallback_evaluate(question, answer, citations, passages)

def _fallback_evaluate(question: str, answer: str, citations: List[Dict[str, Any]], passages: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•˜ëŠ” ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± í‰ê°€
    """
    # ë‹µë³€ì´ ì‹¤ì œë¡œ ë¹„ì–´ìˆëŠ”ì§€ ì²´í¬
    if not answer or answer.strip() == "":
        logger.warning("ë‹µë³€ì´ ë¹„ì–´ìˆìŒ - Fallback í‰ê°€ì—ì„œ 0ì  ì²˜ë¦¬")
        return {
            "score": 0.0,
            "feedback": "ë‹µë³€ì´ ë¹„ì–´ìˆì–´ ì •í™•ì„±, ì™„ì „ì„±, ê´€ë ¨ì„± ëª¨ë‘ 0ì ì…ë‹ˆë‹¤.",
            "needs_replan": True,
            "replan_query": question
        }
    
    # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚° (ë‹µë³€ì´ ìˆìœ¼ë©´ ìµœì†Œ 0.3ì )
    score = 0.3
    
    # ë‹µë³€ ê¸¸ì´ ì²´í¬
    if len(answer) > 50:
        score += 0.2
    elif len(answer) > 20:
        score += 0.1
    
    # ì¸ìš© ì •ë³´ ì²´í¬
    if len(citations) > 0:
        score += 0.2
    
    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì²´í¬
    if len(passages) > 0:
        score += 0.1
    
    # í‚¤ì›Œë“œ ë§¤ì¹­ ì²´í¬
    question_words = set(question.lower().split())
    answer_words = set(answer.lower().split())
    if len(question_words.intersection(answer_words)) > 0:
        score += 0.2
    
    # ì ìˆ˜ ì œí•œ
    score = min(score, 1.0)
    
    needs_replan = score < QUALITY_THRESHOLD
    replan_query = question if needs_replan else ""
    
    logger.info(f"Fallback í‰ê°€ ì™„ë£Œ - ì ìˆ˜: {score:.2f}, ì¬ê²€ìƒ‰ í•„ìš”: {needs_replan}")
    
    return {
        "score": score,
        "feedback": f"Fallback í‰ê°€: ë‹µë³€ê¸¸ì´({len(answer)}), ì¸ìš©({len(citations)}), ë¬¸ì„œ({len(passages)})",
        "needs_replan": needs_replan,
        "replan_query": replan_query
    }
