from typing import Dict, Any
import json
import logging
import time
from app.deps import get_answerer_llm
from graph.models import RecommendResponse, EvidenceInfo, CaveatInfo
from graph.prompts.utils import get_simple_fallback_response
from graph.cache_manager import cache_manager
from .common import (
    get_system_prompt, get_prompt_cached, format_context_optimized,
    process_verify_refine_data, create_optimized_prompt, 
    handle_llm_error_optimized, log_performance
)

logger = logging.getLogger(__name__)

def _format_web_results(web_results: list) -> str:
    """ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬ë§·íŒ…"""
    if not web_results:
        return "ì‹¤ì‹œê°„ ë‰´ìŠ¤ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    web_parts = []
    for i, result in enumerate(web_results[:3], 1):  # ìƒìœ„ 3ê°œë§Œ ì‚¬ìš©
        title = result.get("title", "ì œëª© ì—†ìŒ")
        snippet = result.get("snippet", "")[:200]  # 200ìë¡œ ì œí•œ
        web_parts.append(f"[ë‰´ìŠ¤ {i}] {title}\n{snippet}\n")
    
    return "\n".join(web_parts)

def _parse_llm_response_fallback(llm, prompt: str, question: str) -> Dict[str, Any]:
    """structured output ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ fallback"""
    try:
        logger.debug("Recommend ë…¸ë“œ fallback íŒŒì‹± ì‹œë„")
        response = llm.generate_content(prompt)
        response_text = response.text
        
        return {
            "conclusion": response_text[:500] if response_text else "ì¶”ì²œì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.",
            "evidence": [EvidenceInfo(text="Fallback íŒŒì‹±ìœ¼ë¡œ ìƒì„±ëœ ë‹µë³€", source="Fallback ì‹œìŠ¤í…œ")],
            "caveats": [CaveatInfo(text="ì›ë³¸ structured outputì´ ì‹¤íŒ¨í•˜ì—¬ ì¼ë°˜ íŒŒì‹±ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.", source="Fallback ì‹œìŠ¤í…œ")],
            "web_quotes": [],
            "recommendations": [],
            "web_info": {}
        }
        
    except Exception as fallback_error:
        logger.error(f"Recommend ë…¸ë“œ fallbackë„ ì‹¤íŒ¨: {str(fallback_error)}")
        return get_simple_fallback_response(question, "Recommend")

def _parse_llm_response_structured(llm, prompt: str, emergency_fallback: bool = False) -> Dict[str, Any]:
    """LLM ì‘ë‹µì„ structured outputìœ¼ë¡œ íŒŒì‹±"""
    try:
        # structured output ì‚¬ìš© (ê¸´ê¸‰ íƒˆì¶œ ëª¨ë“œ ì§€ì›)
        structured_llm = llm.with_structured_output(RecommendResponse, emergency_fallback=emergency_fallback)
        response = structured_llm.generate_content(prompt)
        
        return {
            "conclusion": response.conclusion,
            "evidence": response.evidence,
            "caveats": response.caveats,
            "web_quotes": response.web_quotes,
            "recommendations": response.recommendations,
            "web_info": response.web_info
        }
    except Exception as e:
        # structured output ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡°ë¡œ fallback
        error_str = str(e).lower()
        logger.error(f"Recommend ë…¸ë“œ structured output ì‹¤íŒ¨: {str(e)}")
        
        if "quota" in error_str or "limit" in error_str or "429" in error_str or "exceeded" in error_str:
            return {
                "conclusion": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "evidence": [EvidenceInfo(text="Gemini API ì¼ì¼ í• ë‹¹ëŸ‰ ì´ˆê³¼", source="API ì‹œìŠ¤í…œ")],
                "caveats": [
                    CaveatInfo(text="ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", source="API ì‹œìŠ¤í…œ"),
                    CaveatInfo(text="API í• ë‹¹ëŸ‰ì´ ë³µêµ¬ë˜ë©´ ì •ìƒì ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", source="API ì‹œìŠ¤í…œ"),
                    CaveatInfo(text="ì˜¤ë¥˜ ì½”ë“œ: 429 (Quota Exceeded)", source="API ì‹œìŠ¤í…œ")
                ],
                "recommendations": [],
                "web_info": {}
            }
        elif "404" in error_str or "publisher" in error_str or "model" in error_str:
            return {
                "conclusion": "ëª¨ë¸ ì„¤ì • ì˜¤ë¥˜ë¡œ ì¸í•´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "evidence": [EvidenceInfo(text="Gemini ëª¨ë¸ ì„¤ì • ì˜¤ë¥˜", source="API ì‹œìŠ¤í…œ")],
                "caveats": [
                    CaveatInfo(text="ëª¨ë¸ ì´ë¦„ì„ í™•ì¸í•´ì£¼ì„¸ìš”.", source="API ì‹œìŠ¤í…œ"),
                    CaveatInfo(text="ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", source="API ì‹œìŠ¤í…œ")
                ],
                "recommendations": [],
                "web_info": {}
            }
        else:
            # structured output ì‹¤íŒ¨ ì‹œ fallback íŒŒì‹± ì‹œë„
            return _parse_llm_response_fallback(llm, prompt, "ì§ˆë¬¸")

def recommend_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    ì¶”ì²œ ì—ì´ì „íŠ¸: ì—¬í–‰ ì¼ì •, ì§€ì—­, ìµœì‹  ë‰´ìŠ¤ë¥¼ ê³ ë ¤í•˜ì—¬ ë§ì¶¤ íŠ¹ì•½ ì¶”ì²œ (ìµœì í™”ëœ ë²„ì „)
    """
    start_time = time.time()
    question = state.get("question", "")
    refined = state.get("refined", [])
    web_results = state.get("web_results", [])
    
    # ì„±ëŠ¥ ë¡œê¹…
    log_performance("Recommend ì‹œì‘", start_time, 
                   question_length=len(question), refined_count=len(refined), 
                   web_results_count=len(web_results))
    
    # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… (ìµœì í™”ëœ í•¨ìˆ˜ ì‚¬ìš©)
    context = format_context_optimized(refined)
    web_info = _format_web_results(web_results)
    
    # ìºì‹œëœ í”„ë¡¬í”„íŠ¸ ë¡œë“œ (ëª¨ë“ˆ ë ˆë²¨ ìºì‹œ ì‚¬ìš©)
    system_prompt = get_system_prompt()
    recommend_prompt = get_prompt_cached("recommend")
    
    # ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
    full_prompt = f"""{system_prompt}

{recommend_prompt}

## ì§ˆë¬¸
{question}

## ì°¸ê³  ë¬¸ì„œ
{context}

## ì‹¤ì‹œê°„ ë‰´ìŠ¤/ì •ë³´
{web_info}

ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë§ì¶¤ ì¶”ì²œì„ í•´ì£¼ì„¸ìš”."""
    
    try:
        # LLM ì‘ë‹µ ìºì‹œ í™•ì¸
        prompt_hash = cache_manager.generate_prompt_hash(full_prompt)
        cached_response = cache_manager.get_cached_llm_response(prompt_hash)
        if cached_response:
            logger.info("ğŸ” [Recommend] LLM ì‘ë‹µ ìºì‹œ íˆíŠ¸!")
            answer = cached_response
        else:
            # Answerer ì „ìš© LLM ì‚¬ìš© (Gemini 2.5 Flash)
            llm = get_answerer_llm()
            
            # ê°„ì†Œí™”ëœ structured output ì‚¬ìš©
            try:
                answer = _parse_llm_response_structured(llm, full_prompt, emergency_fallback=False)
                
                # LLM ì‘ë‹µ ìºì‹±
                cache_manager.cache_llm_response(prompt_hash, answer)
                logger.info("ğŸ” [Recommend] LLM ì‘ë‹µ ìºì‹œ ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"Structured output ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")
                answer = get_simple_fallback_response(question, "Recommend")
        
        # verify_refine ë°ì´í„° ì²˜ë¦¬ (ìµœì í™”ëœ í•¨ìˆ˜ ì‚¬ìš©)
        answer = process_verify_refine_data(state, answer)
        
        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ web_quotesì— ì¶”ê°€ (ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ)
        if web_results and not answer.get("web_quotes"):
            answer["web_quotes"] = [
                {
                    "text": result.get("snippet", "")[:200] + "...",
                    "source": f"ì›¹ê²€ìƒ‰_{result.get('title', 'ì œëª© ì—†ìŒ')}_{result.get('url', 'URL ì—†ìŒ')}"
                }
                for result in web_results[:3]  # ìƒìœ„ 3ê°œë§Œ
            ]
        
        # web_info í•„ë“œ ì²˜ë¦¬
        if isinstance(answer.get("web_info"), dict):
            web_info_dict = answer["web_info"]
            answer["web_info"] = {
                "latest_news": web_info_dict.get("latest_news", ""),
                "travel_alerts": web_info_dict.get("travel_alerts", "")
            }
        elif not answer.get("web_info"):
            answer["web_info"] = {
                "latest_news": "",
                "travel_alerts": ""
            }
        
        # ì„±ëŠ¥ ë¡œê¹…
        log_performance("Recommend ì™„ë£Œ", start_time, 
                       conclusion_length=len(answer.get('conclusion', '')))
        
        return {
            **state, 
            "draft_answer": answer, 
            "final_answer": answer
        }
        
    except Exception as e:
        # ìµœì í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬
        logger.error(f"Recommend LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
        fallback_answer = handle_llm_error_optimized(e, question, "Recommend")
        return {**state, "draft_answer": fallback_answer, "final_answer": fallback_answer}