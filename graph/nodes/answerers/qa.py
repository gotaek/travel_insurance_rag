from typing import Dict, Any
import json
from pathlib import Path
from app.deps import get_llm
from graph.models import AnswerResponse

def _load_prompt(prompt_name: str) -> str:
    """í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ"""
    # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
    current_dir = Path(__file__).parent.parent.parent
    prompt_path = current_dir / "prompts" / f"{prompt_name}.md"
    return prompt_path.read_text(encoding="utf-8")

def _format_context(passages: list) -> str:
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
    if not passages or passages is None:
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    context_parts = []
    for i, passage in enumerate(passages[:5], 1):  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
        doc_id = passage.get("doc_id", "ì•Œ ìˆ˜ ì—†ìŒ")
        page = passage.get("page", "ì•Œ ìˆ˜ ì—†ìŒ")
        text = passage.get("text", "")[:500]  # 500ìë¡œ ì œí•œ
        context_parts.append(f"[ë¬¸ì„œ {i}] {doc_id} (í˜ì´ì§€ {page})\n{text}\n")
    
    return "\n".join(context_parts)

def _parse_llm_response_fallback(llm, prompt: str) -> Dict[str, Any]:
    """structured output ì‹¤íŒ¨ ì‹œ ì¼ë°˜ LLM í˜¸ì¶œë¡œ fallback"""
    try:
        print("ğŸ”„ QA ë…¸ë“œ fallback íŒŒì‹± ì‹œë„...")
        response = llm.generate_content(prompt, request_options={"timeout": 45})
        response_text = response.text
        
        # JSON ë¶€ë¶„ ì¶”ì¶œ ì‹œë„
        import json
        import re
        
        # JSON íŒ¨í„´ ì°¾ê¸°
        json_pattern = r'\{.*\}'
        json_match = re.search(json_pattern, response_text, re.DOTALL)
        
        if json_match:
            json_str = json_match.group()
            try:
                parsed = json.loads(json_str)
                return {
                    "conclusion": parsed.get("conclusion", "ë‹µë³€ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤."),
                    "evidence": parsed.get("evidence", []),
                    "caveats": parsed.get("caveats", []),
                    "quotes": parsed.get("quotes", [])
                }
            except json.JSONDecodeError:
                pass
        
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
        return {
            "conclusion": response_text[:500] if response_text else "ë‹µë³€ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.",
            "evidence": ["Fallback íŒŒì‹±ìœ¼ë¡œ ìƒì„±ëœ ë‹µë³€"],
            "caveats": ["ì›ë³¸ structured outputì´ ì‹¤íŒ¨í•˜ì—¬ ì¼ë°˜ íŒŒì‹±ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."],
            "quotes": []
        }
        
    except Exception as fallback_error:
        print(f"âŒ QA ë…¸ë“œ fallbackë„ ì‹¤íŒ¨: {str(fallback_error)}")
        return {
            "conclusion": "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "evidence": [f"Fallback íŒŒì‹±ë„ ì‹¤íŒ¨: {str(fallback_error)[:100]}"],
            "caveats": [f"ìƒì„¸ ì˜¤ë¥˜: {str(fallback_error)}", "ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."],
            "quotes": []
        }

def _parse_llm_response_structured(llm, prompt: str) -> Dict[str, Any]:
    """LLM ì‘ë‹µì„ structured outputìœ¼ë¡œ íŒŒì‹±"""
    try:
        # structured output ì‚¬ìš©
        structured_llm = llm.with_structured_output(AnswerResponse)
        response = structured_llm.generate_content(prompt, request_options={"timeout": 45})
        
        return {
            "conclusion": response.conclusion,
            "evidence": response.evidence,
            "caveats": response.caveats,
            "quotes": response.quotes
        }
    except Exception as e:
        # structured output ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡°ë¡œ fallback
        error_str = str(e).lower()
        print(f"âŒ QA ë…¸ë“œ structured output ì‹¤íŒ¨: {str(e)}")
        
        if "quota" in error_str or "limit" in error_str or "429" in error_str or "exceeded" in error_str:
            return {
                "conclusion": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "evidence": ["Gemini API ì¼ì¼ í• ë‹¹ëŸ‰ ì´ˆê³¼"],
                "caveats": [
                    "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    "API í• ë‹¹ëŸ‰ì´ ë³µêµ¬ë˜ë©´ ì •ìƒì ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "ì˜¤ë¥˜ ì½”ë“œ: 429 (Quota Exceeded)"
                ],
                "quotes": []
            }
        else:
            # structured output ì‹¤íŒ¨ ì‹œ fallback íŒŒì‹± ì‹œë„
            return _parse_llm_response_fallback(llm, prompt)

def qa_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    QA ì—ì´ì „íŠ¸: ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€ ìƒì„±
    """
    question = state.get("question", "")
    passages = state.get("passages", [])
    
    # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    context = _format_context(passages)
    
    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    system_prompt = _load_prompt("system_core")
    qa_prompt = _load_prompt("qa")
    
    # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    full_prompt = f"""
        {system_prompt}

        {qa_prompt}

        ## ì§ˆë¬¸
        {question}

        ## ì°¸ê³  ë¬¸ì„œ
        {context}

        ìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
    """
    
    try:
        # LLM í˜¸ì¶œ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
        llm = get_llm()
        
        # structured output ì‚¬ìš©
        answer = _parse_llm_response_structured(llm, full_prompt)
        
        # ì¶œì²˜ ì •ë³´ ì¶”ê°€ (quotesê°€ ë¹„ì–´ìˆì„ ë•Œë§Œ)
        if passages and not answer.get("quotes"):
            answer["quotes"] = [
                {
                    "text": p.get("text", "")[:200] + "...",
                    "source": f"{p.get('doc_id', 'ì•Œ ìˆ˜ ì—†ìŒ')}_í˜ì´ì§€{p.get('page', '?')}"
                }
                for p in passages[:3]  # ìƒìœ„ 3ê°œë§Œ
            ]
        
        return {**state, "draft_answer": answer, "final_answer": answer}
        
    except Exception as e:
        # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ fallback
        error_str = str(e).lower()
        if "quota" in error_str or "limit" in error_str or "429" in error_str:
            fallback_answer = {
                "conclusion": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "evidence": ["Gemini API ì¼ì¼ í• ë‹¹ëŸ‰ ì´ˆê³¼"],
                "caveats": [
                    "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    "API í• ë‹¹ëŸ‰ì´ ë³µêµ¬ë˜ë©´ ì •ìƒì ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                ],
                "quotes": []
            }
        else:
            fallback_answer = {
                "conclusion": f"ì§ˆë¬¸ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤: '{question}'",
                "evidence": ["LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."],
                "caveats": ["ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.", f"ì˜¤ë¥˜: {str(e)}"],
                "quotes": []
            }
        return {**state, "draft_answer": fallback_answer, "final_answer": fallback_answer}