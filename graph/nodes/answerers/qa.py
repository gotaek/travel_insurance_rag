from typing import Dict, Any
import json
import logging
import time
from app.deps import get_answerer_llm
from graph.models import AnswerResponse, EvidenceInfo, CaveatInfo
from graph.prompts.utils import get_simple_fallback_response
from graph.cache_manager import cache_manager
from .common import (
    get_system_prompt, get_prompt_cached, format_context_optimized,
    process_verify_refine_data, create_optimized_prompt, 
    handle_llm_error_optimized, log_performance
)

logger = logging.getLogger(__name__)

def _parse_llm_response_fallback(llm, prompt: str, question: str) -> Dict[str, Any]:
    """structured output ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ fallback"""
    try:
        logger.debug("QA ë…¸ë“œ fallback íŒŒì‹± ì‹œë„")
        response = llm.generate_content(prompt)
        response_text = response.text
        
        return {
            "conclusion": response_text[:500] if response_text else "ë‹µë³€ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.",
            "evidence": [EvidenceInfo(text="Fallback íŒŒì‹±ìœ¼ë¡œ ìƒì„±ëœ ë‹µë³€", source="Fallback ì‹œìŠ¤í…œ")],
            "caveats": [CaveatInfo(text="ì›ë³¸ structured outputì´ ì‹¤íŒ¨í•˜ì—¬ ì¼ë°˜ íŒŒì‹±ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.", source="Fallback ì‹œìŠ¤í…œ")],
            "web_quotes": [],
            "web_info": {}
        }
        
    except Exception as fallback_error:
        logger.error(f"QA ë…¸ë“œ fallbackë„ ì‹¤íŒ¨: {str(fallback_error)}")
        return get_simple_fallback_response(question, "QA")

def _parse_llm_response_structured(llm, prompt: str, emergency_fallback: bool = False) -> Dict[str, Any]:
    """LLM ì‘ë‹µì„ structured outputìœ¼ë¡œ íŒŒì‹±"""
    try:
        # structured output ì‚¬ìš© (ê¸´ê¸‰ íƒˆì¶œ ëª¨ë“œ ì§€ì›)
        structured_llm = llm.with_structured_output(AnswerResponse, emergency_fallback=emergency_fallback)
        response = structured_llm.generate_content(prompt)
        
        return {
            "conclusion": response.conclusion,
            "evidence": response.evidence,
            "caveats": response.caveats,
            "web_quotes": [],
            "web_info": {}
        }
    except Exception as e:
        # structured output ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡°ë¡œ fallback
        error_str = str(e).lower()
        logger.error(f"QA ë…¸ë“œ structured output ì‹¤íŒ¨: {str(e)}")
        
        if "quota" in error_str or "limit" in error_str or "429" in error_str or "exceeded" in error_str:
            return {
                "conclusion": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "evidence": [EvidenceInfo(text="Gemini API ì¼ì¼ í• ë‹¹ëŸ‰ ì´ˆê³¼", source="API ì‹œìŠ¤í…œ")],
                "caveats": [
                    CaveatInfo(text="ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", source="API ì‹œìŠ¤í…œ"),
                    CaveatInfo(text="API í• ë‹¹ëŸ‰ì´ ë³µêµ¬ë˜ë©´ ì •ìƒì ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", source="API ì‹œìŠ¤í…œ"),
                    CaveatInfo(text="ì˜¤ë¥˜ ì½”ë“œ: 429 (Quota Exceeded)", source="API ì‹œìŠ¤í…œ")
                ],
            }
        else:
            # structured output ì‹¤íŒ¨ ì‹œ fallback íŒŒì‹± ì‹œë„
            return _parse_llm_response_fallback(llm, prompt, "ì§ˆë¬¸")

def _format_web_results(web_results: list) -> str:
    """ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬ë§·íŒ…"""
    if not web_results:
        return "ì‹¤ì‹œê°„ ë‰´ìŠ¤ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    web_parts = []
    for i, result in enumerate(web_results[:3], 1):  # ìƒìœ„ 3ê°œë§Œ ì‚¬ìš©
        title = result.get("title", "ì œëª© ì—†ìŒ")
        snippet = result.get("snippet", "")[:200]  # 200ìë¡œ ì œí•œ
        web_parts.append(f"[ë‰´ìŠ¤ {i}] {title}\n{snippet}\n")
    
    return "\n".join(web_parts)

def _generate_simple_llm_response(question: str, start_time: float) -> Dict[str, Any]:
    """
    ë¹„ë„ë©”ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ê°„ë‹¨í•œ LLM ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    ê·¼ê±°ë‚˜ ì£¼ì˜ì‚¬í•­ ì—†ì´ ì¼ë°˜ì ì¸ ë‹µë³€ë§Œ ì œê³µí•©ë‹ˆë‹¤.
    """
    try:
        # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        simple_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”. ëª¨ë¥´ëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ì†”ì§í•˜ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”."""

        # Answerer ì „ìš© LLM ì‚¬ìš©
        llm = get_answerer_llm()
        logger.info(f"ğŸ” [QA] ê°„ë‹¨í•œ LLM ì‘ë‹µ ìƒì„± - í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(simple_prompt)}ì")
        
        # ì¼ë°˜ LLM ì‘ë‹µ ìƒì„± (structured output ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        response = llm.generate_content(simple_prompt)
        response_text = response.text if response.text else "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ê°„ë‹¨í•œ ë‹µë³€ êµ¬ì¡° ìƒì„± (ê·¼ê±°, ì£¼ì˜ì‚¬í•­ ì—†ìŒ)
        simple_answer = {
            "conclusion": response_text,
            "evidence": [],  # ê·¼ê±° ì—†ìŒ
            "caveats": [],   # ì£¼ì˜ì‚¬í•­ ì—†ìŒ
            "web_quotes": [],
            "web_info": {}
        }
        
        # ì„±ëŠ¥ ë¡œê¹…
        log_performance("ê°„ë‹¨í•œ QA ì™„ë£Œ", start_time, 
                       conclusion_length=len(response_text))
        
        logger.info(f"ğŸ” [QA] ê°„ë‹¨í•œ LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ - ë‹µë³€ ê¸¸ì´: {len(response_text)}ì")
        
        return {
            "draft_answer": simple_answer,
            "final_answer": simple_answer
        }
        
    except Exception as e:
        logger.error(f"ê°„ë‹¨í•œ LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ fallback ì‘ë‹µ
        fallback_answer = {
            "conclusion": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "evidence": [],
            "caveats": [],
            "web_quotes": [],
            "web_info": {}
        }
        
        return {
            "draft_answer": fallback_answer,
            "final_answer": fallback_answer
        }

def qa_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    QA ì—ì´ì „íŠ¸: ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€ ìƒì„± (ìµœì í™”ëœ ë²„ì „)
    ë¹„ë„ë©”ì¸ ì§ˆë¬¸ì˜ ê²½ìš° ê°„ë‹¨í•œ LLM ì‘ë‹µ ì œê³µ
    """
    start_time = time.time()
    question = state.get("question", "")
    refined = state.get("refined", [])
    web_results = state.get("web_results", [])
    is_domain_related = state.get("is_domain_related", True)  # ê¸°ë³¸ê°’ì€ True (ê¸°ì¡´ ë™ì‘ ìœ ì§€)
    
    # ì„±ëŠ¥ ë¡œê¹…
    log_performance("QA ì‹œì‘", start_time, 
                   question_length=len(question), refined_count=len(refined),
                   web_results_count=len(web_results))
    
    # ë¹„ë„ë©”ì¸ ì§ˆë¬¸ì¸ ê²½ìš° ê°„ë‹¨í•œ LLM ì‘ë‹µ ìƒì„±
    if not is_domain_related:
        logger.info(f"ğŸ” [QA] ë¹„ë„ë©”ì¸ ì§ˆë¬¸ - ê°„ë‹¨í•œ LLM ì‘ë‹µ ìƒì„±: '{question}'")
        return _generate_simple_llm_response(question, start_time)
    
    # ë„ë©”ì¸ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš° ê¸°ì¡´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    logger.info(f"ğŸ” [QA] ë„ë©”ì¸ ê´€ë ¨ ì§ˆë¬¸ - RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: '{question}'")
    
    # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… (ìµœì í™”ëœ í•¨ìˆ˜ ì‚¬ìš©)
    context = format_context_optimized(refined)
    web_info = _format_web_results(web_results)
    
    # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
    logger.info(f"ğŸ” [QA] ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)}ì, ì›¹ ì •ë³´ ê¸¸ì´: {len(web_info)}ì")
    logger.info(f"ğŸ” [QA] refined ë¬¸ì„œ ìˆ˜: {len(refined)}")
    
    # ìºì‹œëœ í”„ë¡¬í”„íŠ¸ ë¡œë“œ (ëª¨ë“ˆ ë ˆë²¨ ìºì‹œ ì‚¬ìš©)
    system_prompt = get_system_prompt()
    qa_prompt = get_prompt_cached("qa")
    
    logger.info(f"ğŸ” [QA] ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(system_prompt)}ì, QA í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(qa_prompt)}ì")
    
    # ì›¹ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    full_prompt = f"""{system_prompt}

{qa_prompt}

## ì§ˆë¬¸
{question}

## ì°¸ê³  ë¬¸ì„œ
{context}

## ì‹¤ì‹œê°„ ë‰´ìŠ¤/ì •ë³´
{web_info}

ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."""
    
    try:
        # LLM ì‘ë‹µ ìºì‹œ í™•ì¸
        prompt_hash = cache_manager.generate_prompt_hash(full_prompt)
        cached_response = cache_manager.get_cached_llm_response(prompt_hash)
        if cached_response:
            logger.info("ğŸ” [QA] LLM ì‘ë‹µ ìºì‹œ íˆíŠ¸!")
            answer = cached_response
        else:
            # Answerer ì „ìš© LLM ì‚¬ìš© (Gemini 2.5 Flash)
            llm = get_answerer_llm()
            logger.info(f"ğŸ” [QA] LLM ì´ˆê¸°í™” ì™„ë£Œ, í”„ë¡¬í”„íŠ¸ ì´ ê¸¸ì´: {len(full_prompt)}ì")
            
            # ê°„ì†Œí™”ëœ structured output ì‚¬ìš©
            try:
                logger.info("ğŸ” [QA] Structured output ì‹œë„ ì¤‘...")
                answer = _parse_llm_response_structured(llm, full_prompt, emergency_fallback=False)
                logger.info(f"ğŸ” [QA] Structured output ì„±ê³µ - ë‹µë³€ ê¸¸ì´: {len(answer.get('conclusion', ''))}ì")
                
                # LLM ì‘ë‹µ ìºì‹±
                cache_manager.cache_llm_response(prompt_hash, answer)
                logger.info("ğŸ” [QA] LLM ì‘ë‹µ ìºì‹œ ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"ğŸ” [QA] Structured output ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")
                answer = get_simple_fallback_response(question, "QA")
                logger.info(f"ğŸ” [QA] Fallback ë‹µë³€ ìƒì„± ì™„ë£Œ - ë‹µë³€ ê¸¸ì´: {len(answer.get('conclusion', ''))}ì")
        
        # verify_refine ë°ì´í„° ì²˜ë¦¬ (ìµœì í™”ëœ í•¨ìˆ˜ ì‚¬ìš©)
        answer = process_verify_refine_data(state, answer)
        
        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ web_quotesì— ì¶”ê°€ (ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ)
        if web_results and not answer.get("web_quotes"):
            answer["web_quotes"] = [
                {
                    "text": result.get("snippet", "")[:200] + "...",
                    "source": f"ì›¹ê²€ìƒ‰_{result.get('title', 'ì œëª© ì—†ìŒ')}_{result.get('url', 'URL ì—†ìŒ')}"
                }
                for result in web_results[:3]  # ìƒìœ„ 3ê°œë§Œ
            ]
        
        # web_info í•„ë“œ ì²˜ë¦¬
        if isinstance(answer.get("web_info"), dict):
            web_info_dict = answer["web_info"]
            answer["web_info"] = {
                "latest_news": web_info_dict.get("latest_news", ""),
                "travel_alerts": web_info_dict.get("travel_alerts", "")
            }
        elif not answer.get("web_info"):
            answer["web_info"] = {
                "latest_news": "",
                "travel_alerts": ""
            }
        
        # ì„±ëŠ¥ ë¡œê¹…
        log_performance("QA ì™„ë£Œ", start_time, 
                       conclusion_length=len(answer.get('conclusion', '')))
        
        return {
            **state, 
            "draft_answer": answer, 
            "final_answer": answer
        }
        
    except Exception as e:
        # ìµœì í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬
        logger.error(f"QA LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
        fallback_answer = handle_llm_error_optimized(e, question, "QA")
        return {**state, "draft_answer": fallback_answer, "final_answer": fallback_answer}