from typing import Dict, Any
import json
import logging
import time
from app.deps import get_answerer_llm
from graph.models import AnswerResponse, EvidenceInfo, CaveatInfo
from graph.prompts.utils import get_simple_fallback_response
from graph.cache_manager import cache_manager
from .common import (
    get_system_prompt, get_prompt_cached, format_context_optimized,
    process_verify_refine_data, create_optimized_prompt, 
    handle_llm_error_optimized, log_performance
)

logger = logging.getLogger(__name__)

def _parse_llm_response_structured(llm, prompt: str, emergency_fallback: bool = False) -> Dict[str, Any]:
    """LLM ì‘ë‹µì„ structured outputìœ¼ë¡œ íŒŒì‹±"""
    try:
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"structured output ì‹œì‘ - í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")
            logger.debug(f"ê¸´ê¸‰ íƒˆì¶œ ëª¨ë“œ: {emergency_fallback}")
        
        # structured output ì‚¬ìš© (ê¸´ê¸‰ íƒˆì¶œ ëª¨ë“œ ì§€ì›)
        structured_llm = llm.with_structured_output(AnswerResponse, emergency_fallback=emergency_fallback)
        response = structured_llm.generate_content(prompt)
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug("LLM ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
        
        # ì‘ë‹µ í•„ë“œ ì¶”ì¶œ
        conclusion = response.conclusion
        evidence = response.evidence
        caveats = response.caveats
        
        # ì‘ë‹µ í•„ë“œ ê²€ì¦ (ë””ë²„ê·¸ ëª¨ë“œì—ì„œë§Œ)
        
        result = {
            "conclusion": conclusion,
            "evidence": evidence,
            "caveats": caveats,
            "web_quotes": [],
            "web_info": {}
        }
        
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"ìµœì¢… ê²°ê³¼ ìƒì„± ì™„ë£Œ")
        
        return result
    except Exception as e:
        # structured output ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡°ë¡œ fallback
        error_str = str(e).lower()
        logger.error(f"Summarize ë…¸ë“œ structured output ì‹¤íŒ¨: {str(e)}")
        
        if "quota" in error_str or "limit" in error_str or "429" in error_str or "exceeded" in error_str:
            return {
                "conclusion": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "evidence": [EvidenceInfo(text="Gemini API ì¼ì¼ í• ë‹¹ëŸ‰ ì´ˆê³¼", source="API ì‹œìŠ¤í…œ")],
                "caveats": [
                    CaveatInfo(text="ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", source="API ì‹œìŠ¤í…œ"),
                    CaveatInfo(text="API í• ë‹¹ëŸ‰ì´ ë³µêµ¬ë˜ë©´ ì •ìƒì ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", source="API ì‹œìŠ¤í…œ"),
                    CaveatInfo(text="ì˜¤ë¥˜ ì½”ë“œ: 429 (Quota Exceeded)", source="API ì‹œìŠ¤í…œ")
                ],
                "web_quotes": [],
                "web_info": {}
            }
        elif "404" in error_str or "publisher" in error_str or "model" in error_str:
            return {
                "conclusion": "ëª¨ë¸ ì„¤ì • ì˜¤ë¥˜ë¡œ ì¸í•´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "evidence": [EvidenceInfo(text="Gemini ëª¨ë¸ ì„¤ì • ì˜¤ë¥˜", source="API ì‹œìŠ¤í…œ")],
                "caveats": [
                    CaveatInfo(text="ëª¨ë¸ ì´ë¦„ì„ í™•ì¸í•´ì£¼ì„¸ìš”.", source="API ì‹œìŠ¤í…œ"),
                    CaveatInfo(text="ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", source="API ì‹œìŠ¤í…œ")
                ],
                "web_quotes": [],
                "web_info": {}
            }
        else:
            return {
                "conclusion": "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "evidence": [EvidenceInfo(text=f"ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {str(e)[:100]}", source="ì‹œìŠ¤í…œ ì˜¤ë¥˜")],
                "caveats": [
                    CaveatInfo(text=f"ìƒì„¸ ì˜¤ë¥˜: {str(e)}", source="ì‹œìŠ¤í…œ ì˜¤ë¥˜"),
                    CaveatInfo(text="ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.", source="ì‹œìŠ¤í…œ ì˜¤ë¥˜")
                ],
                "web_quotes": [],
                "web_info": {}
            }

def _format_web_results(web_results: list) -> str:
    """ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬ë§·íŒ…"""
    if not web_results:
        return "ì‹¤ì‹œê°„ ë‰´ìŠ¤ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    web_parts = []
    for i, result in enumerate(web_results[:3], 1):  # ìƒìœ„ 3ê°œë§Œ ì‚¬ìš©
        title = result.get("title", "ì œëª© ì—†ìŒ")
        snippet = result.get("snippet", "")[:200]  # 200ìë¡œ ì œí•œ
        web_parts.append(f"[ë‰´ìŠ¤ {i}] {title}\n{snippet}\n")
    
    return "\n".join(web_parts)

def summarize_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    ìš”ì•½ ì—ì´ì „íŠ¸: ì•½ê´€/ë¬¸ì„œë¥¼ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìš”ì•½ (ìµœì í™”ëœ ë²„ì „)
    """
    start_time = time.time()
    question = state.get("question", "")
    refined = state.get("refined", [])
    web_results = state.get("web_results", [])
    
    # ì„±ëŠ¥ ë¡œê¹…
    log_performance("Summarize ì‹œì‘", start_time, 
                   question_length=len(question), refined_count=len(refined),
                   web_results_count=len(web_results))
    
    # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… (ìµœì í™”ëœ í•¨ìˆ˜ ì‚¬ìš©)
    context = format_context_optimized(refined)
    web_info = _format_web_results(web_results)
    
    # ìºì‹œëœ í”„ë¡¬í”„íŠ¸ ë¡œë“œ (ëª¨ë“ˆ ë ˆë²¨ ìºì‹œ ì‚¬ìš©)
    system_prompt = get_system_prompt()
    summarize_prompt = get_prompt_cached("summarize")
    
    # ì›¹ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    full_prompt = f"""{system_prompt}

{summarize_prompt}

## ì§ˆë¬¸
{question}

## ì°¸ê³  ë¬¸ì„œ
{context}

## ì‹¤ì‹œê°„ ë‰´ìŠ¤/ì •ë³´
{web_info}

ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”."""
    
    try:
        # LLM ì‘ë‹µ ìºì‹œ í™•ì¸
        prompt_hash = cache_manager.generate_prompt_hash(full_prompt)
        cached_response = cache_manager.get_cached_llm_response(prompt_hash)
        if cached_response:
            logger.info("ğŸ” [Summarize] LLM ì‘ë‹µ ìºì‹œ íˆíŠ¸!")
            answer = cached_response
        else:
            # Answerer ì „ìš© LLM ì‚¬ìš© (Gemini 2.5 Flash)
            llm = get_answerer_llm()
            
            # ê°„ì†Œí™”ëœ structured output ì‚¬ìš©
            try:
                answer = _parse_llm_response_structured(llm, full_prompt, emergency_fallback=False)
                
                # LLM ì‘ë‹µ ìºì‹±
                cache_manager.cache_llm_response(prompt_hash, answer)
                logger.info("ğŸ” [Summarize] LLM ì‘ë‹µ ìºì‹œ ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"Structured output ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")
                answer = get_simple_fallback_response(question, "Summarize")
        
        # verify_refine ë°ì´í„° ì²˜ë¦¬ (ìµœì í™”ëœ í•¨ìˆ˜ ì‚¬ìš©)
        answer = process_verify_refine_data(state, answer)
        
        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ web_quotesì— ì¶”ê°€ (ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ)
        if web_results and not answer.get("web_quotes"):
            answer["web_quotes"] = [
                {
                    "text": result.get("snippet", "")[:200] + "...",
                    "source": f"ì›¹ê²€ìƒ‰_{result.get('title', 'ì œëª© ì—†ìŒ')}_{result.get('url', 'URL ì—†ìŒ')}"
                }
                for result in web_results[:3]  # ìƒìœ„ 3ê°œë§Œ
            ]
        
        # web_info í•„ë“œ ì²˜ë¦¬
        if isinstance(answer.get("web_info"), dict):
            web_info_dict = answer["web_info"]
            answer["web_info"] = {
                "latest_news": web_info_dict.get("latest_news", ""),
                "travel_alerts": web_info_dict.get("travel_alerts", "")
            }
        elif not answer.get("web_info"):
            answer["web_info"] = {
                "latest_news": "",
                "travel_alerts": ""
            }
        
        # ì„±ëŠ¥ ë¡œê¹…
        log_performance("Summarize ì™„ë£Œ", start_time, 
                       conclusion_length=len(answer.get('conclusion', '')))
        
        return {
            **state, 
            "draft_answer": answer, 
            "final_answer": answer
        }
        
    except Exception as e:
        # ìµœì í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬
        logger.error(f"Summarize LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
        fallback_answer = handle_llm_error_optimized(e, question, "Summarize")
        return {**state, "draft_answer": fallback_answer, "final_answer": fallback_answer}