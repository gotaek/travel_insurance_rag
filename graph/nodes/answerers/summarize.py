from typing import Dict, Any
import json
from pathlib import Path
from app.deps import get_llm
from graph.models import AnswerResponse

def _load_prompt(prompt_name: str) -> str:
    """í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ"""
    # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
    current_dir = Path(__file__).parent.parent.parent
    prompt_path = current_dir / "prompts" / f"{prompt_name}.md"
    return prompt_path.read_text(encoding="utf-8")

def _format_context(passages: list) -> str:
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
    if not passages:
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    context_parts = []
    for i, passage in enumerate(passages[:5], 1):  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
        doc_id = passage.get("doc_id", "ì•Œ ìˆ˜ ì—†ìŒ")
        page = passage.get("page", "ì•Œ ìˆ˜ ì—†ìŒ")
        text = passage.get("text", "")[:500]  # 500ìë¡œ ì œí•œ
        context_parts.append(f"[ë¬¸ì„œ {i}] {doc_id} (í˜ì´ì§€ {page})\n{text}\n")
    
    return "\n".join(context_parts)

def _parse_llm_response_structured(llm, prompt: str, emergency_fallback: bool = False) -> Dict[str, Any]:
    """LLM ì‘ë‹µì„ structured outputìœ¼ë¡œ íŒŒì‹±"""
    try:
        print(f"ğŸ” [Summarize] structured output ì‹œì‘")
        print(f"ğŸ” [Summarize] í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")
        print(f"ğŸ” [Summarize] í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {prompt[:200]}...")
        print(f"ğŸ” [Summarize] ê¸´ê¸‰ íƒˆì¶œ ëª¨ë“œ: {emergency_fallback}")
        
        # structured output ì‚¬ìš© (ê¸´ê¸‰ íƒˆì¶œ ëª¨ë“œ ì§€ì›)
        structured_llm = llm.with_structured_output(AnswerResponse, emergency_fallback=emergency_fallback)
        print(f"ğŸ” [Summarize] structured_llm ìƒì„± ì™„ë£Œ")
        
        response = structured_llm.generate_content(prompt)
        print(f"ğŸ” [Summarize] LLM ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
        print(f"ğŸ” [Summarize] ì‘ë‹µ ê°ì²´ íƒ€ì…: {type(response)}")
        print(f"ğŸ” [Summarize] ì‘ë‹µ ê°ì²´ ì†ì„±ë“¤: {dir(response)}")
        
        # ê° í•„ë“œë³„ë¡œ ìƒì„¸ ë¡œê·¸
        conclusion = response.conclusion
        evidence = response.evidence
        caveats = response.caveats
        quotes = response.quotes
        
        print(f"ğŸ” [Summarize] conclusion: '{conclusion}' (íƒ€ì…: {type(conclusion)}, ê¸¸ì´: {len(str(conclusion))})")
        print(f"ğŸ” [Summarize] evidence: {evidence} (íƒ€ì…: {type(evidence)}, ë¦¬ìŠ¤íŠ¸ ê¸¸ì´: {len(evidence) if evidence else 0})")
        print(f"ğŸ” [Summarize] caveats: {caveats} (íƒ€ì…: {type(caveats)}, ê¸¸ì´: {len(caveats) if caveats else 0})")
        print(f"ğŸ” [Summarize] quotes: {quotes} (íƒ€ì…: {type(quotes)}, ê¸¸ì´: {len(quotes) if quotes else 0})")
        
        result = {
            "conclusion": conclusion,
            "evidence": evidence,
            "caveats": caveats,
            "quotes": quotes
        }
        
        print(f"ğŸ” [Summarize] ìµœì¢… ê²°ê³¼: {result}")
        print(f"ğŸ” [Summarize] ìµœì¢… ê²°ê³¼ íƒ€ì…: {type(result)}")
        
        return result
    except Exception as e:
        # structured output ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡°ë¡œ fallback
        error_str = str(e).lower()
        print(f"âŒ Summarize ë…¸ë“œ structured output ì‹¤íŒ¨: {str(e)}")
        
        if "quota" in error_str or "limit" in error_str or "429" in error_str or "exceeded" in error_str:
            return {
                "conclusion": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "evidence": ["Gemini API ì¼ì¼ í• ë‹¹ëŸ‰ ì´ˆê³¼"],
                "caveats": [
                    "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    "API í• ë‹¹ëŸ‰ì´ ë³µêµ¬ë˜ë©´ ì •ìƒì ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "ì˜¤ë¥˜ ì½”ë“œ: 429 (Quota Exceeded)"
                ],
                "quotes": []
            }
        elif "404" in error_str or "publisher" in error_str or "model" in error_str:
            return {
                "conclusion": "ëª¨ë¸ ì„¤ì • ì˜¤ë¥˜ë¡œ ì¸í•´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "evidence": ["Gemini ëª¨ë¸ ì„¤ì • ì˜¤ë¥˜"],
                "caveats": [
                    "ëª¨ë¸ ì´ë¦„ì„ í™•ì¸í•´ì£¼ì„¸ìš”.",
                    "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                ],
                "quotes": []
            }
        else:
            return {
                "conclusion": "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "evidence": [f"ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {str(e)[:100]}"],
                "caveats": [f"ìƒì„¸ ì˜¤ë¥˜: {str(e)}", "ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."],
                "quotes": []
            }

def summarize_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    ìš”ì•½ ì—ì´ì „íŠ¸: ì•½ê´€/ë¬¸ì„œë¥¼ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìš”ì•½
    """
    question = state.get("question", "")
    passages = state.get("passages", [])
    
    # ê¸´ê¸‰ íƒˆì¶œ ë¡œì§: ì—°ì† êµ¬ì¡°í™” ì‹¤íŒ¨ ê°ì§€
    structured_failure_count = state.get("structured_failure_count", 0)
    max_structured_failures = state.get("max_structured_failures", 2)
    emergency_fallback_used = state.get("emergency_fallback_used", False)
    
    # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    context = _format_context(passages)
    
    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    system_prompt = _load_prompt("system_core")
    summarize_prompt = _load_prompt("summarize")
    
    # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    full_prompt = f"""
{system_prompt}

{summarize_prompt}

## ì§ˆë¬¸
{question}

## ì°¸ê³  ë¬¸ì„œ
{context}

ìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”.
"""
    
    try:
        # LLM í˜¸ì¶œ
        llm = get_llm()
        print(f"ğŸ” [Summarize Node] LLM ê°ì²´ íšë“ ì™„ë£Œ")
        
        # ê¸´ê¸‰ íƒˆì¶œ ëª¨ë“œ ê²°ì •
        use_emergency_fallback = (structured_failure_count >= max_structured_failures) or emergency_fallback_used
        
        if use_emergency_fallback:
            print(f"ğŸš¨ [Summarize Node] ê¸´ê¸‰ íƒˆì¶œ ëª¨ë“œ í™œì„±í™” - êµ¬ì¡°í™” ì‹¤íŒ¨ íšŸìˆ˜: {structured_failure_count}/{max_structured_failures}")
        
        # structured output ì‚¬ìš© (ê¸´ê¸‰ íƒˆì¶œ ëª¨ë“œ ì§€ì›)
        answer = _parse_llm_response_structured(llm, full_prompt, emergency_fallback=use_emergency_fallback)
        print(f"ğŸ” [Summarize Node] structured output ì™„ë£Œ")
        print(f"ğŸ” [Summarize Node] ë°›ì€ answer: {answer}")
        print(f"ğŸ” [Summarize Node] answer íƒ€ì…: {type(answer)}")
        print(f"ğŸ” [Summarize Node] answer keys: {answer.keys() if isinstance(answer, dict) else 'Not a dict'}")
        
        # êµ¬ì¡°í™” ì‹¤íŒ¨ ê°ì§€ ë° ì¹´ìš´í„° ì—…ë°ì´íŠ¸
        is_empty_result = (
            not answer.get("conclusion") or 
            answer.get("conclusion", "").strip() == "" or
            answer.get("conclusion", "").strip() == "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )
        
        if is_empty_result and not use_emergency_fallback:
            # êµ¬ì¡°í™” ì‹¤íŒ¨ ì¹´ìš´í„° ì¦ê°€
            new_failure_count = structured_failure_count + 1
            print(f"âš ï¸ [Summarize Node] êµ¬ì¡°í™” ì‹¤íŒ¨ ê°ì§€ - ì¹´ìš´í„°: {new_failure_count}/{max_structured_failures}")
            
            # ì—°ì† ì‹¤íŒ¨ê°€ ì„ê³„ê°’ì— ë„ë‹¬í•˜ë©´ ê¸´ê¸‰ íƒˆì¶œ ëª¨ë“œë¡œ ì¬ì‹œë„
            if new_failure_count >= max_structured_failures:
                print(f"ğŸš¨ [Summarize Node] ì—°ì† êµ¬ì¡°í™” ì‹¤íŒ¨ ì„ê³„ê°’ ë„ë‹¬ - ê¸´ê¸‰ íƒˆì¶œ ëª¨ë“œë¡œ ì¬ì‹œë„")
                answer = _parse_llm_response_structured(llm, full_prompt, emergency_fallback=True)
                return {
                    **state, 
                    "draft_answer": answer, 
                    "final_answer": answer,
                    "structured_failure_count": new_failure_count,
                    "emergency_fallback_used": True
                }
            else:
                return {
                    **state, 
                    "draft_answer": answer, 
                    "final_answer": answer,
                    "structured_failure_count": new_failure_count
                }
        
        # ê° í•„ë“œë³„ ìƒì„¸ ê²€ì‚¬
        for key, value in answer.items():
            print(f"ğŸ” [Summarize Node] {key}: '{value}' (íƒ€ì…: {type(value)}, ê¸¸ì´: {len(str(value)) if value else 0})")
        
        # ì¶œì²˜ ì •ë³´ ì¶”ê°€
        if passages:
            print(f"ğŸ” [Summarize Node] ì¶œì²˜ ì •ë³´ ì¶”ê°€ ì‹œì‘ - passages ìˆ˜: {len(passages)}")
            answer["quotes"] = [
                {
                    "text": p.get("text", "")[:200] + "...",
                    "source": f"{p.get('doc_id', 'ì•Œ ìˆ˜ ì—†ìŒ')}_{p.get('doc_name', 'ë¬¸ì„œ')}_í˜ì´ì§€{p.get('page', '?')}"
                }
                for p in passages[:3]  # ìƒìœ„ 3ê°œë§Œ
            ]
            print(f"ğŸ” [Summarize Node] ì¶œì²˜ ì •ë³´ ì¶”ê°€ ì™„ë£Œ - quotes ìˆ˜: {len(answer.get('quotes', []))}")
        
        # ì„±ê³µ ì‹œ êµ¬ì¡°í™” ì‹¤íŒ¨ ì¹´ìš´í„° ë¦¬ì…‹
        final_result = {
            **state, 
            "draft_answer": answer, 
            "final_answer": answer,
            "structured_failure_count": 0,
            "emergency_fallback_used": False
        }
        print(f"ğŸ” [Summarize Node] ìµœì¢… ê²°ê³¼ ìƒì„± ì™„ë£Œ")
        print(f"ğŸ” [Summarize Node] final_result keys: {final_result.keys()}")
        print(f"ğŸ” [Summarize Node] draft_answer: {final_result.get('draft_answer')}")
        print(f"ğŸ” [Summarize Node] final_answer: {final_result.get('final_answer')}")
        
        return final_result
        
    except Exception as e:
        # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ fallback
        fallback_answer = {
            "conclusion": f"ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: '{question}'",
            "evidence": ["LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."],
            "caveats": ["ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.", f"ì˜¤ë¥˜: {str(e)}"],
            "quotes": []
        }
        return {**state, "draft_answer": fallback_answer, "final_answer": fallback_answer}