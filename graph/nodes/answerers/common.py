# common.py â€” answerer ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
"""
ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ê³µí†µ í•¨ìˆ˜ë“¤
- ì¤‘ë³µ ì½”ë“œ ì œê±°
- ë¹ ë¥¸ ì‘ë‹µ ì²˜ë¦¬
- íš¨ìœ¨ì ì¸ ë¡œê¹…
"""

from typing import Dict, Any, List
import logging
from graph.models import EvidenceInfo, CaveatInfo
from graph.prompts.utils import get_cached_prompt, get_simple_fallback_response

logger = logging.getLogger(__name__)

# ì „ì—­ í”„ë¡¬í”„íŠ¸ ìºì‹œ (ëª¨ë“ˆ ë ˆë²¨)
_system_prompt = None
_prompt_cache = {}

def get_system_prompt() -> str:
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìºì‹œ (ëª¨ë“ˆ ë ˆë²¨)"""
    global _system_prompt
    if _system_prompt is None:
        _system_prompt = get_cached_prompt("system_core")
    return _system_prompt

def get_prompt_cached(prompt_name: str) -> str:
    """í”„ë¡¬í”„íŠ¸ ìºì‹œ (ëª¨ë“ˆ ë ˆë²¨)"""
    if prompt_name not in _prompt_cache:
        _prompt_cache[prompt_name] = get_cached_prompt(prompt_name)
    return _prompt_cache[prompt_name]

def format_context_optimized(passages: List[Dict]) -> str:
    """ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
    if not passages:
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ìƒìœ„ 5ê°œë§Œ ì²˜ë¦¬, í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
    context_parts = []
    for i, passage in enumerate(passages[:5], 1):
        doc_id = passage.get("doc_id", "ì•Œ ìˆ˜ ì—†ìŒ")
        page = passage.get("page", "ì•Œ ìˆ˜ ì—†ìŒ")
        text = passage.get("text", "")[:500]  # 500ìë¡œ ì œí•œ
        context_parts.append(f"[ë¬¸ì„œ {i}] {doc_id} (í˜ì´ì§€ {page})\n{text}\n")
    
    return "\n".join(context_parts)

def process_verify_refine_data(state: Dict[str, Any], answer: Dict[str, Any]) -> Dict[str, Any]:
    """verify_refine ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬ (evidence ê°œìˆ˜ ì œí•œ)"""
    # verify_refineì—ì„œ ìƒì„±ëœ ì •ë³´ë“¤
    citations = state.get("citations", [])
    warnings = state.get("warnings", [])
    verification_status = state.get("verification_status", "pass")
    policy_disclaimer = state.get("policy_disclaimer", "")
    
    # citationsë¥¼ evidenceì— ì¶”ê°€ (quotes ëŒ€ì‹  evidence ì‚¬ìš©)
    if citations and not answer.get("evidence"):
        citation_evidence = [
            EvidenceInfo(
                text=c.get("snippet", "")[:200] + "...",
                source=f"{c.get('insurer', '')}_{c.get('doc_id', 'ì•Œ ìˆ˜ ì—†ìŒ')}_í˜ì´ì§€{c.get('page', '?')}"
            )
            for c in citations[:3]  # ìƒìœ„ 3ê°œë§Œ
        ]
        answer["evidence"] = citation_evidence
    
    # warningsë¥¼ caveatsì— ë°˜ì˜
    if warnings:
        warning_caveats = [
            CaveatInfo(text=f"âš ï¸ {warning}", source="ê²€ì¦ ì‹œìŠ¤í…œ") 
            for warning in warnings[:2]  # ìƒìœ„ 2ê°œ ê²½ê³ ë§Œ
        ]
        answer["caveats"].extend(warning_caveats)
    
    # policy_disclaimerë¥¼ caveatsì— ì¶”ê°€
    if policy_disclaimer:
        answer["caveats"].append(CaveatInfo(text=f"ğŸ“‹ {policy_disclaimer}", source="ë²•ì  ë©´ì±… ì¡°í•­"))
    
    # verification_statusì— ë”°ë¥¸ ë‹µë³€ ì¡°ì •
    if verification_status == "fail":
        answer["conclusion"] = "ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."
        answer["caveats"].append(CaveatInfo(text="ì¶”ê°€ ê²€ìƒ‰ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", source="ê²€ì¦ ì‹œìŠ¤í…œ"))
    elif verification_status == "warn":
        answer["caveats"].append(CaveatInfo(text="ì¼ë¶€ ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ìƒì¶©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", source="ê²€ì¦ ì‹œìŠ¤í…œ"))
    
    # evidence ê°œìˆ˜ë¥¼ 5ê°œë¡œ ì œí•œ (ì„±ëŠ¥ ìµœì í™”)
    if "evidence" in answer and len(answer["evidence"]) > 5:
        try:
            # score ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 5ê°œë§Œ ì„ íƒ
            sorted_evidence = sorted(
                answer["evidence"], 
                key=lambda x: getattr(x, 'score', 0) if hasattr(x, 'score') else 0, 
                reverse=True
            )
            answer["evidence"] = sorted_evidence[:5]
        except Exception:
            # scoreê°€ ì—†ëŠ” ê²½ìš° ë‹¨ìˆœíˆ ì•ì˜ 5ê°œë§Œ ì„ íƒ
            answer["evidence"] = answer["evidence"][:5]
    
    return answer

def create_optimized_prompt(system_prompt: str, task_prompt: str, question: str, context: str) -> str:
    """ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    return f"""{system_prompt}

{task_prompt}

## ì§ˆë¬¸
{question}

## ì°¸ê³  ë¬¸ì„œ
{context}

ìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."""

def handle_llm_error_optimized(error: Exception, question: str, node_type: str) -> Dict[str, Any]:
    """ìµœì í™”ëœ LLM ì˜¤ë¥˜ ì²˜ë¦¬"""
    error_str = str(error).lower()
    
    if "quota" in error_str or "limit" in error_str or "429" in error_str or "exceeded" in error_str:
        return {
            "conclusion": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "evidence": [EvidenceInfo(text="Gemini API ì¼ì¼ í• ë‹¹ëŸ‰ ì´ˆê³¼", source="API ì‹œìŠ¤í…œ")],
            "caveats": [
                CaveatInfo(text="ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", source="API ì‹œìŠ¤í…œ"),
                CaveatInfo(text="API í• ë‹¹ëŸ‰ì´ ë³µêµ¬ë˜ë©´ ì •ìƒì ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", source="API ì‹œìŠ¤í…œ")
            ],
        }
    elif "404" in error_str or "publisher" in error_str or "model" in error_str:
        return {
            "conclusion": "ëª¨ë¸ ì„¤ì • ì˜¤ë¥˜ë¡œ ì¸í•´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "evidence": [EvidenceInfo(text="Gemini ëª¨ë¸ ì„¤ì • ì˜¤ë¥˜", source="API ì‹œìŠ¤í…œ")],
            "caveats": [
                CaveatInfo(text="ëª¨ë¸ ì´ë¦„ì„ í™•ì¸í•´ì£¼ì„¸ìš”.", source="API ì‹œìŠ¤í…œ"),
                CaveatInfo(text="ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", source="API ì‹œìŠ¤í…œ")
            ],
        }
    else:
        return get_simple_fallback_response(question, node_type)

def log_performance(operation: str, start_time: float, **kwargs):
    """ì„±ëŠ¥ ë¡œê¹… (ë””ë²„ê·¸ ëª¨ë“œì—ì„œë§Œ)"""
    if logger.isEnabledFor(logging.DEBUG):
        import time
        duration = time.time() - start_time
        logger.debug(f"{operation} ì™„ë£Œ - ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ, {kwargs}")
