from typing import Dict, Any, List, Optional, Tuple
import os
import yaml
import hashlib
import re
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import logging

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

POLICY_PATH = os.getenv("POLICY_PATH", "config/policies.yaml")

# ì •ì±… ìºì‹œ
_policy_cache = None
_cache_timestamp = None

def _load_policies() -> Dict[str, Any]:
    """ì •ì±… íŒŒì¼ ë¡œë“œ ë° ìºì‹œ ê´€ë¦¬"""
    global _policy_cache, _cache_timestamp
    
    current_time = datetime.now()
    
    # ìºì‹œê°€ ì—†ê±°ë‚˜ 5ë¶„ ì´ìƒ ì§€ë‚¬ìœ¼ë©´ ì¬ë¡œë“œ
    if _policy_cache is None or _cache_timestamp is None or (current_time - _cache_timestamp).seconds > 300:
        try:
            if os.path.exists(POLICY_PATH):
                with open(POLICY_PATH, "r", encoding="utf-8") as f:
                    _policy_cache = yaml.safe_load(f) or {}
                _cache_timestamp = current_time
            else:
                _policy_cache = {}
                logger.warning(f"ì •ì±… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {POLICY_PATH}")
        except Exception as e:
            logger.error(f"ì •ì±… íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            _policy_cache = {}
    
    return _policy_cache

def _validate_policy_schema(policies: Dict[str, Any]) -> List[str]:
    """ì •ì±… ìŠ¤í‚¤ë§ˆ ê²€ì¦"""
    warnings = []
    required_keys = ["legal", "answer"]
    
    for key in required_keys:
        if key not in policies:
            warnings.append(f"í•„ìˆ˜ ì •ì±… í‚¤ ëˆ„ë½: {key}")
    
    # answer ì„¹ì…˜ í•„ìˆ˜ í‚¤ ê²€ì¦
    answer_section = policies.get("answer", {})
    answer_required = ["min_citations", "min_context"]
    for key in answer_required:
        if key not in answer_section:
            warnings.append(f"answer ì„¹ì…˜ í•„ìˆ˜ í‚¤ ëˆ„ë½: {key}")
    
    return warnings

def _get_intent_based_requirements(intent: str, policies: Dict[str, Any]) -> Dict[str, int]:
    """ì˜ë„ë³„ ê¸°ì¤€ ì ìš©"""
    base_requirements = {
        "qa": {"min_context": 1, "min_citations": 1, "min_insurers": 1},
        "compare": {"min_context": 3, "min_citations": 3, "min_insurers": 2},
        "summary": {"min_context": 2, "min_citations": 2, "min_insurers": 1},
        "recommend": {"min_context": 3, "min_citations": 3, "min_insurers": 2}
    }
    
    # ì •ì±…ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥
    policy_requirements = policies.get("intent_requirements", {}).get(intent, {})
    
    requirements = base_requirements.get(intent, base_requirements["qa"])
    requirements.update(policy_requirements)
    
    return requirements

def _check_score_and_freshness(refined: List[Dict[str, Any]], policies: Dict[str, Any]) -> Tuple[bool, List[str]]:
    """ìŠ¤ì½”ì–´ ë° ì‹ ì„ ë„ ì„ê³„ì¹˜ ê²€ì¦"""
    warnings = []
    needs_more_search = False
    
    # ìŠ¤ì½”ì–´ ì„ê³„ì¹˜
    min_score = policies.get("quality", {}).get("min_score", 0.3)
    # ì‹ ì„ ë„ ì„ê³„ì¹˜ (ì¼ ë‹¨ìœ„)
    max_age_days = policies.get("quality", {}).get("max_age_days", 365)
    
    low_score_count = 0
    old_doc_count = 0
    
    for doc in refined:
        score = doc.get("score", 0.0)
        if score < min_score:
            low_score_count += 1
        
        # ë²„ì „ ë‚ ì§œ íŒŒì‹±
        version_date = doc.get("version_date")
        if version_date:
            try:
                if isinstance(version_date, str):
                    doc_date = datetime.strptime(version_date, "%Y-%m-%d")
                else:
                    doc_date = version_date
                
                age_days = (datetime.now() - doc_date).days
                if age_days > max_age_days:
                    old_doc_count += 1
            except:
                warnings.append(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {version_date}")
    
    if low_score_count > 0:
        warnings.append(f"ë‚®ì€ ìŠ¤ì½”ì–´ ë¬¸ì„œ {low_score_count}ê°œ ë°œê²¬ (ì„ê³„ì¹˜: {min_score})")
        needs_more_search = True
    
    if old_doc_count > 0:
        warnings.append(f"ì˜¤ë˜ëœ ë¬¸ì„œ {old_doc_count}ê°œ ë°œê²¬ (ìµœëŒ€ {max_age_days}ì¼)")
        needs_more_search = True
    
    return needs_more_search, warnings

def _remove_duplicates_and_validate_sources(refined: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], List[str]]:
    """ì¤‘ë³µ ì œê±° ë° ì¶œì²˜ í’ˆì§ˆ ê²€ì¦"""
    warnings = []
    
    # ì¤‘ë³µ ì œê±°: (doc_id, page, version) ê¸°ì¤€
    seen = set()
    unique_docs = []
    
    for doc in refined:
        key = (doc.get("doc_id"), doc.get("page"), doc.get("version"))
        if key not in seen:
            seen.add(key)
            unique_docs.append(doc)
        else:
            warnings.append(f"ì¤‘ë³µ ë¬¸ì„œ ì œê±°: {key}")
    
    # ì¶œì²˜ ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ì ìš©
    source_weights = {
        "ê³µì‹ì•½ê´€": 1.0,
        "ê³µì§€": 0.9,
        "ì•ˆë‚´": 0.8,
        "ê¸°íƒ€": 0.7
    }
    
    for doc in unique_docs:
        doc_type = doc.get("doc_type", "ê¸°íƒ€")
        weight = source_weights.get(doc_type, 0.7)
        doc["source_weight"] = weight
    
    return unique_docs, warnings

def _detect_conflicts(refined: List[Dict[str, Any]]) -> List[str]:
    """ìƒì¶© íƒì§€: ë™ì¼ ë³´ì¥ í•­ëª©ì— ì„œë¡œ ë‹¤ë¥¸ í•œë„/ë©´ì±…"""
    warnings = []
    
    # ë³´ì¥ í•­ëª©ë³„ ì •ë³´ ìˆ˜ì§‘
    coverage_info = defaultdict(list)
    
    for doc in refined:
        text = doc.get("text", "").lower()
        insurer = doc.get("insurer", "")
        
        # ê°„ë‹¨í•œ ê¸ˆì•¡ íŒ¨í„´ ë§¤ì¹­ (ì²œë§Œì› í¬í•¨)
        amount_pattern = r'(\d+(?:,\d+)*)(ì²œë§Œì›|ì–µì›|ë§Œì›|ì²œì›|ì›)'
        matches = re.findall(amount_pattern, text)
        
        for amount, unit in matches:
            full_amount = f"{amount}{unit}"
            normalized_value = _normalize_amount(full_amount)
            coverage_info[f"í•œë„_{normalized_value}"].append({
                "insurer": insurer,
                "value": full_amount,
                "normalized": normalized_value,
                "doc_id": doc.get("doc_id")
            })
    
    # ìƒì¶© ê²€ì‚¬ - ëª¨ë“  ë³´ì¥ í•­ëª©ì„ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ë³´ê³  ìƒì¶© ê²€ì‚¬
    all_coverage_items = []
    for coverage_type, info_list in coverage_info.items():
        all_coverage_items.extend(info_list)
    
    if len(all_coverage_items) > 1:
        # ë³´í—˜ì‚¬ë³„ë¡œ ê·¸ë£¹í•‘
        insurer_groups = defaultdict(list)
        for item in all_coverage_items:
            insurer_groups[item["insurer"]].append(item)
        
        # ë³´í—˜ì‚¬ë³„ë¡œ ë‹¤ë¥¸ í•œë„ê°€ ìˆëŠ”ì§€ í™•ì¸
        if len(insurer_groups) > 1:
            unique_values = set(item["normalized"] for item in all_coverage_items)
            if len(unique_values) > 1:
                insurers = list(insurer_groups.keys())
                warnings.append(f"ìƒì¶© íƒì§€: ë³´í—˜ì‚¬ë³„ ë‹¤ë¥¸ í•œë„ ({insurers})")
    
    return warnings

def _normalize_amount(amount_str: str) -> str:
    """ê¸ˆì•¡ ë¬¸ìì—´ì„ ì •ê·œí™” (ì²œì› ë‹¨ìœ„ë¡œ í†µì¼)"""
    original = amount_str
    
    if "ì²œë§Œì›" in original:
        # ì²œë§Œì›ì„ ì²œì›ìœ¼ë¡œ ë³€í™˜
        num = int(original.replace("ì²œë§Œì›", "").replace(",", ""))
        return f"{num * 1000}ì²œì›"
    elif "ì–µì›" in original:
        # ì–µì›ì„ ì²œì›ìœ¼ë¡œ ë³€í™˜
        num = int(original.replace("ì–µì›", "").replace(",", ""))
        return f"{num * 10000}ì²œì›"
    elif "ë§Œì›" in original:
        # ë§Œì›ì„ ì²œì›ìœ¼ë¡œ ë³€í™˜
        num = int(original.replace("ë§Œì›", "").replace(",", ""))
        return f"{num * 10}ì²œì›"
    elif "ì²œì›" in original:
        return original
    elif "ì›" in original:
        # ì›ì„ ì²œì›ìœ¼ë¡œ ë³€í™˜
        num = int(original.replace("ì›", "").replace(",", ""))
        return f"{num // 1000}ì²œì›"
    return original

def _build_standardized_citations(refined: List[Dict[str, Any]], insurer_filter: List[str] = None) -> List[Dict[str, Any]]:
    """í‘œì¤€í™”ëœ ì¸ìš© êµ¬ì¡° ìƒì„± (ë³´í—˜ì‚¬ ìš°ì„ ìˆœìœ„ ì ìš©)"""
    citations = []
    seen_hashes = set()
    
    # ë³´í—˜ì‚¬ í•„í„°ê°€ ìˆìœ¼ë©´ ìš°ì„ ìˆœìœ„ ì •ë ¬
    if insurer_filter:
        refined = _prioritize_insurer_documents(refined, insurer_filter)
    
    for doc in refined:
        # í…ìŠ¤íŠ¸ í•´ì‹œ ìƒì„± (ì¤‘ë³µ ì œê±°ìš©)
        text_hash = hashlib.md5(doc.get("text", "").encode()).hexdigest()[:8]
        
        if text_hash in seen_hashes:
            continue
        seen_hashes.add(text_hash)
        
        # PII ë§ˆìŠ¤í‚¹ (ì„ íƒì )
        snippet = doc.get("text", "")[:120]
        # ê°„ë‹¨í•œ PII íŒ¨í„´ ë§ˆìŠ¤í‚¹
        snippet = re.sub(r'\d{3}-\d{4}-\d{4}', 'XXX-XXXX-XXXX', snippet)  # ì „í™”ë²ˆí˜¸
        snippet = re.sub(r'\d{6}-\d{7}', 'XXXXXX-XXXXXXX', snippet)  # ì£¼ë¯¼ë²ˆí˜¸
        
        # ë³´í—˜ì‚¬ ë§¤ì¹­ ì—¬ë¶€ í™•ì¸
        is_insurer_match = False
        if insurer_filter:
            doc_insurer = doc.get("insurer", "").lower()
            for filter_insurer in insurer_filter:
                if filter_insurer.lower() in doc_insurer or doc_insurer in filter_insurer.lower():
                    is_insurer_match = True
                    break
        
        citation = {
            "doc_id": doc.get("doc_id"),
            "page": doc.get("page"),
            "version": doc.get("version"),
            "insurer": doc.get("insurer"),
            "url": doc.get("url", ""),
            "hash": text_hash,
            "snippet": snippet,
            "score": doc.get("score", 0.0),
            "version_date": doc.get("version_date"),
            "doc_type": doc.get("doc_type", "ê¸°íƒ€"),
            "source_weight": doc.get("source_weight", 1.0),
            "is_insurer_match": is_insurer_match  # ë³´í—˜ì‚¬ ë§¤ì¹­ ì—¬ë¶€ ì¶”ê°€
        }
        citations.append(citation)
    
    return citations

def _prioritize_insurer_documents(refined: List[Dict[str, Any]], insurer_filter: List[str]) -> List[Dict[str, Any]]:
    """
    ë³´í—˜ì‚¬ ë§¤ì¹­ ë¬¸ì„œë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.
    
    Args:
        refined: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        insurer_filter: ìš°ì„ ìˆœìœ„ë¥¼ ë¶€ì—¬í•  ë³´í—˜ì‚¬ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ë³´í—˜ì‚¬ ìš°ì„ ìˆœìœ„ê°€ ì ìš©ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    if not insurer_filter:
        return refined
    
    # ë³´í—˜ì‚¬ ë§¤ì¹­ ë¬¸ì„œì™€ ë¹„ë§¤ì¹­ ë¬¸ì„œ ë¶„ë¦¬
    insurer_matched = []
    other_docs = []
    
    for doc in refined:
        doc_insurer = doc.get("insurer", "").lower()
        is_matched = False
        
        for filter_insurer in insurer_filter:
            if filter_insurer.lower() in doc_insurer or doc_insurer in filter_insurer.lower():
                is_matched = True
                break
        
        if is_matched:
            insurer_matched.append(doc)
        else:
            other_docs.append(doc)
    
    # ë³´í—˜ì‚¬ ë§¤ì¹­ ë¬¸ì„œë¥¼ ë¨¼ì € ë°°ì¹˜í•˜ê³ , ê° ê·¸ë£¹ ë‚´ì—ì„œëŠ” ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
    insurer_matched.sort(key=lambda x: x.get("score", 0.0), reverse=True)
    other_docs.sort(key=lambda x: x.get("score", 0.0), reverse=True)
    
    return insurer_matched + other_docs

def _determine_verification_status(warnings: List[str], requirements: Dict[str, int], 
                                 refined: List[Dict[str, Any]], citations: List[Dict[str, Any]]) -> Tuple[str, str]:
    """ê²€ì¦ ìƒíƒœ ë° ë‹¤ìŒ ì•¡ì…˜ ê²°ì •"""
    
    # ì‹¬ê°í•œ ê²½ê³ ê°€ ìˆìœ¼ë©´ fail
    critical_warnings = [w for w in warnings if any(keyword in w for keyword in ["ìƒì¶©", "ì˜¤ë˜ëœ", "ë‚®ì€ ìŠ¤ì½”ì–´"])]
    if critical_warnings:
        return "fail", "broaden_search"
    
    # ìš”êµ¬ì‚¬í•­ ë¯¸ë‹¬ ì²´í¬
    context_insufficient = len(refined) < requirements["min_context"]
    citations_insufficient = len(citations) < requirements["min_citations"]
    
    # ë³´í—˜ì‚¬ ë‹¤ì–‘ì„± ì²´í¬
    unique_insurers = len(set(doc.get("insurer", "") for doc in refined))
    insurers_insufficient = unique_insurers < requirements.get("min_insurers", 1)
    
    # ìš”êµ¬ì‚¬í•­ ë¯¸ë‹¬ì´ ìˆìœ¼ë©´ warn
    if context_insufficient or citations_insufficient or insurers_insufficient:
        return "warn", "broaden_search"
    
    # ê²½ê³ ê°€ ìˆìœ¼ë©´ warn, ì—†ìœ¼ë©´ pass
    if warnings:
        return "warn", "proceed"
    else:
        return "pass", "proceed"

def _generate_metrics(warnings: List[str], refined: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ë¡œê¹…/ë©”íŠ¸ë¦­ ìƒì„±"""
    metrics = {
        "total_documents": len(refined),
        "unique_insurers": len(set(doc.get("insurer", "") for doc in refined)),
        "avg_score": sum(doc.get("score", 0.0) for doc in refined) / len(refined) if refined else 0.0,
        "warning_counts": {}
    }
    
    # ê²½ê³  ì½”ë“œí™”
    warning_codes = {
        "ìƒì¶©": "coverage_conflict",
        "ì¤‘ë³µ": "duplicate_document", 
        "ë‚®ì€ ìŠ¤ì½”ì–´": "low_score",
        "ì˜¤ë˜ëœ": "outdated_document",
        "ë¶€ì¡±": "insufficient_context"
    }
    
    for warning in warnings:
        for keyword, code in warning_codes.items():
            if keyword in warning:
                metrics["warning_counts"][code] = metrics["warning_counts"].get(code, 0) + 1
                break
    
    return metrics

def verify_refine_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """ê°œì„ ëœ ê²€ì¦ ë° ì •ì œ ë…¸ë“œ (ë³´í—˜ì‚¬ ìš°ì„ ìˆœìœ„ ì ìš©)"""
    import logging
    logger = logging.getLogger(__name__)
    
    # ì…ë ¥ ë°ì´í„°
    refined = state.get("refined", []) or []
    warnings = state.get("warnings", []) or []
    intent = state.get("intent", "qa")
    insurer_filter = state.get("insurer_filter", None)  # Plannerì—ì„œ ì „ë‹¬ëœ ë³´í—˜ì‚¬ í•„í„°
    
    logger.info(f"ğŸ” [VerifyRefine] ì‹œì‘ - ì˜ë„: {intent}, ë³´í—˜ì‚¬ í•„í„°: {insurer_filter}")
    logger.info(f"ğŸ” [VerifyRefine] ì…ë ¥ íŒ¨ì‹œì§€ ìˆ˜: {len(refined)}")
    
    # ì •ì±… ë¡œë“œ ë° ìŠ¤í‚¤ë§ˆ ê²€ì¦
    policies = _load_policies()
    schema_warnings = _validate_policy_schema(policies)
    logger.info(f"ğŸ” [VerifyRefine] ì •ì±… ë¡œë“œ ì™„ë£Œ, ìŠ¤í‚¤ë§ˆ ê²½ê³ : {len(schema_warnings)}ê°œ")
    
    # ìŠ¤í‚¤ë§ˆ ê²½ê³  ì¶”ê°€
    warnings.extend(schema_warnings)
    
    # ì˜ë„ë³„ ê¸°ì¤€ ì ìš©
    requirements = _get_intent_based_requirements(intent, policies)
    logger.info(f"ğŸ” [VerifyRefine] ì˜ë„ë³„ ê¸°ì¤€ ì ìš©: {requirements}")
    
    # ìŠ¤ì½”ì–´ ë° ì‹ ì„ ë„ ê²€ì¦
    needs_more_search, quality_warnings = _check_score_and_freshness(refined, policies)
    warnings.extend(quality_warnings)
    logger.info(f"ğŸ” [VerifyRefine] í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ - ì¶”ê°€ ê²€ìƒ‰ í•„ìš”: {needs_more_search}, ê²½ê³ : {len(quality_warnings)}ê°œ")
    
    # ì¤‘ë³µ ì œê±° ë° ì¶œì²˜ í’ˆì§ˆ ê²€ì¦
    unique_refined, dedup_warnings = _remove_duplicates_and_validate_sources(refined)
    warnings.extend(dedup_warnings)
    logger.info(f"ğŸ” [VerifyRefine] ì¤‘ë³µ ì œê±° ì™„ë£Œ - {len(refined)} â†’ {len(unique_refined)}ê°œ, ê²½ê³ : {len(dedup_warnings)}ê°œ")
    
    # ìƒì¶© íƒì§€
    conflict_warnings = _detect_conflicts(unique_refined)
    warnings.extend(conflict_warnings)
    logger.info(f"ğŸ” [VerifyRefine] ìƒì¶© íƒì§€ ì™„ë£Œ - ê²½ê³ : {len(conflict_warnings)}ê°œ")
    
    # intentì— ë”°ë¥¸ ë™ì  ë¬¸ì„œ ìˆ˜ ì œí•œ
    if intent == "compare":
        doc_limit = 8  # ë¹„êµ ì§ˆë¬¸ì€ ë” ë§ì€ ë¬¸ì„œ í•„ìš”
        logger.info(f"ğŸ” [VerifyRefine] Compare intent - ë¬¸ì„œ 8ê°œë¡œ ì œí•œ")
    else:
        doc_limit = 5  # ê¸°ë³¸ ë¬¸ì„œ ìˆ˜
        logger.info(f"ğŸ” [VerifyRefine] {intent} intent - ë¬¸ì„œ 5ê°œë¡œ ì œí•œ")
    
    unique_refined.sort(key=lambda x: x.get("score", 0.0), reverse=True)
    unique_refined = unique_refined[:doc_limit]
    logger.info(f"ğŸ” [VerifyRefine] ë¬¸ì„œ {doc_limit}ê°œë¡œ ì œí•œ ì™„ë£Œ")
    
    # í‘œì¤€í™”ëœ ì¸ìš© ìƒì„± (ë³´í—˜ì‚¬ ìš°ì„ ìˆœìœ„ ì ìš©)
    citations = _build_standardized_citations(unique_refined, insurer_filter)
    logger.info(f"ğŸ” [VerifyRefine] ì¸ìš© ìƒì„± ì™„ë£Œ - {len(citations)}ê°œ")
    
    # ê²€ì¦ ìƒíƒœ ê²°ì •
    verification_status, next_action = _determine_verification_status(
        warnings, requirements, unique_refined, citations
    )
    logger.info(f"ğŸ” [VerifyRefine] ê²€ì¦ ìƒíƒœ: {verification_status}, ë‹¤ìŒ ì•¡ì…˜: {next_action}")
    
    # ë©”íŠ¸ë¦­ ìƒì„±
    metrics = _generate_metrics(warnings, unique_refined)
    logger.info(f"ğŸ” [VerifyRefine] ë©”íŠ¸ë¦­ ìƒì„± ì™„ë£Œ - ì´ ê²½ê³ : {len(warnings)}ê°œ")
    
    # ë²•ì  ë©´ì±… ì¡°í•­ (ê¸°ë³¸ê°’ ë³´ì¥)
    disclaimer = policies.get("legal", {}).get("disclaimer", 
        "ë³¸ ë‹µë³€ì€ ì°¸ê³ ìš© ì •ë³´ì´ë©°, ì‹¤ì œ ë³´ìƒ/ë³´ì¥ ì—¬ë¶€ëŠ” ë³´í—˜ì¦ê¶Œê³¼ ìµœì‹  ì•½ê´€ì— ë”°ë¦…ë‹ˆë‹¤.")
    
    # ê²°ê³¼ ë°˜í™˜
    out = {
        **state,
        "refined": unique_refined,
        "citations": citations,
        "warnings": warnings,
        "verification_status": verification_status,
        "next_action": next_action,
        "needs_more_search": needs_more_search,
        "requirements": requirements,
        "metrics": metrics,
        "policy_disclaimer": disclaimer,
        "insurer_filter": insurer_filter  # ë³´í—˜ì‚¬ í•„í„° ì •ë³´ ìœ ì§€
    }
    
    logger.info(f"ê²€ì¦ ì™„ë£Œ: {verification_status}, ë‹¤ìŒ ì•¡ì…˜: {next_action}, ê²½ê³ : {len(warnings)}ê°œ, ë³´í—˜ì‚¬ í•„í„°: {insurer_filter}")
    
    return out